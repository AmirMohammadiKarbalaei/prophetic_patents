{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\amoha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\amoha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from utilities.utils_clean import *\n",
    "from utilities.test_dataset_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#download_patents_pto(start_year=2015, end_year=2015, kind='grant',download_path=\"data\")\n",
    "#unzip_files(\"data\",\"patent_grants_2015\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ipg150106.xml\n",
      "Processing ipg150113.xml\n",
      "Processing ipg150120.xml\n",
      "Processing ipg150127.xml\n",
      "Processing ipg150203.xml\n",
      "Processing ipg150210.xml\n",
      "Processing ipg150217.xml\n",
      "Processing ipg150224.xml\n",
      "Processing ipg150303.xml\n",
      "Processing ipg150310.xml\n",
      "Processing ipg150317.xml\n",
      "Processing ipg150324.xml\n",
      "Processing ipg150331.xml\n",
      "Processing ipg150407.xml\n",
      "Processing ipg150414.xml\n",
      "Processing ipg150421.xml\n",
      "Processing ipg150428.xml\n",
      "Processing ipg150505.xml\n",
      "Processing ipg150512.xml\n",
      "Processing ipg150519.xml\n",
      "Processing ipg150526.xml\n",
      "Processing ipg150602.xml\n",
      "Processing ipg150609.xml\n",
      "Processing ipg150616.xml\n",
      "Processing ipg150623.xml\n",
      "Processing ipg150630.xml\n",
      "Processing ipg150707.xml\n",
      "Processing ipg150714.xml\n",
      "Processing ipg150721.xml\n",
      "Processing ipg150728.xml\n",
      "Processing ipg150804.xml\n",
      "Processing ipg150811.xml\n",
      "Processing ipg150818.xml\n",
      "Processing ipg150825.xml\n",
      "Processing ipg150901.xml\n",
      "Processing ipg150908.xml\n",
      "Processing ipg150915.xml\n",
      "Processing ipg150922.xml\n",
      "Processing ipg150929.xml\n",
      "Processing ipg151006.xml\n",
      "Processing ipg151013.xml\n",
      "Processing ipg151020.xml\n",
      "Processing ipg151027.xml\n",
      "Processing ipg151103.xml\n",
      "Processing ipg151110.xml\n",
      "Processing ipg151117.xml\n",
      "Processing ipg151124.xml\n",
      "Processing ipg151201.xml\n",
      "Processing ipg151208.xml\n",
      "Processing ipg151215.xml\n",
      "Processing ipg151222.xml\n",
      "Processing ipg151229.xml\n",
      "Found 25081 out of 25081 documents\n",
      "Saved 25081 patents to test_dataset_2015.pkl\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "test_dataset = create_test_dataset_from_freilich( #\"patent_xmls_2010.json\"\n",
    "    year=2015,  # The year you want to analyze\n",
    "    freilich_data_path=\"Freilich.Data.Compressed.xlsb\",  # Path to your Freilich dataset\n",
    "    path_to_all_xmls_for_chosen_year=\"../app/data/patent_grants_2015\"  # Directory containing XML files\n",
    ")\n",
    "\n",
    "# # Optional: Check the results\n",
    "# print(f\"Number of patents extracted: {len(test_dataset)}\")\n",
    "# print(\"Sample document numbers:\", list(test_dataset.keys())[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved as patent_xmls_2015.json\n"
     ]
    }
   ],
   "source": [
    "save_as_json(test_dataset, \"patent_xmls_2015.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patentnumber</th>\n",
       "      <th>xml</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RE045323</td>\n",
       "      <td>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\\n&lt;!DOCT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RE045324</td>\n",
       "      <td>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\\n&lt;!DOCT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RE045325</td>\n",
       "      <td>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\\n&lt;!DOCT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8925349</td>\n",
       "      <td>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\\n&lt;!DOCT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8925551</td>\n",
       "      <td>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\\n&lt;!DOCT...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  patentnumber                                                xml\n",
       "0     RE045323  <?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n<!DOCT...\n",
       "1     RE045324  <?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n<!DOCT...\n",
       "2     RE045325  <?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n<!DOCT...\n",
       "3      8925349  <?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n<!DOCT...\n",
       "4      8925551  <?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n<!DOCT..."
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = read_json(\"patent_xmls_2015.json\")\n",
    "df = pd.DataFrame.from_dict(data,orient=\"index\").reset_index()\n",
    "df.columns = [\"patentnumber\",\"xml\"]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>patentnumber</th>\n",
       "      <th>appyear</th>\n",
       "      <th>issueyear</th>\n",
       "      <th>inventorcountry</th>\n",
       "      <th>divisional</th>\n",
       "      <th>continuation</th>\n",
       "      <th>prioritydate</th>\n",
       "      <th>priority.formatted</th>\n",
       "      <th>priorityyear</th>\n",
       "      <th>...</th>\n",
       "      <th>yr4</th>\n",
       "      <th>yr8</th>\n",
       "      <th>yr11</th>\n",
       "      <th>claims</th>\n",
       "      <th>wordaverage</th>\n",
       "      <th>litigated</th>\n",
       "      <th>orangebook</th>\n",
       "      <th>forward.cites</th>\n",
       "      <th>industry</th>\n",
       "      <th>subindustry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>497653</td>\n",
       "      <td>8969641</td>\n",
       "      <td>1976.0</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>US</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19760903.0</td>\n",
       "      <td>28006.0</td>\n",
       "      <td>1976.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>497654</td>\n",
       "      <td>9134302</td>\n",
       "      <td>1995.0</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>US</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>19830505.0</td>\n",
       "      <td>30441.0</td>\n",
       "      <td>1983.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>497655</td>\n",
       "      <td>8926966</td>\n",
       "      <td>1995.0</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>US</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>19900921.0</td>\n",
       "      <td>33137.0</td>\n",
       "      <td>1990.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>31.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>497656</td>\n",
       "      <td>9181140</td>\n",
       "      <td>1993.0</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>US</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>19920916.0</td>\n",
       "      <td>33863.0</td>\n",
       "      <td>1992.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>497657</td>\n",
       "      <td>9199886</td>\n",
       "      <td>2009.0</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>US</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>19940119.0</td>\n",
       "      <td>34353.0</td>\n",
       "      <td>1994.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0 patentnumber  appyear  issueyear inventorcountry  divisional  \\\n",
       "0      497653      8969641   1976.0     2015.0              US         0.0   \n",
       "1      497654      9134302   1995.0     2015.0              US         1.0   \n",
       "2      497655      8926966   1995.0     2015.0              US         1.0   \n",
       "3      497656      9181140   1993.0     2015.0              US         0.0   \n",
       "4      497657      9199886   2009.0     2015.0              US         0.0   \n",
       "\n",
       "   continuation  prioritydate  priority.formatted  priorityyear  ...  yr4  \\\n",
       "0           0.0    19760903.0             28006.0        1976.0  ...  NaN   \n",
       "1           1.0    19830505.0             30441.0        1983.0  ...  NaN   \n",
       "2           1.0    19900921.0             33137.0        1990.0  ...  NaN   \n",
       "3           1.0    19920916.0             33863.0        1992.0  ...  0.0   \n",
       "4           1.0    19940119.0             34353.0        1994.0  ...  0.0   \n",
       "\n",
       "   yr8  yr11  claims wordaverage  litigated  orangebook  forward.cites  \\\n",
       "0  NaN   NaN     NaN         NaN        0.0         0.0            0.0   \n",
       "1  NaN   NaN     NaN         NaN        0.0         0.0            0.0   \n",
       "2  NaN   NaN     NaN         NaN        0.0         0.0            0.0   \n",
       "3  0.0   0.0     NaN         NaN        0.0         0.0            0.0   \n",
       "4  0.0   0.0     NaN         NaN        0.0         0.0            0.0   \n",
       "\n",
       "   industry  subindustry  \n",
       "0       1.0         19.0  \n",
       "1       NaN          NaN  \n",
       "2       3.0         31.0  \n",
       "3       NaN          NaN  \n",
       "4       NaN          NaN  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test_data = load_from_pickle(\"../data/test_dataset_2015.pkl\")\n",
    "\n",
    "# df_test_data = pd.DataFrame(test_data,index=[\"xml\"],).T.reset_index()\n",
    "# df_test_data.columns = [\"patentnumber\",\"xml\"] \n",
    "\n",
    "\n",
    "# df = read_xlsb_file()\n",
    "\n",
    "# df[df.issueyear == 2015].to_csv(\"freilichdataet_2015.csv\")\n",
    "df2 = pd.read_csv(\"freilichdataet_2015.csv\")\n",
    "df2[\"patentnumber\"] = df2[\"patentnumber\"].astype(str).transform(lambda x: x.replace(\".0\", \"\"))\n",
    "df2[\"patentnumber\"] = df2[\"patentnumber\"].apply(remove_leadiong_zeros)\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "327\n",
      "Number of exact matches: 10 out of 327 , Percentage: 3.058103975535168\n",
      "Number of exact num of patent extracted: 293 out of 327\n",
      "Avg Total error: 0.7492354740061162, num of corrects:  293\n",
      "Avg Total prophetic error: 17.122324159021407, num of corrects: 10\n",
      "Avg Total nonprophetic error: 16.587155963302752, num of corrects: 11\n",
      "11268.0\n",
      "Sum of number of Unknowns: 0\n",
      "Number of patets with experiments extracted 327\n"
     ]
    }
   ],
   "source": [
    "import sqlite3 as sql\n",
    "def test_algorithm(year):\n",
    "    conn = sql.connect(\"../app/db/patents.db\")\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Execute query and fetch results\n",
    "    stats = cursor.execute(\"select * from patent_statistics\").fetchall()\n",
    "    stats_df = pd.DataFrame(stats, columns=[\"year\", \"patentnumber\", \"prophetic_patents\", \"nonprophetic_patents\",\"unknown_patents\"])\n",
    "    conn.close()\n",
    "\n",
    "    df_year = df[df.issueyear == year][[\"patentnumber\",\"prophetic\",\"nonprophetic\",\"issueyear\"]]\n",
    "    df_year[\"patentnumber\"] = df_year[\"patentnumber\"].astype(str).transform(lambda x: x.replace(\".0\", \"\"))\n",
    "\n",
    "    merged = stats_df.merge(df_year, on=\"patentnumber\")\n",
    "    print(len(merged))\n",
    "    # final = dic_to_dic_w_tense_test(doc_w_exp,threshold=0)\n",
    "    # df_final = pd.DataFrame(final).T.reset_index()\n",
    "    df_final = merged\n",
    "    #df_final.columns = [\"patentnumber\",\"past\",\"present\",\"Unknown\"]\n",
    "    df_final[\"patentnumber\"] = df_final[\"patentnumber\"].apply(remove_leadiong_zeros)\n",
    "    df_check =merged\n",
    "    #df_check[\"past\"] = df_check[\"past\"] + df_check[\"Unknown\"]\n",
    "    df_check[\"Total_Extracted\"] = df_check[\"nonprophetic_patents\"] + df_check[\"prophetic_patents\"]  #+ df_check[\"Unknown\"]\n",
    "    df_check[\"Total_Freilich\"] = df_check[\"prophetic\"] + df_check[\"nonprophetic\"]\n",
    "    df_check[\"prophetic_error\"] = np.sqrt((df_check[\"prophetic\"] - df_check[\"prophetic_patents\"])**2)\n",
    "    df_check[\"nonprophetic_error\"] = np.sqrt((df_check[\"nonprophetic\"] - df_check[\"nonprophetic_patents\"])**2)\n",
    "    df_check[\"Total_Mean_error\"] = np.sqrt((df_check[\"Total_Freilich\"] - df_check[\"Total_Extracted\"])**2)\n",
    "    df_check[\"Sum_error\"] = df_check[\"prophetic_error\"] + df_check[\"nonprophetic_error\"] + df_check[\"Total_Mean_error\"]\n",
    "    print(f\"Number of exact matches: {len(df_check[(df_check.prophetic_error ==0) & (df_check.nonprophetic_error ==0) & (df_check.Total_Mean_error ==0)])} out of {len(df_check)} , Percentage: {len(df_check[(df_check.prophetic_error ==0) & (df_check.nonprophetic_error ==0) & (df_check.Total_Mean_error ==0)])/len(df_check)*100}\")\n",
    "    print(f\"Number of exact num of patent extracted: {len(df_check[(df_check.Total_Extracted == df_check.Total_Freilich)])} out of {len(df_check)}\")\n",
    "    print(f\"Avg Total error: {df_check['Total_Mean_error'].mean()}, num of corrects:  {df_check[df_check['Total_Mean_error'] == 0].shape[0]}\")\n",
    "    print(f\"Avg Total prophetic error: {df_check['prophetic_error'].mean()}, num of corrects: {df_check[df_check['prophetic_error'] == 0].shape[0]}\")\n",
    "    print(f\"Avg Total nonprophetic error: {df_check['nonprophetic_error'].mean()}, num of corrects: {df_check[df_check['nonprophetic_error'] == 0].shape[0]}\")\n",
    "    print(f\"{df_check['Sum_error'].sum()}\")\n",
    "    print(f\"Sum of number of Unknowns: {df_check['unknown_patents'].sum()}\")\n",
    "    print(f\"Number of patets with experiments extracted {len(df_check)}\")\n",
    "    df_check.sort_values(\"Sum_error\",ascending=False).head(20)\n",
    "test_algorithm(2010)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patentnumber</th>\n",
       "      <th>xml</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7641702</td>\n",
       "      <td>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\\n&lt;!DOCT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7641704</td>\n",
       "      <td>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\\n&lt;!DOCT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7641709</td>\n",
       "      <td>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\\n&lt;!DOCT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7641721</td>\n",
       "      <td>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\\n&lt;!DOCT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7641723</td>\n",
       "      <td>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\\n&lt;!DOCT...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  patentnumber                                                xml\n",
       "0      7641702  <?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n<!DOCT...\n",
       "1      7641704  <?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n<!DOCT...\n",
       "2      7641709  <?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n<!DOCT...\n",
       "3      7641721  <?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n<!DOCT...\n",
       "4      7641723  <?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n<!DOCT..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset = read_json(\"patent_xmls_2010.json\")\n",
    "test_dataset_df = pd.DataFrame.from_dict(test_dataset, orient='index').reset_index()\n",
    "test_dataset_df.columns = ['patentnumber', 'xml']\n",
    "test_dataset_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25081\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patentnumber</th>\n",
       "      <th>xml</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>appyear</th>\n",
       "      <th>issueyear</th>\n",
       "      <th>inventorcountry</th>\n",
       "      <th>divisional</th>\n",
       "      <th>continuation</th>\n",
       "      <th>prioritydate</th>\n",
       "      <th>priority.formatted</th>\n",
       "      <th>...</th>\n",
       "      <th>yr4</th>\n",
       "      <th>yr8</th>\n",
       "      <th>yr11</th>\n",
       "      <th>claims</th>\n",
       "      <th>wordaverage</th>\n",
       "      <th>litigated</th>\n",
       "      <th>orangebook</th>\n",
       "      <th>forward.cites</th>\n",
       "      <th>industry</th>\n",
       "      <th>subindustry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RE045323</td>\n",
       "      <td>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\\n&lt;!DOCT...</td>\n",
       "      <td>517229</td>\n",
       "      <td>2014.0</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>DE</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20090408.0</td>\n",
       "      <td>39911.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RE045324</td>\n",
       "      <td>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\\n&lt;!DOCT...</td>\n",
       "      <td>498769</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>US</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20040603.0</td>\n",
       "      <td>38141.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RE045325</td>\n",
       "      <td>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\\n&lt;!DOCT...</td>\n",
       "      <td>502690</td>\n",
       "      <td>2013.0</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>US</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20080418.0</td>\n",
       "      <td>39556.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8925349</td>\n",
       "      <td>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\\n&lt;!DOCT...</td>\n",
       "      <td>522537</td>\n",
       "      <td>2009.0</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>CN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20090519.0</td>\n",
       "      <td>39952.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>51.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8925551</td>\n",
       "      <td>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\\n&lt;!DOCT...</td>\n",
       "      <td>521220</td>\n",
       "      <td>2005.0</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>US</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20040226.0</td>\n",
       "      <td>38043.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>32.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  patentnumber                                                xml  Unnamed: 0  \\\n",
       "0     RE045323  <?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n<!DOCT...      517229   \n",
       "1     RE045324  <?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n<!DOCT...      498769   \n",
       "2     RE045325  <?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n<!DOCT...      502690   \n",
       "3      8925349  <?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n<!DOCT...      522537   \n",
       "4      8925551  <?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n<!DOCT...      521220   \n",
       "\n",
       "   appyear  issueyear inventorcountry  divisional  continuation  prioritydate  \\\n",
       "0   2014.0     2015.0              DE         0.0           0.0    20090408.0   \n",
       "1   2012.0     2015.0              US         0.0           0.0    20040603.0   \n",
       "2   2013.0     2015.0              US         0.0           1.0    20080418.0   \n",
       "3   2009.0     2015.0              CN         0.0           0.0    20090519.0   \n",
       "4   2005.0     2015.0              US         0.0           0.0    20040226.0   \n",
       "\n",
       "   priority.formatted  ...  yr4 yr8  yr11  claims  wordaverage litigated  \\\n",
       "0             39911.0  ...  NaN NaN   NaN     NaN          NaN       0.0   \n",
       "1             38141.0  ...  NaN NaN   NaN     NaN          NaN       0.0   \n",
       "2             39556.0  ...  NaN NaN   NaN     NaN          NaN       0.0   \n",
       "3             39952.0  ...  NaN NaN   NaN     NaN          NaN       0.0   \n",
       "4             38043.0  ...  NaN NaN   NaN     NaN          NaN       0.0   \n",
       "\n",
       "   orangebook  forward.cites  industry  subindustry  \n",
       "0         0.0            0.0       NaN          NaN  \n",
       "1         0.0            0.0       NaN          NaN  \n",
       "2         0.0            0.0       NaN          NaN  \n",
       "3         0.0            0.0       5.0         51.0  \n",
       "4         0.0            0.0       3.0         32.0  \n",
       "\n",
       "[5 rows x 33 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged = df.merge(df2, on=\"patentnumber\")\n",
    "print(len(merged))\n",
    "merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'nonprophetic_patents'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3621\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3620\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3621\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3622\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx:136\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx:163\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5198\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5206\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'nonprophetic_patents'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[1;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 9>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m df_check \u001b[38;5;241m=\u001b[39mmerged\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m#df_check[\"past\"] = df_check[\"past\"] + df_check[\"Unknown\"]\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m df_check[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal_Extracted\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf_check\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnonprophetic_patents\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m+\u001b[39m df_check[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprophetic_patents\u001b[39m\u001b[38;5;124m\"\u001b[39m]  \u001b[38;5;66;03m#+ df_check[\"Unknown\"]\u001b[39;00m\n\u001b[0;32m     10\u001b[0m df_check[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal_Freilich\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m df_check[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprophetic\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m df_check[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonprophetic\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     11\u001b[0m df_check[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprophetic_error\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msqrt((df_check[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprophetic\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m-\u001b[39m df_check[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprophetic_patents\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:3505\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3503\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   3504\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3505\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3506\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3507\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3623\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3621\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3622\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m-> 3623\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3624\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3625\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3626\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3627\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3628\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'nonprophetic_patents'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# final = dic_to_dic_w_tense_test(doc_w_exp,threshold=0)\n",
    "# df_final = pd.DataFrame(final).T.reset_index()\n",
    "df_final = merged\n",
    "#df_final.columns = [\"patentnumber\",\"past\",\"present\",\"Unknown\"]\n",
    "df_final[\"patentnumber\"] = df_final[\"patentnumber\"].apply(remove_leadiong_zeros)\n",
    "df_check =merged\n",
    "#df_check[\"past\"] = df_check[\"past\"] + df_check[\"Unknown\"]\n",
    "df_check[\"Total_Extracted\"] = df_check[\"nonprophetic_patents\"] + df_check[\"prophetic_patents\"]  #+ df_check[\"Unknown\"]\n",
    "df_check[\"Total_Freilich\"] = df_check[\"prophetic\"] + df_check[\"nonprophetic\"]\n",
    "df_check[\"prophetic_error\"] = np.sqrt((df_check[\"prophetic\"] - df_check[\"prophetic_patents\"])**2)\n",
    "df_check[\"nonprophetic_error\"] = np.sqrt((df_check[\"nonprophetic\"] - df_check[\"nonprophetic_patents\"])**2)\n",
    "df_check[\"Total_Mean_error\"] = np.sqrt((df_check[\"Total_Freilich\"] - df_check[\"Total_Extracted\"])**2)\n",
    "df_check[\"Sum_error\"] = df_check[\"prophetic_error\"] + df_check[\"nonprophetic_error\"] + df_check[\"Total_Mean_error\"]\n",
    "print(f\"Number of exact matches: {len(df_check[(df_check.prophetic_error ==0) & (df_check.nonprophetic_error ==0) & (df_check.Total_Mean_error ==0)])} out of {len(df_check)} , Percentage: {len(df_check[(df_check.prophetic_error ==0) & (df_check.nonprophetic_error ==0) & (df_check.Total_Mean_error ==0)])/len(df_check)*100}\")\n",
    "print(f\"Number of exact num of patent extracted: {len(df_check[(df_check.Total_Extracted == df_check.Total_Freilich)])} out of {len(df_check)}\")\n",
    "print(f\"Avg Total error: {df_check['Total_Mean_error'].mean()}, num of corrects:  {df_check[df_check['Total_Mean_error'] == 0].shape[0]}\")\n",
    "print(f\"Avg Total prophetic error: {df_check['prophetic_error'].mean()}, num of corrects: {df_check[df_check['prophetic_error'] == 0].shape[0]}\")\n",
    "print(f\"Avg Total nonprophetic error: {df_check['nonprophetic_error'].mean()}, num of corrects: {df_check[df_check['nonprophetic_error'] == 0].shape[0]}\")\n",
    "print(f\"{df_check['Sum_error'].sum()}\")\n",
    "print(f\"Sum of number of Unknowns: {df._check['unknown_patents'].sum()}\")\n",
    "print(f\"Number of patets with experiments extracted {len(df_check)}\")\n",
    "df_check.sort_values(\"Sum_error\",ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities.nlp_processing import analyze_sentence_tense\n",
    "def dic_to_dic_w_tense_test(doc_w_exp, threshold=0):\n",
    "    dic = {}\n",
    "    pattern = r\"\\(\\d+\\)\\s*([A-Za-z0-9\\-\\(\\)\\{\\},:;=\\[\\]\\+\\*\\s\\.\\^\\$\\%]+(?:\\.(?:sup|delta|Hz|NMR)[^\\)]*)?)\"\n",
    "\n",
    "    for key, value in doc_w_exp.items():\n",
    "        tense_counts = {\"past\": 0, \"present\": 0, \"unknown\": 0}\n",
    "\n",
    "        if isinstance(value, list) and len(value) == 1:\n",
    "            desc =  \"\".join(value[0][\"content\"]) # value[0][\"title\"] + \".\" +\n",
    "            if len(desc) > threshold:\n",
    "                tense = analyze_sentence_tense(desc)\n",
    "                if tense != \"unknown\":\n",
    "                    tense_counts[tense] += 1\n",
    "                else:\n",
    "                    matches = re.findall(pattern, desc)\n",
    "                    if matches:\n",
    "                        tense_counts[\"past\"] += 1\n",
    "                    else:\n",
    "                        tense_counts[\"unknown\"] += 1\n",
    "                dic[key] = tense_counts\n",
    "\n",
    "        elif isinstance(value, list) and len(value) > 1:\n",
    "            for ls in value:\n",
    "                desc = \"\".join(ls[\"content\"]) #ls[\"title\"] + \".\" + \n",
    "                if len(desc) > threshold:\n",
    "                    if len(desc) > 0:\n",
    "                        tense = analyze_sentence_tense(desc)\n",
    "\n",
    "                        if tense != \"unknown\":\n",
    "                            tense_counts[tense] += 1\n",
    "                        else:\n",
    "                            matches = re.findall(pattern, desc)\n",
    "                            if matches:\n",
    "                                tense_counts[\"past\"] += 1\n",
    "                            else:\n",
    "                                tense_counts[\"unknown\"] += 1\n",
    "            dic[key] = tense_counts\n",
    "\n",
    "        # elif isinstance(value, dict):\n",
    "        #     print(value)\n",
    "        #     for ex, desc in value.items():\n",
    "        #         if len(desc) > threshold:\n",
    "        #             tense = analyze_sentence_tense(desc)\n",
    "        #             if tense != \"unknown\":\n",
    "        #                 tense_counts[tense] += 1\n",
    "        #             else:\n",
    "        #                 matches = re.findall(pattern, desc)\n",
    "        #                 if matches:\n",
    "        #                     tense_counts[\"past\"] += 1\n",
    "        #                 else:\n",
    "        #                     tense_counts[\"unknown\"] += 1\n",
    "        #     dic[key] = tense_counts\n",
    "        # else:\n",
    "        #     print(type(value))\n",
    "        #     print(value)\n",
    "        #     print(key)\n",
    "\n",
    "    return dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n'\n",
      " '<!DOCTYPE sequence-cwu SYSTEM \"us-sequence-listing-2004-03-09.dtd\" [ ]>\\n'\n",
      " '<sequence-cwu id=\"SEQLST-1\">\\n'\n",
      " '<publication-reference>\\n'\n",
      " '<document-id>\\n'\n",
      " '<country>US</country>\\n'\n",
      " '<doc-number>07723468</doc-number>\\n'\n",
      " '<kind>B2</kind>\\n'\n",
      " '<date>20100525</date>\\n'\n",
      " '</document-id>\\n'\n",
      " '</publication-reference>\\n'\n",
      " '<number>1</number>\\n'\n",
      " '<sequence-list-new-rules>\\n'\n",
      " '<s100>\\n'\n",
      " '<s160>7</s160>\\n'\n",
      " '</s100>\\n'\n",
      " '<s200>\\n'\n",
      " '<s210>1</s210>\\n'\n",
      " '<s211>18</s211>\\n'\n",
      " '<s212>PRT</s212>\\n'\n",
      " '<s213>Artificial Sequence</s213>\\n'\n",
      " '<s220>\\n'\n",
      " '<s223>Synthesized Sequence</s223>\\n'\n",
      " '</s220>\\n'\n",
      " '</s200>\\n'\n",
      " '<s400> 1\\n'\n",
      " 'Xaa Cys Arg Arg Leu Cys Tyr Lys Gln Arg Cys Val Thr Tyr Cys Arg\\n'\n",
      " ' 1               5                  10                  15\\n'\n",
      " 'Gly Xaa\\n'\n",
      " '</s400>\\n'\n",
      " '<s200>\\n'\n",
      " '<s210>2</s210>\\n'\n",
      " '<s211>18</s211>\\n'\n",
      " '<s212>PRT</s212>\\n'\n",
      " '<s213>Acanthoscurria gomesiana</s213>\\n'\n",
      " '<s220>\\n'\n",
      " '<s221>\\n'\n",
      " 'VARIANT\\n'\n",
      " '</s221>\\n'\n",
      " '<s222>\\n'\n",
      " '1\\n'\n",
      " '</s222>\\n'\n",
      " '<s223>Xaa = Pyroglutamic Acid</s223>\\n'\n",
      " '</s220>\\n'\n",
      " '</s200>\\n'\n",
      " '<s400> 2\\n'\n",
      " 'Xaa Cys Arg Arg Leu Cys Tyr Lys Gln Arg Cys Val Thr Tyr Cys Arg\\n'\n",
      " ' 1               5                  10                  15\\n'\n",
      " 'Gly Arg\\n'\n",
      " '</s400>\\n'\n",
      " '<s200>\\n'\n",
      " '<s210>3</s210>\\n'\n",
      " '<s211>18</s211>\\n'\n",
      " '<s212>PRT</s212>\\n'\n",
      " '<s213>Artificial Sequence</s213>\\n'\n",
      " '<s220>\\n'\n",
      " '<s223>Synthesized Sequence</s223>\\n'\n",
      " '</s220>\\n'\n",
      " '</s200>\\n'\n",
      " '<s400> 3\\n'\n",
      " 'Gln Cys Arg Arg Leu Cys Tyr Lys Gln Arg Cys Val Thr Tyr Cys Arg\\n'\n",
      " ' 1               5                  10                  15\\n'\n",
      " 'Gly Arg\\n'\n",
      " '</s400>\\n'\n",
      " '<s200>\\n'\n",
      " '<s210>4</s210>\\n'\n",
      " '<s211>18</s211>\\n'\n",
      " '<s212>PRT</s212>\\n'\n",
      " '<s213>Artificial Sequence</s213>\\n'\n",
      " '<s220>\\n'\n",
      " '<s223>Synthesized sequence</s223>\\n'\n",
      " '</s220>\\n'\n",
      " '</s200>\\n'\n",
      " '<s400> 4\\n'\n",
      " 'Xaa Cys Arg Arg Leu Asp Tyr Lys Gln Arg Xaa Val Thr Tyr Cys Arg\\n'\n",
      " ' 1               5                  10                  15\\n'\n",
      " 'Gly Xaa\\n'\n",
      " '</s400>\\n'\n",
      " '<s200>\\n'\n",
      " '<s210>5</s210>\\n'\n",
      " '<s211>18</s211>\\n'\n",
      " '<s212>PRT</s212>\\n'\n",
      " '<s213>Artificial Sequence</s213>\\n'\n",
      " '<s220>\\n'\n",
      " '<s223>Synthesized sequence</s223>\\n'\n",
      " '</s220>\\n'\n",
      " '</s200>\\n'\n",
      " '<s400> 5\\n'\n",
      " 'Xaa Glu Arg Arg Leu Cys Tyr Lys Gln Arg Cys Val Thr Tyr Lys Arg\\n'\n",
      " ' 1               5                  10                  15\\n'\n",
      " 'Gly Xaa\\n'\n",
      " '</s400>\\n'\n",
      " '<s200>\\n'\n",
      " '<s210>6</s210>\\n'\n",
      " '<s211>18</s211>\\n'\n",
      " '<s212>PRT</s212>\\n'\n",
      " '<s213>Artificial Sequence</s213>\\n'\n",
      " '<s220>\\n'\n",
      " '<s223>Synthesized sequence</s223>\\n'\n",
      " '</s220>\\n'\n",
      " '</s200>\\n'\n",
      " '<s400> 6\\n'\n",
      " 'Xaa Asp Arg Arg Leu Cys Tyr Lys Gln Arg Cys Val Thr Tyr Xaa Arg\\n'\n",
      " ' 1               5                  10                  15\\n'\n",
      " 'Gly Xaa\\n'\n",
      " '</s400>\\n'\n",
      " '<s200>\\n'\n",
      " '<s210>7</s210>\\n'\n",
      " '<s211>18</s211>\\n'\n",
      " '<s212>PRT</s212>\\n'\n",
      " '<s213>Artificial Sequence</s213>\\n'\n",
      " '<s220>\\n'\n",
      " '<s223>Synthesized sequence</s223>\\n'\n",
      " '</s220>\\n'\n",
      " '</s200>\\n'\n",
      " '<s400> 7\\n'\n",
      " 'Xaa Asp Arg Arg Leu Cys Tyr Lys Gln Arg Cys Val Thr Tyr Xaa Arg\\n'\n",
      " ' 1               5                  10                  15\\n'\n",
      " 'Gly Xaa\\n'\n",
      " '</s400>\\n'\n",
      " '</sequence-list-new-rules>\\n'\n",
      " '</sequence-cwu>\\n')\n"
     ]
    }
   ],
   "source": [
    "import pprint as pp\n",
    "pp.pprint(merged[merged.patentnumber == \"7723468\"].xml.values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found_heading: 5386, not_found_heading: 2661, gibberish: 1731,too_short: 222\n",
      "5000 patents MAE: 0.0, total_error: 0, highest_difference: 0, highest_difference_patent: \n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def extract_examples_start_w_word(xml_siblings):\n",
    "    examples = []\n",
    "    current_example = None\n",
    "    in_example = False\n",
    "\n",
    "    for tag in xml_siblings:\n",
    "        if tag.name == \"heading\":\n",
    "            if (\n",
    "                tag.text.strip().lower().startswith(\"example\")\n",
    "                or tag.text.strip().lower().startswith(\"experiment\")\n",
    "                or tag.text.strip().lower().startswith(\"test\")\n",
    "                or tag.text.strip().lower().startswith(\"trial\")\n",
    "                or \"test\" in tag.text.strip().lower()\n",
    "                or \"experiment\" in tag.text.strip().lower()\n",
    "                or \"example\" in tag.text.strip().lower()\n",
    "                or \"trial\" in tag.text.strip().lower()\n",
    "            ):\n",
    "                in_example = True\n",
    "                current_example = {\n",
    "                    \"number\": tag.text.strip(),\n",
    "                    \"title\": xml_siblings[xml_siblings.index(tag) + 1].text.strip(),\n",
    "                    \"content\": [],\n",
    "                }\n",
    "                examples.append(current_example)\n",
    "        elif tag.name == \"heading\" and (\n",
    "            tag.text.strip().lower().startswith(\"example\")\n",
    "            or tag.text.strip().lower().startswith(\"experiment\")\n",
    "            or tag.text.strip().lower().startswith(\"test\")\n",
    "            or tag.text.strip().lower().startswith(\"trial\")\n",
    "            or \"test\" in tag.text.strip().lower()\n",
    "            or \"experiment\" in tag.text.strip().lower()\n",
    "            or \"example\" in tag.text.strip().lower()\n",
    "            or \"trial\" in tag.text.strip().lower()\n",
    "        ):\n",
    "            in_example = False\n",
    "        # else:\n",
    "        #     # If we hit any other heading, stop collecting content\n",
    "        #     in_example = False\n",
    "        elif in_example and current_example is not None:\n",
    "            current_example[\"content\"].append(tag.text.strip())\n",
    "\n",
    "    return examples\n",
    "num_of_paterns = 5000\n",
    "mae = 0\n",
    "highest_difference = 0\n",
    "found_heading = 0\n",
    "not_found_heading = 0\n",
    "gib = 0\n",
    "short = 0\n",
    "mostdifss = []\n",
    "doc_w_exp = {}\n",
    "\n",
    "for row in df[:10000].iterrows():\n",
    "    xml = row[1][\"xml\"]\n",
    "    if len(xml)>2000:\n",
    "        s_tags = re.findall(r'<s\\d+>.*?</s\\d+>', xml)\n",
    "        if len(s_tags) > 0:\n",
    "            #print(f\"Patent {row[1]['patentnumber']} is gibberish\")\n",
    "            gib+=1\n",
    "        else:\n",
    "            heading = extract_experiments_w_heading(xml)\n",
    "            #janetsnumexamples = row[1][\"prophetic\"] + row[1][\"nonprophetic\"]\n",
    "\n",
    "            if heading:\n",
    "                found_heading += 1\n",
    "                examples = extract_examples_start_w_word(heading[0].find_next_siblings())\n",
    "                if len(examples)==0:\n",
    "                    soup = BeautifulSoup(xml, 'xml')\n",
    "                    siblings = soup.find_all(['heading', 'p'])\n",
    "                    examples = extract_examples_start_w_word(siblings)\n",
    "                numexamples = len(examples)\n",
    "            else:\n",
    "                not_found_heading += 1\n",
    "                soup = BeautifulSoup(xml, 'xml')\n",
    "                siblings = soup.find_all(['heading', 'p'])\n",
    "                examples = extract_examples_start_w_word(siblings)\n",
    "                numexamples = len(examples)\n",
    "\n",
    "\n",
    "\n",
    "            # #difference = abs(numexamples-janetsnumexamples)\n",
    "            # mae += difference\n",
    "            # if difference > 0:\n",
    "            #     mostdifss.append([difference, row[1][\"patentnumber\"]])\n",
    "            # if difference > highest_difference:\n",
    "            #     highest_difference = difference\n",
    "            #     highest_difference_patent = row[1][\"patentnumber\"]\n",
    "            if len(examples)>0:\n",
    "                doc_w_exp[row[1][\"patentnumber\"]] = examples\n",
    "\n",
    "    else:\n",
    "        short+=1\n",
    "        #print(f\"skipping {row[1]['patentnumber']}, patent is too short\")\n",
    "        \n",
    "        \n",
    "print(f\"found_heading: {found_heading}, not_found_heading: {not_found_heading}, gibberish: {gib},too_short: {short}\")\n",
    "print(f\"{num_of_paterns} patents MAE: {mae/num_of_paterns}, total_error: {mae}, highest_difference: {highest_difference}, highest_difference_patent: \") #highest_difference_patent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discovered prefixes:\n",
      "content_number\n",
      "example         82.320069\n",
      "comparative      4.013693\n",
      "examples         2.527713\n",
      "reference        2.359714\n",
      "synthesis        1.443805\n",
      "preparation      1.299003\n",
      "production       0.533519\n",
      "experimental     0.504699\n",
      "industrial       0.484314\n",
      "test             0.409101\n",
      "preparative      0.228450\n",
      "synthetic        0.205957\n",
      "formulation      0.179948\n",
      "intermediate     0.103330\n",
      "biological       0.098409\n",
      "application      0.090677\n",
      "experiment       0.075916\n",
      "Name: count, dtype: float64\n",
      "\n",
      "Discovered special phrases:\n",
      "['comparative example 1', 'comparative example 2', 'comparative example 3', 'comparative example 4', 'comparative example 5', 'comparative example 6', 'reference example 1', 'comparative example 7', 'synthesis example 1', 'synthesis example 2', 'reference example 2', 'comparative example 8', 'synthesis example 3', 'test example 1', 'preparation example 1', 'comparative example 9', 'reference example 3', 'synthesis example 4', 'experimental example 1', 'test example 2', 'preparation example 2', 'production example 1', 'synthesis example 5', 'comparative example 10', 'experimental example 2', 'reference example 4', 'synthesis example 6', 'test example 3', 'preparation example 3', 'reference example 5', 'comparative example 11', 'preparation example 4', 'synthesis example 7', 'experimental example 3', 'comparative example a']\n"
     ]
    }
   ],
   "source": [
    "a = []\n",
    "num_examples = 0\n",
    "for k, pat_exs in doc_w_exp.items():\n",
    "    num_examples += len(pat_exs)\n",
    "    for ex in pat_exs:\n",
    "        ola = \"\".join(ex[\"content\"]) + ex[\"title\"]\n",
    "        if len(ola) == 0:\n",
    "            #print(ex)\n",
    "            continue\n",
    "        a.append({\n",
    "            'patent_number': k,\n",
    "            'content_number': ex[\"number\"]\n",
    "        })\n",
    "df_ex_types = pd.DataFrame(a)\n",
    "val_counts= df_ex_types.content_number.value_counts()\n",
    "valc_df = pd.DataFrame(val_counts).reset_index()\n",
    "\n",
    "# Group by type and sum counts\n",
    "def get_example_type(content_number):\n",
    "    content_lower = content_number.lower()\n",
    "    for prefix in start_w:\n",
    "        if content_lower.startswith(prefix):\n",
    "            return prefix\n",
    "    for special in ls:\n",
    "        if special in content_lower:\n",
    "            return special\n",
    "    return \"other\"\n",
    "\n",
    "# Extract first word from each content_number\n",
    "def get_first_word(text):\n",
    "    return text.lower().split()[0]\n",
    "\n",
    "# Get common prefixes (appearing more than 100 times)\n",
    "common_prefixes = (df_ex_types['content_number']\n",
    "                  .apply(get_first_word)\n",
    "                  .value_counts()\n",
    "                  .loc[lambda x: x > 100])\n",
    "\n",
    "start_w = common_prefixes.index.tolist()\n",
    "print(\"Discovered prefixes:\")\n",
    "print(common_prefixes/df_ex_types.shape[0] * 100)\n",
    "\n",
    "# Function to detect patterns that occur in middle of text\n",
    "def find_common_phrases(df, min_count=50):\n",
    "    # Get all content numbers as lowercase\n",
    "    texts = df['content_number'].str.lower()\n",
    "    # Find phrases with 2 or more words that appear frequently\n",
    "    phrases = texts[texts.str.contains(' .+ ')]  # Contains at least 2 spaces\n",
    "    common_phrases = phrases.value_counts().loc[lambda x: x > min_count]\n",
    "    return common_phrases.index.tolist()\n",
    "\n",
    "ls = find_common_phrases(df_ex_types)\n",
    "print(\"\\nDiscovered special phrases:\")\n",
    "print(ls)\n",
    "\n",
    "# Rest of your grouping code remains the same\n",
    "valc_df['type'] = valc_df['content_number'].apply(get_example_type)\n",
    "type_counts = valc_df.groupby('type')['count'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For 10000 patents in 2015:\n",
    "\n",
    "# ---------------\n",
    "# Starts with\n",
    "# comparative example :  1035\n",
    "# example example :  6682\n",
    "# reference example :  158\n",
    "# synthesis example :  156\n",
    "# test example :  103\n",
    "# ---------------\n",
    "# Ohter\n",
    "# industrial applicability :  663"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Unique content numbers: 23329\n",
      "\n",
      "Content number frequencies:\n",
      "content_number\n",
      "Example 1                       6682\n",
      "Example 2                       6024\n",
      "Example 3                       5260\n",
      "Example 4                       4561\n",
      "Example 5                       3855\n",
      "                                ... \n",
      "Example #N.1.1                     1\n",
      "Example #O.1.1                     1\n",
      "Example #X.1.1                     1\n",
      "Example #38                        1\n",
      "VI. Industrial Applicability       1\n",
      "Name: count, Length: 23329, dtype: int64\n",
      "\n",
      "Unique patent numbers: 7965\n",
      "\n",
      "Patent number frequencies:\n",
      "patent_number\n",
      "8952157    1446\n",
      "8987441    1040\n",
      "8969587     996\n",
      "8987242     901\n",
      "9006265     800\n",
      "           ... \n",
      "9034169       1\n",
      "8956508       1\n",
      "8956528       1\n",
      "8956536       1\n",
      "8986522       1\n",
      "Name: count, Length: 7965, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nUnique content numbers:\", len(df_ex_types['content_number'].unique()))\n",
    "print(\"\\nContent number frequencies:\")\n",
    "print(df_ex_types['content_number'].value_counts())\n",
    "\n",
    "print(\"\\nUnique patent numbers:\", len(df_ex_types['patent_number'].unique()))\n",
    "print(\"\\nPatent number frequencies:\")\n",
    "print(df_ex_types['patent_number'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INDUSTRIAL APPLICABILITY  :  30\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of patents with difference > 0: 104\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[4.0, '8926548'],\n",
       " [1.0, '8926595'],\n",
       " [1.0, '8926732'],\n",
       " [1.0, '8926824'],\n",
       " [6.0, '8926862'],\n",
       " [4.0, '8926864'],\n",
       " [1.0, '8926872'],\n",
       " [1.0, '8926955'],\n",
       " [11.0, '8926966'],\n",
       " [7.0, '8926979'],\n",
       " [1.0, '8927015'],\n",
       " [1.0, '8927023'],\n",
       " [5.0, '8927036'],\n",
       " [2.0, '8927098'],\n",
       " [1.0, '8927150'],\n",
       " [2.0, '8927157'],\n",
       " [1.0, '8927236'],\n",
       " [4.0, '8927254'],\n",
       " [1.0, '8927429'],\n",
       " [1.0, '8927469'],\n",
       " [1.0, '8927479'],\n",
       " [1.0, '8927536'],\n",
       " [1.0, '8927541'],\n",
       " [1.0, '8927546'],\n",
       " [2.0, '8927568'],\n",
       " [4.0, '8927583'],\n",
       " [6.0, '8927588'],\n",
       " [3.0, '8927596'],\n",
       " [1.0, '8927617'],\n",
       " [1.0, '8927619'],\n",
       " [2.0, '8927631'],\n",
       " [1.0, '8927642'],\n",
       " [1.0, '8927647'],\n",
       " [1.0, '8927678'],\n",
       " [1.0, '8927681'],\n",
       " [2.0, '8927683'],\n",
       " [1.0, '8927698'],\n",
       " [7.0, '8927710'],\n",
       " [15.0, '8927721'],\n",
       " [1.0, '8927725'],\n",
       " [1.0, '8927730'],\n",
       " [1.0, '8927738'],\n",
       " [1.0, '8927746'],\n",
       " [1.0, '8927750'],\n",
       " [1.0, '8927777'],\n",
       " [1.0, '8927781'],\n",
       " [1.0, '8927801'],\n",
       " [9.0, '8927811'],\n",
       " [1.0, '8932406'],\n",
       " [3.0, '8932470'],\n",
       " [4.0, '8932480'],\n",
       " [2.0, '8932491'],\n",
       " [2.0, '8932492'],\n",
       " [1.0, '8932579'],\n",
       " [3.0, '8932591'],\n",
       " [1.0, '8932615'],\n",
       " [2.0, '8932626'],\n",
       " [2.0, '8932688'],\n",
       " [4.0, '8932705'],\n",
       " [6.0, '8932717'],\n",
       " [1.0, '8932842'],\n",
       " [1.0, '8932853'],\n",
       " [2.0, '8932867'],\n",
       " [1.0, '8932981'],\n",
       " [1.0, '8933000'],\n",
       " [2.0, '8933012'],\n",
       " [23.0, '8933032'],\n",
       " [12.0, '8933042'],\n",
       " [2.0, '8933065'],\n",
       " [1.0, '8933067'],\n",
       " [1.0, '8933071'],\n",
       " [56.0, '8933072'],\n",
       " [1.0, '8933074'],\n",
       " [1.0, '8933094'],\n",
       " [3.0, '8933100'],\n",
       " [2.0, '8933103'],\n",
       " [3.0, '8933105'],\n",
       " [2.0, '8933117'],\n",
       " [15.0, '8933131'],\n",
       " [1.0, '8933135'],\n",
       " [1.0, '8933181'],\n",
       " [1.0, '8933204'],\n",
       " [1.0, '8933221'],\n",
       " [2.0, '8933224'],\n",
       " [1.0, '8933228'],\n",
       " [6.0, '8933229'],\n",
       " [1.0, '8933251'],\n",
       " [1.0, '8933267'],\n",
       " [1.0, '8933294'],\n",
       " [3.0, '8933318'],\n",
       " [1.0, '8934216'],\n",
       " [1.0, '8934637'],\n",
       " [1.0, '8936675'],\n",
       " [1.0, '8936685'],\n",
       " [1.0, '8936706'],\n",
       " [1.0, '8936833'],\n",
       " [1.0, '8936836'],\n",
       " [3.0, '8936849'],\n",
       " [3.0, '8936859'],\n",
       " [3.0, '8936910'],\n",
       " [1.0, '8936939'],\n",
       " [1.0, '8937025'],\n",
       " [1.0, '8937045'],\n",
       " [6.0, '8937055']]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"Number of patents with difference > 0: {len(mostdifss)}\")\n",
    "mostdifss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patentnumber</th>\n",
       "      <th>prophetic</th>\n",
       "      <th>nonprophetic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8285</th>\n",
       "      <td>8933072</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     patentnumber  prophetic  nonprophetic\n",
       "8285      8933072        0.0           9.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# highest difference\n",
    "df2[df2.patentnumber == \"8933072\"][[\"patentnumber\",\"prophetic\",\"nonprophetic\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2897131579.py, line 34)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[16], line 34\u001b[1;36m\u001b[0m\n\u001b[1;33m    Number of exact matches: 2870 out of 3971 , Percentage: 72.27398640141023\u001b[0m\n\u001b[1;37m           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "final = dic_to_dic_w_tense_test(doc_w_exp,threshold=0)\n",
    "df_final = pd.DataFrame(final).T.reset_index()\n",
    "df_final.columns = [\"patentnumber\",\"past\",\"present\",\"Unknown\"]\n",
    "df_final[\"patentnumber\"] = df_final[\"patentnumber\"].apply(remove_leadiong_zeros)\n",
    "df_check = df_final.merge(merged,on= \"patentnumber\",how=\"left\")[[\"patentnumber\",\"present\",\"past\",\"Unknown\",\"prophetic\",\"nonprophetic\",\"allprophetic\",\"someprophetic\"]]\n",
    "#df_check[\"past\"] = df_check[\"past\"] + df_check[\"Unknown\"]\n",
    "df_check[\"Total_Extracted\"] = df_check[\"past\"] + df_check[\"present\"]  #+ df_check[\"Unknown\"]\n",
    "df_check[\"Total_Freilich\"] = df_check[\"prophetic\"] + df_check[\"nonprophetic\"]\n",
    "df_check[\"prophetic_error\"] = np.sqrt((df_check[\"prophetic\"] - df_check[\"present\"])**2)\n",
    "df_check[\"nonprophetic_error\"] = np.sqrt((df_check[\"nonprophetic\"] - df_check[\"past\"])**2)\n",
    "df_check[\"Total_Mean_error\"] = np.sqrt((df_check[\"Total_Freilich\"] - df_check[\"Total_Extracted\"])**2)\n",
    "df_check[\"Sum_error\"] = df_check[\"prophetic_error\"] + df_check[\"nonprophetic_error\"] + df_check[\"Total_Mean_error\"]\n",
    "print(f\"Number of exact matches: {len(df_check[(df_check.prophetic_error ==0) & (df_check.nonprophetic_error ==0) & (df_check.Total_Mean_error ==0)])} out of {len(df_check)} , Percentage: {len(df_check[(df_check.prophetic_error ==0) & (df_check.nonprophetic_error ==0) & (df_check.Total_Mean_error ==0)])/len(df_check)*100}\")\n",
    "print(f\"Number of exact num of patent extracted: {len(df_check[(df_check.Total_Extracted == df_check.Total_Freilich)])} out of {len(df_check)}\")\n",
    "print(f\"Avg Total error: {df_check['Total_Mean_error'].mean()}, num of corrects:  {df_check[df_check['Total_Mean_error'] == 0].shape[0]}\")\n",
    "print(f\"Avg Total prophetic error: {df_check['prophetic_error'].mean()}, num of corrects: {df_check[df_check['prophetic_error'] == 0].shape[0]}\")\n",
    "print(f\"Avg Total nonprophetic error: {df_check['nonprophetic_error'].mean()}, num of corrects: {df_check[df_check['nonprophetic_error'] == 0].shape[0]}\")\n",
    "print(f\"{df_check['Sum_error'].sum()}\")\n",
    "print(f\"Sum of number of Unknowns: {df_check['Unknown'].sum()}\")\n",
    "print(f\"Number of patets with experiments extracted {len(df_check)}\")\n",
    "df_check.sort_values(\"Sum_error\",ascending=False).head(20)\n",
    "\n",
    "\n",
    "# 1000\n",
    "# Number of exact matches: 577 out of 799 , Percentage: 72.21526908635795\n",
    "# Number of exact num of patent extracted: 694 out of 799\n",
    "# Avg Total error: 0.5181476846057572, num of corrects:  694\n",
    "# Avg Total prophetic error: 0.5857321652065082, num of corrects: 652\n",
    "# Avg Total nonprophetic error: 0.9887359198998749, num of corrects: 587\n",
    "# 1672.0\n",
    "# Sum of number of Unknowns: 0\n",
    "# Number of patets with experiments extracted 799\n",
    "\n",
    "# 5000\n",
    "# Number of exact matches: 2870 out of 3971 , Percentage: 72.27398640141023\n",
    "# Number of exact num of patent extracted: 3461 out of 3971\n",
    "# Avg Total error: 0.5560312263913372, num of corrects:  3461\n",
    "# Avg Total prophetic error: 0.8645177537144296, num of corrects: 3209\n",
    "# Avg Total nonprophetic error: 1.2976580206497104, num of corrects: 2942\n",
    "# 10794.0\n",
    "# Sum of number of Unknowns: 0\n",
    "# Number of patets with experiments extracted 3971"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# patent 9102614 injanets says 4 examples but there are 161\n",
    "# patent 9102599 in janets says 1 ,theres more than 1 actually \n",
    "# Some xml files are made with error : e.g. these dont have much information : skipping 9102601, 9102662,9102692\n",
    "# these have gibberish: 9102724,9102705,9102727,9102628"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_siblings_test(xml_siblings):\n",
    "    examples = []\n",
    "\n",
    "    # Find all matching headings directly from xml_siblings\n",
    "    example_headings = [\n",
    "        tag\n",
    "        for tag in xml_siblings\n",
    "        if tag.name == \"heading\"\n",
    "        and any(\n",
    "            keyword in tag.text.strip().lower().replace(\" \", \"\")\n",
    "            for keyword in [\"example\", \"experiment\", \"test\"]\n",
    "        )\n",
    "        # and not any(\n",
    "        #     excluded in tag.text.strip().lower().replace(\" \", \"\")\n",
    "        #     for excluded in [\"reference\", \"preparation\"]\n",
    "        # )\n",
    "    ]\n",
    "\n",
    "    for heading in example_headings:\n",
    "        current_content = []\n",
    "        idx = xml_siblings.index(heading)\n",
    "\n",
    "        # Get title from next heading if available\n",
    "        title = \"\"\n",
    "        if idx + 1 < len(xml_siblings) and xml_siblings[idx + 1].name == \"heading\":\n",
    "            title = xml_siblings[idx + 1].text.strip()\n",
    "\n",
    "        # Collect content until next example heading\n",
    "        i = idx + 1\n",
    "        while i < len(xml_siblings):\n",
    "            if (\n",
    "                xml_siblings[i].name == \"heading\"\n",
    "                and any(\n",
    "                    keyword in xml_siblings[i].text.strip().lower().replace(\" \", \"\")\n",
    "                    for keyword in [\"example\", \"experiment\", \"test\"]\n",
    "                )\n",
    "                # and not any(\n",
    "                #     excluded in xml_siblings[i].text.strip().lower().replace(\" \", \"\")\n",
    "                #     for excluded in [\"reference\", \"preparation\"]\n",
    "                # )\n",
    "            ):\n",
    "                break\n",
    "            if xml_siblings[i].name == \"p\":\n",
    "                current_content.append(xml_siblings[i].text.strip())\n",
    "            i += 1\n",
    "\n",
    "        examples.append(\n",
    "            {\"number\": heading.text.strip(), \"title\": title, \"content\": current_content}\n",
    "        )\n",
    "\n",
    "    return examples if examples else None\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "has more than 1 Examples heading, 349\n",
      "has more than 1 Examples heading, 388\n",
      "has more than 1 Examples heading, 689\n",
      "has more than 1 Examples heading, 768\n",
      "has more than 1 Examples heading, 781\n",
      "has more than 1 Examples heading, 814\n",
      "1000/25081 so far found 771 docs with experiments\n"
     ]
    }
   ],
   "source": [
    "def extract_examples_from_heading(heading):\n",
    "    extracted_examples = []\n",
    "    example_start_w = process_siblings(heading.find_next_siblings())\n",
    "    if example_start_w:\n",
    "        if not example_start_w[0][\"content\"]:\n",
    "            extracted_ex_w_word = extract_examples_start_w_word(heading.find_next_siblings())\n",
    "            if extracted_ex_w_word:\n",
    "                if isinstance(extracted_ex_w_word, list):\n",
    "                    if extracted_ex_w_word and len(extracted_ex_w_word[0][\"content\"])>0:\n",
    "                        extracted_examples.append(extracted_ex_w_word)\n",
    "                elif extracted_ex_w_word[\"content\"]:\n",
    "                    extracted_examples.append(extracted_ex_w_word)\n",
    "        else:\n",
    "            if len(example_start_w[0][\"content\"])>0:\n",
    "                extracted_examples.append(example_start_w)\n",
    "    else:\n",
    "        extracted_ex_w_word = extract_examples_start_w_word(heading.find_next_siblings())\n",
    "        if extracted_ex_w_word:\n",
    "            if isinstance(extracted_ex_w_word, list):\n",
    "                if extracted_ex_w_word and len(extracted_ex_w_word[0][\"content\"])>0:\n",
    "                    extracted_examples.append(extracted_ex_w_word)\n",
    "            elif extracted_ex_w_word[\"content\"]:\n",
    "                if len(extracted_ex_w_word[\"content\"])>0:\n",
    "                    extracted_examples.append(extracted_ex_w_word)\n",
    "        else:\n",
    "            num_dot_examples = extract_num_dot_examples(str(heading.find_next_siblings()))\n",
    "            if num_dot_examples:\n",
    "                extracted_examples.append(num_dot_examples)\n",
    "    return extracted_examples\n",
    "\n",
    "def extract_examples(xml):\n",
    "    heading = extract_experiments_w_heading(xml)\n",
    "    if heading:\n",
    "        if len(heading) > 1:\n",
    "            print(f\"has more than 1 Examples heading, {i}\")\n",
    "        elif len(heading) == 1:\n",
    "            return extract_examples_from_heading(heading[0])\n",
    "        else:\n",
    "            extracted_ex_w_word = extract_examples_w_word(xml)\n",
    "            if extracted_ex_w_word:\n",
    "                if isinstance(extracted_ex_w_word, list):\n",
    "                    if extracted_ex_w_word and extracted_ex_w_word[0][\"content\"]:\n",
    "                        return [extracted_ex_w_word]\n",
    "                elif extracted_ex_w_word[\"content\"]:\n",
    "                    return [extracted_ex_w_word]\n",
    "            else:\n",
    "                example_start_w = process_siblings(heading[0].find_next_siblings())\n",
    "                if example_start_w and example_start_w[0][\"content\"]:\n",
    "                    return [example_start_w]\n",
    "    else: \n",
    "        example_start_w = extract_examples_w_word(xml)\n",
    "        if example_start_w:\n",
    "            return [example_start_w]\n",
    "        else:\n",
    "            num_dot_examples = extract_num_dot_examples(xml)\n",
    "            if num_dot_examples:\n",
    "                return [num_dot_examples]\n",
    "    return []\n",
    "\n",
    "doc_w_exp = {}\n",
    "for i, xml in enumerate(merged[\"xml\"][:1000], start=1):\n",
    "    if i % 1000 == 0:\n",
    "        print(f\"{i}/{len(merged.xml)} so far found {len(doc_w_exp)} docs with experiments\")\n",
    "    \n",
    "    extracted_examples = extract_examples(xml)\n",
    "    if extracted_examples:\n",
    "        doc_w_exp[find_doc_number(xml)[0]] = extracted_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/25081 so far found 54 docs with experiments\n",
      "200/25081 so far found 111 docs with experiments\n",
      "300/25081 so far found 169 docs with experiments\n",
      "has more than 1 Examples heading, 349\n",
      "has more than 1 Examples heading, 388\n",
      "400/25081 so far found 225 docs with experiments\n",
      "500/25081 so far found 294 docs with experiments\n",
      "600/25081 so far found 352 docs with experiments\n",
      "has more than 1 Examples heading, 689\n",
      "700/25081 so far found 395 docs with experiments\n",
      "has more than 1 Examples heading, 768\n",
      "has more than 1 Examples heading, 781\n",
      "800/25081 so far found 463 docs with experiments\n",
      "has more than 1 Examples heading, 814\n",
      "900/25081 so far found 522 docs with experiments\n",
      "1000/25081 so far found 575 docs with experiments\n"
     ]
    }
   ],
   "source": [
    "def extract_examples_from_heading(heading):\n",
    "    length_threshold = 30\n",
    "    extracted_examples = []\n",
    "    example_start_w = process_siblings(heading.find_next_siblings())\n",
    "    if example_start_w:\n",
    "        if not example_start_w[0][\"content\"]:\n",
    "            extracted_ex_w_word = extract_examples_start_w_word(heading.find_next_siblings())\n",
    "            if extracted_ex_w_word:\n",
    "                if isinstance(extracted_ex_w_word, list):\n",
    "                    extracted_ex_w_word = [ex for ex in extracted_ex_w_word if len(ex[\"content\"])>length_threshold]\n",
    "                    extracted_examples.append(extracted_ex_w_word)\n",
    "                elif extracted_ex_w_word[\"content\"]>length_threshold:\n",
    "                    extracted_examples.append(extracted_ex_w_word)\n",
    "        else:\n",
    "            if isinstance(example_start_w, list):\n",
    "                example_start_w = [ex for ex in example_start_w if len(ex[\"content\"])>length_threshold]\n",
    "                extracted_examples.append(example_start_w)\n",
    "    else:\n",
    "        extracted_ex_w_word = extract_examples_start_w_word(heading.find_next_siblings())\n",
    "        if extracted_ex_w_word:\n",
    "            if isinstance(extracted_ex_w_word, list):\n",
    "                extracted_ex_w_word = [ex for ex in extracted_ex_w_word if len(ex[\"content\"])>length_threshold]\n",
    "                extracted_examples.append(extracted_ex_w_word)\n",
    "            elif len(extracted_ex_w_word[\"content\"])>length_threshold:\n",
    "                extracted_examples.append(extracted_ex_w_word)\n",
    "        else:\n",
    "            num_dot_examples = extract_num_dot_examples(str(heading.find_next_siblings()))\n",
    "            if isinstance(num_dot_examples, list):\n",
    "                num_dot_examples = [ex for ex in num_dot_examples if len(ex[\"content\"])>length_threshold]\n",
    "                extracted_examples.append(num_dot_examples)\n",
    "            elif isinstance(num_dot_examples, dict):\n",
    "                num_dot_examples = [ex for ex in num_dot_examples.items() if len(ex[\"content\"])>length_threshold]\n",
    "\n",
    "                extracted_examples.append(num_dot_examples)\n",
    "    return extracted_examples\n",
    "\n",
    "def extract_examples(xml):\n",
    "    length_threshold = 30\n",
    "    heading = extract_experiments_w_heading(xml)\n",
    "    if heading:\n",
    "        if len(heading) > 1:\n",
    "            print(f\"has more than 1 Examples heading, {i}\")\n",
    "        elif len(heading) == 1:\n",
    "            return extract_examples_from_heading(heading[0])\n",
    "        else:\n",
    "            pass\n",
    "        # else:\n",
    "        #     extracted_ex_w_word = extract_examples_w_word(xml)\n",
    "        #     if extracted_ex_w_word:\n",
    "        #         if isinstance(extracted_ex_w_word, list):\n",
    "        #             extracted_ex_w_word = [ex for ex in extracted_ex_w_word if len(ex[\"content\"])>length_threshold]\n",
    "        #             return extracted_ex_w_word if extracted_ex_w_word\n",
    "        #         elif extracted_ex_w_word[\"content\"]>length_threshold:\n",
    "        #             return extracted_ex_w_word if extracted_ex_w_word\n",
    "        #     else:\n",
    "        #         example_start_w = process_siblings(heading[0].find_next_siblings())\n",
    "        #         if isinstance(example_start_w, list):\n",
    "        #             example_start_w = [ex for ex in example_start_w if len(ex[\"content\"])>length_threshold]\n",
    "        #             return example_start_w if example_start_w\n",
    "    else: \n",
    "        example_start_w = extract_examples_w_word(xml)\n",
    "        if isinstance(example_start_w, list):\n",
    "            example_start_w = [ex for ex in example_start_w if len(ex[\"content\"])>length_threshold]\n",
    "            return example_start_w\n",
    "        else:\n",
    "            num_dot_examples = extract_num_dot_examples(xml)\n",
    "            if isinstance(num_dot_examples, list):\n",
    "                num_dot_examples = [ex for ex in num_dot_examples if len(ex[\"content\"])>length_threshold]\n",
    "                return num_dot_examples\n",
    "\n",
    "doc_w_exp = {}\n",
    "for i, xml in enumerate(merged[\"xml\"][:1000], start=1):\n",
    "    if i % 100 == 0:\n",
    "        print(f\"{i}/{len(merged.xml)} so far found {len(doc_w_exp)} docs with experiments\")\n",
    "    \n",
    "    extracted_examples = extract_examples(xml)\n",
    "    if extracted_examples:\n",
    "        doc_w_exp[find_doc_number(xml)[0]] = extracted_examples\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "has more than 1 Examples heading, 349\n",
      "has more than 1 Examples heading, 388\n",
      "has more than 1 Examples heading, 689\n",
      "has more than 1 Examples heading, 768\n",
      "has more than 1 Examples heading, 781\n",
      "has more than 1 Examples heading, 814\n",
      "1000/25081 so far found 769 docs with experiments\n",
      "has more than 1 Examples heading, 1054\n",
      "has more than 1 Examples heading, 1417\n",
      "has more than 1 Examples heading, 1475\n",
      "has more than 1 Examples heading, 1810\n"
     ]
    }
   ],
   "source": [
    "\n",
    "doc_w_exp = {}\n",
    "for i, xml in enumerate(merged[:1999].xml.values, start=1):\n",
    "    if i % 1000 == 0:\n",
    "        print(f\"{i}/{len(merged.xml)} so far found {len(doc_w_exp)} docs with experiments\")\n",
    "    \n",
    "    extracted_examples = extract_examples(xml)\n",
    "    if extracted_examples:\n",
    "        doc_w_exp[find_doc_number(xml)[0]] = extracted_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 18977 patents from doc_w_exp_new_algorithm.pkl\n"
     ]
    }
   ],
   "source": [
    "# save_as_pickle(doc_w_exp,\"doc_w_exp_new_algorithm.pkl\")\n",
    "doc_w_exp = load_from_pickle(\"doc_w_exp_new_algorithm.pkl\")\n",
    "\n",
    "# save_as_json(doc_w_exp, \"doc_w_exp_2015.json\")\n",
    "# doc_w_exp = read_json(\"../data/doc_w_exp_2015.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import html\n",
    "import unicodedata\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def clean_text_updated(text):\n",
    "    \"\"\"\n",
    "    Clean text by removing HTML tags, special characters, extra spaces,\n",
    "    and normalizing the content while keeping meaningful punctuation.\n",
    "\n",
    "    Args:\n",
    "        text (str): Input text to clean\n",
    "\n",
    "    Returns:\n",
    "        str: Cleaned text\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "\n",
    "    # Decode HTML entities & remove HTML tags\n",
    "    text = html.unescape(text)\n",
    "    text = BeautifulSoup(text, \"html.parser\").get_text()\n",
    "\n",
    "    # Normalize Unicode characters (e.g., é → e)\n",
    "    text = unicodedata.normalize(\"NFKC\", text)\n",
    "\n",
    "    # Remove unwanted special characters but keep punctuation\n",
    "    text = re.sub(r\"[^\\w\\s.,!?'\\-]\", \"\", text)\n",
    "\n",
    "    # Normalize spaces: remove multiple spaces, newlines, and tabs\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "def check_tense_nltk_updated(sentence):\n",
    "    words = word_tokenize(sentence)\n",
    "    tagged = pos_tag(words)\n",
    "\n",
    "    # Define tense categories\n",
    "    past = {\"VBD\", \"VBN\"}\n",
    "    present = {\"VB\", \"VBG\", \"VBP\", \"VBZ\"}\n",
    "    future = {\"MD\"}\n",
    "\n",
    "    tenses = {\"past\": 0, \"present\": 0, \"future\": 0}\n",
    "\n",
    "    for i, (word, tag) in enumerate(tagged):\n",
    "        # Count past tense words\n",
    "        if tag in past:\n",
    "            tenses[\"past\"] += 1\n",
    "\n",
    "        # Count present tense words\n",
    "        elif tag in present:\n",
    "            tenses[\"present\"] += 1\n",
    "\n",
    "        # Future tense handling\n",
    "        elif tag in future:\n",
    "            if word.lower() in {\"will\", \"shall\"}:\n",
    "                # Ensure 'will' or 'shall' is followed by a verb (to confirm future tense)\n",
    "                if i + 1 < len(tagged) and tagged[i + 1][1] in {\"VB\", \"VBP\"}:\n",
    "                    tenses[\"future\"] += 1\n",
    "\n",
    "    return max(tenses, key=tenses.get) if max(tenses.values()) > 0 else \"Unknown\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from utilities.nlp_processing import check_tense_nltk,clean_text\n",
    "\n",
    "def tense_with_pattern(description):\n",
    "    future_pattern = r'\\b(?:will|would|shall|should)\\b'\n",
    "    past_pattern = r'\\b(?:was|were|had|did)\\b'\n",
    "\n",
    "    future_matches = re.findall(future_pattern, description)\n",
    "    past_matches = re.findall(past_pattern, description)\n",
    "    if future_matches and past_matches:\n",
    "        if len(future_matches) > len(past_matches):\n",
    "            return \"present\"\n",
    "        else:\n",
    "            return \"past\"\n",
    "    if future_pattern or past_pattern:\n",
    "        if future_pattern:\n",
    "            return \"present\"\n",
    "        else:\n",
    "            return \"past\"\n",
    "    return \"Unkown\"\n",
    "\n",
    "\n",
    "def dic_to_dic_w_tense(doc_w_exp,threshold = 50):\n",
    "    dic = {}\n",
    "    pattern = r'\\(\\d+\\)\\s*([A-Za-z0-9\\-\\(\\)\\{\\},:;=\\[\\]\\+\\*\\s\\.\\^\\$\\%]+(?:\\.(?:sup|delta|Hz|NMR)[^\\)]*)?)'\n",
    "    pattern2 = r'\\(\\d+\\)\\s*(?:[A-Za-z]+\\s*\\d*\\-?[A-Za-z]*[\\(\\{][^)]*[\\)\\}][^;]*|(?:\\.\\d*H\\-?NMR[^;]*|\\d+\\.[a-zA-Z]*\\d*))'\n",
    "\n",
    "\n",
    "    for key, value in doc_w_exp.items():\n",
    "        tense_counts = {\"past\": 0, \"present\": 0,\"Unknown\":0}\n",
    "        \n",
    "        if isinstance(value, list) and len(value) == 1 and len(value[0][0][\"content\"]) > threshold:\n",
    "            desc = value[0][0][\"title\"] + \".\" + \"\".join(value[0][0][\"content\"])\n",
    "            tense = check_tense_nltk_updated(clean_text(desc))\n",
    "            if tense !=  \"Unknown\":\n",
    "                tense_counts[tense] += 1\n",
    "            else:\n",
    "                matches = re.findall(pattern, desc)\n",
    "                if matches:\n",
    "                    tense_counts[\"past\"] += 1\n",
    "                else:\n",
    "                    tense_counts[\"Unknown\"] += 1\n",
    "            dic[key] = tense_counts\n",
    "\n",
    "\n",
    "        elif isinstance(value[0], list) and len(value[0]) > 1:\n",
    "            for ls in value[0]:\n",
    "                if len(ls[\"content\"]) > threshold:\n",
    "                    desc = \"\".join(ls[\"content\"])\n",
    "                    tense = check_tense_nltk_updated(clean_text(desc))\n",
    "                    if tense !=  \"Unknown\":\n",
    "                        tense_counts[tense] += 1\n",
    "                    else:\n",
    "                        matches = re.findall(pattern, desc)\n",
    "                        if matches:\n",
    "                            tense_counts[\"past\"] += 1\n",
    "                        else:\n",
    "                            tense_counts[\"Unknown\"] += 1\n",
    "            dic[key] = tense_counts\n",
    "\n",
    "        elif isinstance(value[0], dict):\n",
    "            for ex, desc in value[0].items():\n",
    "                if len(desc) > threshold:\n",
    "                    tense = check_tense_nltk_updated(clean_text(desc))\n",
    "                    if tense !=  \"Unknown\":\n",
    "                        tense_counts[tense] += 1\n",
    "                    else:\n",
    "                        matches = re.findall(pattern, desc)\n",
    "                        if matches:\n",
    "                            tense_counts[\"past\"] += 1\n",
    "                        else:\n",
    "                            tense_counts[\"Unknown\"] += 1\n",
    "            dic[key] = tense_counts\n",
    "        else:\n",
    "            print(type(value[0]))\n",
    "            print(value[0])\n",
    "\n",
    "    return dic\n",
    "\n",
    "import re\n",
    "\n",
    "def dic_to_dic_w_unknown_tense(doc_w_exp):\n",
    "    dic = {}\n",
    "    pattern = r'\\(\\d+\\)\\s*([A-Za-z0-9\\-\\(\\)\\{\\},:;=\\[\\]\\+\\*\\s\\.\\^\\$\\%]+(?:\\.(?:sup|delta|Hz|NMR)[^\\)]*)?)'\n",
    "    pattern2 = r'\\(\\d+\\)\\s*(?:[A-Za-z]+\\s*\\d*\\-?[A-Za-z]*[\\(\\{][^)]*[\\)\\}][^;]*|(?:\\.\\d*H\\-?NMR[^;]*|\\d+\\.[a-zA-Z]*\\d*))'\n",
    "\n",
    "    for key, value in doc_w_exp.items():\n",
    "        unknown_experiments = []  # Initialize a list to store the experiments classified as Unknown\n",
    "        \n",
    "        if isinstance(value[0], list) and len(value[0]) == 1:\n",
    "            desc = value[0][0][\"title\"] + \".\" + \"\".join(value[0][0][\"content\"])\n",
    "            tense = check_tense_nltk_updated(clean_text(desc))\n",
    "            if tense == \"Unknown\":\n",
    "                unknown_experiments.append(desc)\n",
    "\n",
    "        elif isinstance(value[0], list) and len(value[0]) > 1:\n",
    "            for ls in value[0]:\n",
    "                desc = ls[\"title\"] + \".\" + \"\".join(ls[\"content\"])\n",
    "                tense = check_tense_nltk_updated(clean_text(desc))\n",
    "                if tense == \"Unknown\":\n",
    "                    unknown_experiments.append(desc)\n",
    "\n",
    "        elif isinstance(value[0], dict):\n",
    "            for ex, description in value[0].items():\n",
    "                tense = check_tense_nltk_updated(clean_text(description))\n",
    "                if tense == \"Unknown\":\n",
    "                    unknown_experiments.append(description)\n",
    "\n",
    "        # Only add to the dictionary if there are \"Unknown\" experiments\n",
    "        if unknown_experiments:\n",
    "            dic[key] = unknown_experiments\n",
    "\n",
    "    return dic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1140748136.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [105]\u001b[1;36m\u001b[0m\n\u001b[1;33m    unkown:\u001b[0m\n\u001b[1;37m           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# 8946300 example 1-5\n",
    "# 08946296 example 2 onwards \n",
    "# 08932484 examples 2 to 11\n",
    "\n",
    "\n",
    "# 08933074:  \n",
    "# Synthesis of 5,5-dimethyl-4-(pyridin-4-yl)-3-(4-(quinolin-2-ylmethoxy)phenyl) furan-2(5H)-one (Example 23)\n",
    "# 5,5-dimethyl-4-(pyridin-4-yl)-3-(4-(quinolin-2-ylmethoxy)phenyl) furan-2(5H)-one (Example 23)\n",
    "\n",
    "# 08945522:\n",
    "# Example 2\n",
    "# (69) 3-(3-(Triethoxysilyl)propylamino)phenyl benzoate and\n",
    "# (140) Example 9\n",
    "# (141) 4-(3-(Triethoxysilyl)propylamino)phenyl benzoate and\n",
    "# (142) Example 10\n",
    "# (143) 4-(Bis(3-(triethoxysilyl)propyl)amino)phenyl benzoate\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[8969641,8926966,8951515,8940436,9023999,9011956,9018396,9213027,9108964,9084992]#examples our algorithm didnt find but freilich did \n",
    "# many dont have an overarching examples section but have example 1, example 2 etc\n",
    "# check these xml, the text has examples section:9194008,9018332,9054322,9150561,9213027, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pprint as p\n",
    "# p.pprint(merged[merged.patentnumber == \"9018332\"][\"xml\"].values[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ERRORS IN FREILICH DATASET:\n",
    "patentnumber 8957069: in freilich 108.0\tnonproph and 0 prophetic but there are many more examples than 108, our algorithm: 267 nonprophetic and 1 prophetic and 3 unknown\n",
    "patent number 8946443:we extracted 271\tfreilich: 108, this patent has refrtence examples, are we extracting these or not? \n",
    "patent number 8952010: same issue do we take refrence examples? \n",
    "patent 8933099: there are far more than 33 examples in this patent but freilich says 33, our algorithm found 131\n",
    "what to do with Preparation examples?? e.g 8962612 in freilich has 6 we got 70."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# improvement for our algorithm\n",
    "1. remove duplicates somehow, e.g in patent number 08987242, there some examples are extracted twice just because the name e.g example aa137 appears multiple times.\n",
    "2. better classification of the patents. e.g patent 8987295 has 442, we extract the same number as frelich but we clasify differently.\n",
    "3. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
