{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\amoha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\amoha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from utilities.utils_clean import *\n",
    "from utilities.test_dataset_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#download_patents_pto(start_year=2015, end_year=2015, kind='grant',download_path=\"data\")\n",
    "#unzip_files(\"data\",\"patent_grants_2015\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "test_dataset = create_test_dataset_from_freilich( #\"patent_xmls_2010.json\"\n",
    "    year=2015,  # The year you want to analyze\n",
    "    freilich_data_path=\"Freilich.Data.Compressed.xlsb\",  # Path to your Freilich dataset\n",
    "    path_to_all_xmls_for_chosen_year=\"../app/data/patent_grants_2015\"  # Directory containing XML files\n",
    ")\n",
    "\n",
    "# # Optional: Check the results\n",
    "# print(f\"Number of patents extracted: {len(test_dataset)}\")\n",
    "# print(\"Sample document numbers:\", list(test_dataset.keys())[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved as patent_xmls_2015.json\n"
     ]
    }
   ],
   "source": [
    "save_as_json(test_dataset, \"patent_xmls_2015.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patentnumber</th>\n",
       "      <th>xml</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RE045323</td>\n",
       "      <td>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\\n&lt;!DOCT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RE045324</td>\n",
       "      <td>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\\n&lt;!DOCT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RE045325</td>\n",
       "      <td>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\\n&lt;!DOCT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8925349</td>\n",
       "      <td>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\\n&lt;!DOCT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8925551</td>\n",
       "      <td>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\\n&lt;!DOCT...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  patentnumber                                                xml\n",
       "0     RE045323  <?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n<!DOCT...\n",
       "1     RE045324  <?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n<!DOCT...\n",
       "2     RE045325  <?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n<!DOCT...\n",
       "3      8925349  <?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n<!DOCT...\n",
       "4      8925551  <?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n<!DOCT..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = read_json(\"patent_xmls_2015.json\")\n",
    "df = pd.DataFrame.from_dict(data,orient=\"index\").reset_index()\n",
    "df.columns = [\"patentnumber\",\"xml\"]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "soup = BeautifulSoup(df[df[\"patentnumber\"]==\"8927546\"].xml.values[0], \"xml\")\n",
    "siblings = soup.find_all([\"heading\", \"p\"])\n",
    "examples = extract_examples_start_w_word(siblings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk import pos_tag, word_tokenize\n",
    "def analyze_sentence_tense(text, threshold=0.5):\n",
    "    text = text.replace(\"  \", \"\").replace(\"\\n\", \" \").replace(\"\\t\", \" \")\n",
    "\n",
    "    # Ensure required NLTK data is available\n",
    "    try:\n",
    "        nltk.data.find(\"taggers/averaged_perceptron_tagger\")\n",
    "    except LookupError:\n",
    "        nltk.download(\"averaged_perceptron_tagger\")\n",
    "        nltk.download(\"punkt\")\n",
    "\n",
    "    # Tokenize and POS tag the text\n",
    "    tokens = word_tokenize(text)\n",
    "    tagged = pos_tag(tokens)\n",
    "\n",
    "    verb_tenses = []\n",
    "\n",
    "    # Time indicators (adverbs, phrases)\n",
    "    # future_time = {'tomorrow', 'soon', 'later', 'in the future'}\n",
    "    # past_time = {'yesterday', 'last', 'ago', 'previously', 'earlier'}\n",
    "    # present_time = {'now', 'currently', 'at the moment', 'as we speak'}\n",
    "\n",
    "    # Check for time-related words\n",
    "    text_lower = text.lower()\n",
    "    # if any(word in text_lower for word in future_time):\n",
    "    #     verb_tenses.append('Future')\n",
    "    # if any(word in text_lower for word in past_time):\n",
    "    #     verb_tenses.append('Past')\n",
    "    # if any(word in text_lower for word in present_time):\n",
    "    #     verb_tenses.append('Present')\n",
    "    # if \"was\" in text_lower or \"were\" in text_lower:\n",
    "    #     return \"past\"\n",
    "\n",
    "    # Helper function to check for auxiliary/modal verbs\n",
    "    def has_auxiliary(aux_list):\n",
    "        return any(aux in text_lower for aux in aux_list)\n",
    "\n",
    "    # Iterate through words with their POS tags\n",
    "    for i, (word, tag) in enumerate(tagged):\n",
    "        if tag.startswith(\"VB\"):  # Checking for verb forms\n",
    "            # Present Continuous: \"is/are + VBG\"\n",
    "            if tag == \"VBG\" and i > 0 and tagged[i - 1][0].lower() in [\"is\", \"are\"]:\n",
    "                verb_tenses.append(\"Present\")  ####\n",
    "\n",
    "            # Past Continuous: \"was/were + VBG\"\n",
    "            elif tag == \"VBG\" and i > 0 and tagged[i - 1][0].lower() in [\"was\", \"were\"]:\n",
    "                verb_tenses.append(\"Present\")\n",
    "\n",
    "            # Future Continuous: \"will be + VBG\"\n",
    "            elif (\n",
    "                tag == \"VBG\"\n",
    "                and i > 1\n",
    "                and tagged[i - 2][0].lower() == \"will\"\n",
    "                and tagged[i - 1][0].lower() == \"be\"\n",
    "            ):\n",
    "                verb_tenses.append(\"Present\")\n",
    "\n",
    "            # \"Going to\" Future: \"am/is/are going to + VB\"\n",
    "            elif (\n",
    "                word.lower() == \"going\"\n",
    "                and i < len(tagged) - 1\n",
    "                and tagged[i + 1][0].lower() == \"to\"\n",
    "            ):\n",
    "                verb_tenses.append(\"Present\")\n",
    "\n",
    "            # Future Simple: \"will + VB\"\n",
    "            elif i > 0 and tagged[i - 1][0].lower() == \"will\":\n",
    "                verb_tenses.append(\"present\")\n",
    "\n",
    "            # Past Simple: \"baked\", \"traveled\" (VBD)\n",
    "            elif tag == \"VBD\":\n",
    "                verb_tenses.append(\"Past\")\n",
    "\n",
    "            # Present Simple: \"walks\", \"runs\", \"eats\" (VBP, VBZ)\n",
    "            elif tag in [\"VBP\", \"VBZ\"]:\n",
    "                verb_tenses.append(\"Present\")\n",
    "\n",
    "            # Past Participle: \"was analyzed\"\n",
    "            elif tag == \"VBN\" and has_auxiliary([\"was\", \"were\"]):\n",
    "                verb_tenses.append(\"Past\")\n",
    "\n",
    "            # Present Perfect: \"has analyzed\"\n",
    "            elif tag == \"VBN\" and has_auxiliary([\"has\", \"have\"]):\n",
    "                verb_tenses.append(\"Present\")\n",
    "\n",
    "            # Future Perfect: \"will have analyzed\"\n",
    "            elif tag == \"VBN\" and has_auxiliary([\"will have\"]):\n",
    "                verb_tenses.append(\"Present\")\n",
    "\n",
    "    # If no tenses were found, return \"unknown\"\n",
    "    # if not verb_tenses:\n",
    "    #     return \"past\"\n",
    "\n",
    "    # Use Counter to determine the most common tense\n",
    "    tense_counts = Counter(verb_tenses)\n",
    "    try:\n",
    "        primary_tense = tense_counts.most_common(1)[0][0]\n",
    "    except IndexError:\n",
    "        return \"unknown\"\n",
    "\n",
    "    # Confidence calculation\n",
    "    total_verbs = sum(tense_counts.values())\n",
    "    # confidence = tense_counts.most_common(1)[0][1] / total_verbs\n",
    "    if total_verbs == 0:\n",
    "        return \"unknown\"\n",
    "    # if total_verbs<10:\n",
    "    #     print(primary_tense)\n",
    "    #     print(text)\n",
    "\n",
    "    # If confidence is too low, return \"unknown\"\n",
    "    # if confidence < threshold:\n",
    "    #     # print(primary_tense)\n",
    "    #     # print(text)\n",
    "\n",
    "    #     return \"past\"\n",
    "\n",
    "    return primary_tense.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'past'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyze_sentence_tense(\"\".join((examples[25][\"content\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_data = load_from_pickle(\"../data/test_dataset_2015.pkl\")\n",
    "\n",
    "# df_test_data = pd.DataFrame(test_data,index=[\"xml\"],).T.reset_index()\n",
    "# df_test_data.columns = [\"patentnumber\",\"xml\"] \n",
    "\n",
    "\n",
    "# df = read_xlsb_file()\n",
    "\n",
    "# df[df.issueyear == 2012].to_csv(\"freilichdataet_2012.csv\")\n",
    "df2 = pd.read_csv(\"freilichdataet_2011.csv\")\n",
    "df2[\"patentnumber\"] = df2[\"patentnumber\"].astype(str).transform(lambda x: x.replace(\".0\", \"\"))\n",
    "df2[\"patentnumber\"] = df2[\"patentnumber\"].apply(remove_leadiong_zeros)\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "943\n",
      "Number of exact matches: 628 out of 943 , Percentage: 66.59597030752916\n",
      "Number of exact num of patent extracted: 796 out of 943\n",
      "Avg Total error: 1.6246023329798516, num of corrects:  796\n",
      "Avg Total prophetic error: 1.1018027571580065, num of corrects: 721\n",
      "Avg Total nonprophetic error: 2.482502651113468, num of corrects: 631\n",
      "4912.0\n",
      "Sum of number of Unknowns: 1065\n",
      "Number of patets with experiments extracted 943\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>patentnumber</th>\n",
       "      <th>prophetic_patents</th>\n",
       "      <th>nonprophetic_patents</th>\n",
       "      <th>unknown_patents</th>\n",
       "      <th>prophetic</th>\n",
       "      <th>nonprophetic</th>\n",
       "      <th>issueyear</th>\n",
       "      <th>Total_Extracted</th>\n",
       "      <th>Total_Freilich</th>\n",
       "      <th>prophetic_error</th>\n",
       "      <th>nonprophetic_error</th>\n",
       "      <th>Total_Mean_error</th>\n",
       "      <th>Sum_error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>268</th>\n",
       "      <td>1250</td>\n",
       "      <td>7879893</td>\n",
       "      <td>32</td>\n",
       "      <td>75</td>\n",
       "      <td>558</td>\n",
       "      <td>13.0</td>\n",
       "      <td>652.0</td>\n",
       "      <td>2011.0</td>\n",
       "      <td>107</td>\n",
       "      <td>665.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>577.0</td>\n",
       "      <td>558.0</td>\n",
       "      <td>1154.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472</th>\n",
       "      <td>1507</td>\n",
       "      <td>7872139</td>\n",
       "      <td>16</td>\n",
       "      <td>445</td>\n",
       "      <td>147</td>\n",
       "      <td>0.0</td>\n",
       "      <td>608.0</td>\n",
       "      <td>2011.0</td>\n",
       "      <td>461</td>\n",
       "      <td>608.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>163.0</td>\n",
       "      <td>147.0</td>\n",
       "      <td>326.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>468</th>\n",
       "      <td>1503</td>\n",
       "      <td>7872126</td>\n",
       "      <td>67</td>\n",
       "      <td>117</td>\n",
       "      <td>0</td>\n",
       "      <td>218.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2011.0</td>\n",
       "      <td>184</td>\n",
       "      <td>218.0</td>\n",
       "      <td>151.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>302.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>705</th>\n",
       "      <td>1858</td>\n",
       "      <td>7863292</td>\n",
       "      <td>44</td>\n",
       "      <td>168</td>\n",
       "      <td>41</td>\n",
       "      <td>0.0</td>\n",
       "      <td>253.0</td>\n",
       "      <td>2011.0</td>\n",
       "      <td>212</td>\n",
       "      <td>253.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>170.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>427</th>\n",
       "      <td>1462</td>\n",
       "      <td>7872010</td>\n",
       "      <td>18</td>\n",
       "      <td>180</td>\n",
       "      <td>31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>2011.0</td>\n",
       "      <td>198</td>\n",
       "      <td>255.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>150.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>1656</td>\n",
       "      <td>7875639</td>\n",
       "      <td>3</td>\n",
       "      <td>373</td>\n",
       "      <td>72</td>\n",
       "      <td>0.0</td>\n",
       "      <td>448.0</td>\n",
       "      <td>2011.0</td>\n",
       "      <td>376</td>\n",
       "      <td>448.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>150.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>859</th>\n",
       "      <td>2094</td>\n",
       "      <td>7868037</td>\n",
       "      <td>39</td>\n",
       "      <td>72</td>\n",
       "      <td>1</td>\n",
       "      <td>104.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2011.0</td>\n",
       "      <td>111</td>\n",
       "      <td>112.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>130.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>1234</td>\n",
       "      <td>7879849</td>\n",
       "      <td>5</td>\n",
       "      <td>184</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>2011.0</td>\n",
       "      <td>189</td>\n",
       "      <td>245.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>122.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>424</th>\n",
       "      <td>1459</td>\n",
       "      <td>7872004</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>53</td>\n",
       "      <td>0.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>2011.0</td>\n",
       "      <td>11</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>106.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311</th>\n",
       "      <td>1294</td>\n",
       "      <td>7880012</td>\n",
       "      <td>42</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>2011.0</td>\n",
       "      <td>82</td>\n",
       "      <td>82.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>84.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265</th>\n",
       "      <td>1247</td>\n",
       "      <td>7879888</td>\n",
       "      <td>17</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2011.0</td>\n",
       "      <td>53</td>\n",
       "      <td>54.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>70.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>598</th>\n",
       "      <td>1689</td>\n",
       "      <td>7875726</td>\n",
       "      <td>18</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>49.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>2011.0</td>\n",
       "      <td>48</td>\n",
       "      <td>83.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>70.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>555</td>\n",
       "      <td>7884210</td>\n",
       "      <td>22</td>\n",
       "      <td>207</td>\n",
       "      <td>13</td>\n",
       "      <td>1.0</td>\n",
       "      <td>241.0</td>\n",
       "      <td>2011.0</td>\n",
       "      <td>229</td>\n",
       "      <td>242.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>68.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>766</th>\n",
       "      <td>1920</td>\n",
       "      <td>7863465</td>\n",
       "      <td>17</td>\n",
       "      <td>79</td>\n",
       "      <td>14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>2011.0</td>\n",
       "      <td>96</td>\n",
       "      <td>112.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>66.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>762</th>\n",
       "      <td>1916</td>\n",
       "      <td>7863454</td>\n",
       "      <td>29</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2011.0</td>\n",
       "      <td>64</td>\n",
       "      <td>64.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>64.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>467</th>\n",
       "      <td>1502</td>\n",
       "      <td>7872125</td>\n",
       "      <td>0</td>\n",
       "      <td>52</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>2011.0</td>\n",
       "      <td>52</td>\n",
       "      <td>77.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>574</td>\n",
       "      <td>7884256</td>\n",
       "      <td>38</td>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>2011.0</td>\n",
       "      <td>79</td>\n",
       "      <td>79.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>44.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>426</th>\n",
       "      <td>1461</td>\n",
       "      <td>7872009</td>\n",
       "      <td>0</td>\n",
       "      <td>115</td>\n",
       "      <td>21</td>\n",
       "      <td>0.0</td>\n",
       "      <td>136.0</td>\n",
       "      <td>2011.0</td>\n",
       "      <td>115</td>\n",
       "      <td>136.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>425</th>\n",
       "      <td>1460</td>\n",
       "      <td>7872006</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>2011.0</td>\n",
       "      <td>62</td>\n",
       "      <td>81.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>38.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>520</td>\n",
       "      <td>7884120</td>\n",
       "      <td>23</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>7.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>2011.0</td>\n",
       "      <td>29</td>\n",
       "      <td>32.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>38.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     year patentnumber  prophetic_patents  nonprophetic_patents  \\\n",
       "268  1250      7879893                 32                    75   \n",
       "472  1507      7872139                 16                   445   \n",
       "468  1503      7872126                 67                   117   \n",
       "705  1858      7863292                 44                   168   \n",
       "427  1462      7872010                 18                   180   \n",
       "566  1656      7875639                  3                   373   \n",
       "859  2094      7868037                 39                    72   \n",
       "252  1234      7879849                  5                   184   \n",
       "424  1459      7872004                  0                    11   \n",
       "311  1294      7880012                 42                    40   \n",
       "265  1247      7879888                 17                    36   \n",
       "598  1689      7875726                 18                    30   \n",
       "132   555      7884210                 22                   207   \n",
       "766  1920      7863465                 17                    79   \n",
       "762  1916      7863454                 29                    35   \n",
       "467  1502      7872125                  0                    52   \n",
       "150   574      7884256                 38                    41   \n",
       "426  1461      7872009                  0                   115   \n",
       "425  1460      7872006                  0                    62   \n",
       "98    520      7884120                 23                     6   \n",
       "\n",
       "     unknown_patents  prophetic  nonprophetic  issueyear  Total_Extracted  \\\n",
       "268              558       13.0         652.0     2011.0              107   \n",
       "472              147        0.0         608.0     2011.0              461   \n",
       "468                0      218.0           0.0     2011.0              184   \n",
       "705               41        0.0         253.0     2011.0              212   \n",
       "427               31        0.0         255.0     2011.0              198   \n",
       "566               72        0.0         448.0     2011.0              376   \n",
       "859                1      104.0           8.0     2011.0              111   \n",
       "252                0        0.0         245.0     2011.0              189   \n",
       "424               53        0.0          64.0     2011.0               11   \n",
       "311                0        0.0          82.0     2011.0               82   \n",
       "265                0       52.0           2.0     2011.0               53   \n",
       "598                1       49.0          34.0     2011.0               48   \n",
       "132               13        1.0         241.0     2011.0              229   \n",
       "766               14        0.0         112.0     2011.0               96   \n",
       "762                0       61.0           3.0     2011.0               64   \n",
       "467                0        0.0          77.0     2011.0               52   \n",
       "150                0       60.0          19.0     2011.0               79   \n",
       "426               21        0.0         136.0     2011.0              115   \n",
       "425                0        0.0          81.0     2011.0               62   \n",
       "98                 3        7.0          25.0     2011.0               29   \n",
       "\n",
       "     Total_Freilich  prophetic_error  nonprophetic_error  Total_Mean_error  \\\n",
       "268           665.0             19.0               577.0             558.0   \n",
       "472           608.0             16.0               163.0             147.0   \n",
       "468           218.0            151.0               117.0              34.0   \n",
       "705           253.0             44.0                85.0              41.0   \n",
       "427           255.0             18.0                75.0              57.0   \n",
       "566           448.0              3.0                75.0              72.0   \n",
       "859           112.0             65.0                64.0               1.0   \n",
       "252           245.0              5.0                61.0              56.0   \n",
       "424            64.0              0.0                53.0              53.0   \n",
       "311            82.0             42.0                42.0               0.0   \n",
       "265            54.0             35.0                34.0               1.0   \n",
       "598            83.0             31.0                 4.0              35.0   \n",
       "132           242.0             21.0                34.0              13.0   \n",
       "766           112.0             17.0                33.0              16.0   \n",
       "762            64.0             32.0                32.0               0.0   \n",
       "467            77.0              0.0                25.0              25.0   \n",
       "150            79.0             22.0                22.0               0.0   \n",
       "426           136.0              0.0                21.0              21.0   \n",
       "425            81.0              0.0                19.0              19.0   \n",
       "98             32.0             16.0                19.0               3.0   \n",
       "\n",
       "     Sum_error  \n",
       "268     1154.0  \n",
       "472      326.0  \n",
       "468      302.0  \n",
       "705      170.0  \n",
       "427      150.0  \n",
       "566      150.0  \n",
       "859      130.0  \n",
       "252      122.0  \n",
       "424      106.0  \n",
       "311       84.0  \n",
       "265       70.0  \n",
       "598       70.0  \n",
       "132       68.0  \n",
       "766       66.0  \n",
       "762       64.0  \n",
       "467       50.0  \n",
       "150       44.0  \n",
       "426       42.0  \n",
       "425       38.0  \n",
       "98        38.0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sqlite3 as sql\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "def test_algorithm(year):\n",
    "    conn = sql.connect(\"../app/db/patents.db\")\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Execute query and fetch results\n",
    "    stats = cursor.execute(\"select * from patent_statistics\").fetchall()\n",
    "    stats_df = pd.DataFrame(stats, columns=[\"year\", \"patentnumber\", \"prophetic_patents\", \"nonprophetic_patents\",\"unknown_patents\"])\n",
    "    conn.close()\n",
    "\n",
    "    df_year = df2[df2.issueyear == year][[\"patentnumber\",\"prophetic\",\"nonprophetic\",\"issueyear\"]]\n",
    "    df_year[\"patentnumber\"] = df_year[\"patentnumber\"].astype(str).transform(lambda x: x.replace(\".0\", \"\"))\n",
    "\n",
    "    merged = stats_df.merge(df_year, on=\"patentnumber\")\n",
    "    print(len(merged))\n",
    "    # final = dic_to_dic_w_tense_test(doc_w_exp,threshold=0)\n",
    "    # df_final = pd.DataFrame(final).T.reset_index()\n",
    "    df_final = merged\n",
    "    #df_final.columns = [\"patentnumber\",\"past\",\"present\",\"Unknown\"]\n",
    "    df_final[\"patentnumber\"] = df_final[\"patentnumber\"].apply(remove_leadiong_zeros)\n",
    "    df_check =merged\n",
    "    #df_check[\"past\"] = df_check[\"past\"] + df_check[\"Unknown\"]\n",
    "    df_check[\"Total_Extracted\"] = df_check[\"nonprophetic_patents\"] + df_check[\"prophetic_patents\"]  #+ df_check[\"Unknown\"]\n",
    "    df_check[\"Total_Freilich\"] = df_check[\"prophetic\"] + df_check[\"nonprophetic\"]\n",
    "    df_check[\"prophetic_error\"] = np.sqrt((df_check[\"prophetic\"] - df_check[\"prophetic_patents\"])**2)\n",
    "    df_check[\"nonprophetic_error\"] = np.sqrt((df_check[\"nonprophetic\"] - df_check[\"nonprophetic_patents\"])**2)\n",
    "    df_check[\"Total_Mean_error\"] = np.sqrt((df_check[\"Total_Freilich\"] - df_check[\"Total_Extracted\"])**2)\n",
    "    df_check[\"Sum_error\"] = df_check[\"prophetic_error\"] + df_check[\"nonprophetic_error\"] + df_check[\"Total_Mean_error\"]\n",
    "    print(f\"Number of exact matches: {len(df_check[(df_check.prophetic_error ==0) & (df_check.nonprophetic_error ==0) & (df_check.Total_Mean_error ==0)])} out of {len(df_check)} , Percentage: {len(df_check[(df_check.prophetic_error ==0) & (df_check.nonprophetic_error ==0) & (df_check.Total_Mean_error ==0)])/len(df_check)*100}\")\n",
    "    print(f\"Number of exact num of patent extracted: {len(df_check[(df_check.Total_Extracted == df_check.Total_Freilich)])} out of {len(df_check)}\")\n",
    "    print(f\"Avg Total error: {df_check['Total_Mean_error'].mean()}, num of corrects:  {df_check[df_check['Total_Mean_error'] == 0].shape[0]}\")\n",
    "    print(f\"Avg Total prophetic error: {df_check['prophetic_error'].mean()}, num of corrects: {df_check[df_check['prophetic_error'] == 0].shape[0]}\")\n",
    "    print(f\"Avg Total nonprophetic error: {df_check['nonprophetic_error'].mean()}, num of corrects: {df_check[df_check['nonprophetic_error'] == 0].shape[0]}\")\n",
    "    print(f\"{df_check['Sum_error'].sum()}\")\n",
    "    print(f\"Sum of number of Unknowns: {df_check['unknown_patents'].sum()}\")\n",
    "    print(f\"Number of patets with experiments extracted {len(df_check)}\")\n",
    "    return df_check\n",
    "df_check = test_algorithm(2011)\n",
    "df_check.sort_values(\"Sum_error\",ascending=False).head(20)\n",
    "# 2011\n",
    "# 943\n",
    "# Number of exact matches: 628 out of 943 , Percentage: 66.59597030752916\n",
    "# Number of exact num of patent extracted: 796 out of 943\n",
    "# Avg Total error: 1.6246023329798516, num of corrects:  796\n",
    "# Avg Total prophetic error: 1.1018027571580065, num of corrects: 721\n",
    "# Avg Total nonprophetic error: 2.482502651113468, num of corrects: 631\n",
    "# 4912.0\n",
    "# Sum of number of Unknowns: 1065\n",
    "# Number of patets with experiments extracted 943\n",
    "\n",
    "\n",
    "# 2015\n",
    "# 996\n",
    "# Number of exact matches: 636 out of 996 , Percentage: 63.85542168674698\n",
    "# Number of exact num of patent extracted: 828 out of 996\n",
    "# Avg Total error: 0.7710843373493976, num of corrects:  828\n",
    "# Avg Total prophetic error: 1.3744979919678715, num of corrects: 736\n",
    "# Avg Total nonprophetic error: 2.0973895582329316, num of corrects: 642\n",
    "# 4226.0\n",
    "# Sum of number of Unknowns: 461\n",
    "# Number of patets with experiments extracted 996"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>patentnumber</th>\n",
       "      <th>prophetic_patents</th>\n",
       "      <th>nonprophetic_patents</th>\n",
       "      <th>unknown_patents</th>\n",
       "      <th>prophetic</th>\n",
       "      <th>nonprophetic</th>\n",
       "      <th>issueyear</th>\n",
       "      <th>Total_Extracted</th>\n",
       "      <th>Total_Freilich</th>\n",
       "      <th>prophetic_error</th>\n",
       "      <th>nonprophetic_error</th>\n",
       "      <th>Total_Mean_error</th>\n",
       "      <th>Sum_error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>383</td>\n",
       "      <td>RE042127</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2011.0</td>\n",
       "      <td>10</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>400</td>\n",
       "      <td>7883615</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2011.0</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>404</td>\n",
       "      <td>7883639</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2011.0</td>\n",
       "      <td>8</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>415</td>\n",
       "      <td>7883686</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2011.0</td>\n",
       "      <td>4</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>418</td>\n",
       "      <td>7883691</td>\n",
       "      <td>7</td>\n",
       "      <td>30</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>2011.0</td>\n",
       "      <td>37</td>\n",
       "      <td>40.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>895</th>\n",
       "      <td>2133</td>\n",
       "      <td>7868126</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>2011.0</td>\n",
       "      <td>21</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>896</th>\n",
       "      <td>2134</td>\n",
       "      <td>7868128</td>\n",
       "      <td>1</td>\n",
       "      <td>48</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>2011.0</td>\n",
       "      <td>49</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>932</th>\n",
       "      <td>2170</td>\n",
       "      <td>7868204</td>\n",
       "      <td>1</td>\n",
       "      <td>87</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>2011.0</td>\n",
       "      <td>88</td>\n",
       "      <td>98.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>935</th>\n",
       "      <td>2173</td>\n",
       "      <td>7868210</td>\n",
       "      <td>13</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>10.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>2011.0</td>\n",
       "      <td>25</td>\n",
       "      <td>35.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>26.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>942</th>\n",
       "      <td>2182</td>\n",
       "      <td>7868333</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2011.0</td>\n",
       "      <td>8</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>147 rows Ã— 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     year patentnumber  prophetic_patents  nonprophetic_patents  \\\n",
       "0     383     RE042127                  0                    10   \n",
       "9     400      7883615                  0                     2   \n",
       "11    404      7883639                  8                     0   \n",
       "16    415      7883686                  0                     4   \n",
       "19    418      7883691                  7                    30   \n",
       "..    ...          ...                ...                   ...   \n",
       "895  2133      7868126                  0                    21   \n",
       "896  2134      7868128                  1                    48   \n",
       "932  2170      7868204                  1                    87   \n",
       "935  2173      7868210                 13                    12   \n",
       "942  2182      7868333                  0                     8   \n",
       "\n",
       "     unknown_patents  prophetic  nonprophetic  issueyear  Total_Extracted  \\\n",
       "0                  0        0.0          12.0     2011.0               10   \n",
       "9                  0        0.0           3.0     2011.0                2   \n",
       "11                 0        8.0           1.0     2011.0                8   \n",
       "16                 0        0.0           6.0     2011.0                4   \n",
       "19                 3        0.0          40.0     2011.0               37   \n",
       "..               ...        ...           ...        ...              ...   \n",
       "895                0        0.0          28.0     2011.0               21   \n",
       "896                0        0.0          52.0     2011.0               49   \n",
       "932                0        3.0          95.0     2011.0               88   \n",
       "935                5       10.0          25.0     2011.0               25   \n",
       "942                0        0.0          10.0     2011.0                8   \n",
       "\n",
       "     Total_Freilich  prophetic_error  nonprophetic_error  Total_Mean_error  \\\n",
       "0              12.0              0.0                 2.0               2.0   \n",
       "9               3.0              0.0                 1.0               1.0   \n",
       "11              9.0              0.0                 1.0               1.0   \n",
       "16              6.0              0.0                 2.0               2.0   \n",
       "19             40.0              7.0                10.0               3.0   \n",
       "..              ...              ...                 ...               ...   \n",
       "895            28.0              0.0                 7.0               7.0   \n",
       "896            52.0              1.0                 4.0               3.0   \n",
       "932            98.0              2.0                 8.0              10.0   \n",
       "935            35.0              3.0                13.0              10.0   \n",
       "942            10.0              0.0                 2.0               2.0   \n",
       "\n",
       "     Sum_error  \n",
       "0          4.0  \n",
       "9          2.0  \n",
       "11         2.0  \n",
       "16         4.0  \n",
       "19        20.0  \n",
       "..         ...  \n",
       "895       14.0  \n",
       "896        8.0  \n",
       "932       20.0  \n",
       "935       26.0  \n",
       "942        4.0  \n",
       "\n",
       "[147 rows x 14 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_check[(df_check.Total_Extracted<df_check.Total_Freilich)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patentnumber</th>\n",
       "      <th>xml</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7641702</td>\n",
       "      <td>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\\n&lt;!DOCT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7641704</td>\n",
       "      <td>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\\n&lt;!DOCT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7641709</td>\n",
       "      <td>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\\n&lt;!DOCT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7641721</td>\n",
       "      <td>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\\n&lt;!DOCT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7641723</td>\n",
       "      <td>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\\n&lt;!DOCT...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  patentnumber                                                xml\n",
       "0      7641702  <?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n<!DOCT...\n",
       "1      7641704  <?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n<!DOCT...\n",
       "2      7641709  <?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n<!DOCT...\n",
       "3      7641721  <?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n<!DOCT...\n",
       "4      7641723  <?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n<!DOCT..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset = read_json(\"patent_xmls_2010.json\")\n",
    "test_dataset_df = pd.DataFrame.from_dict(test_dataset, orient='index').reset_index()\n",
    "test_dataset_df.columns = ['patentnumber', 'xml']\n",
    "test_dataset_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m merged \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241m.\u001b[39mmerge(df2, on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpatentnumber\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(merged))\n\u001b[0;32m      3\u001b[0m merged\u001b[38;5;241m.\u001b[39mhead()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "merged = df.merge(df2, on=\"patentnumber\")\n",
    "print(len(merged))\n",
    "merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ipg110104.xml... (1/3)\n",
      "Processed 100/5130...\n",
      "Processed 200/5130...\n",
      "Processed 300/5130...\n",
      "Processed 400/5130...\n",
      "Processed 500/5130...\n",
      "Processed 600/5130...\n",
      "Processed 700/5130...\n",
      "Processed 800/5130...\n",
      "Processed 900/5130...\n",
      "Processed 1000/5130...\n",
      "Processed 1100/5130...\n",
      "Processed 1200/5130...\n",
      "Processed 1300/5130...\n",
      "Processed 1400/5130...\n",
      "Processed 1500/5130...\n",
      "Processed 1600/5130...\n",
      "Processed 1700/5130...\n",
      "Processed 1800/5130...\n",
      "Processed 1900/5130...\n",
      "Processed 2000/5130...\n",
      "Processed 2100/5130...\n",
      "Processed 2200/5130...\n",
      "Processed 2300/5130...\n",
      "Processed 2400/5130...\n",
      "Processed 2500/5130...\n",
      "Processed 2600/5130...\n",
      "Processed 2700/5130...\n",
      "Processed 2800/5130...\n",
      "Processed 2900/5130...\n",
      "Processed 3000/5130...\n",
      "Processed 3100/5130...\n",
      "Processed 3200/5130...\n",
      "Processed 3300/5130...\n",
      "Processed 3400/5130...\n",
      "Processed 3500/5130...\n",
      "Processed 3600/5130...\n",
      "Processed 3700/5130...\n",
      "Processed 3800/5130...\n",
      "Processed 3900/5130...\n",
      "Processed 4000/5130...\n",
      "Processed 4100/5130...\n",
      "Processed 4200/5130...\n",
      "Processed 4300/5130...\n",
      "Processed 4400/5130...\n",
      "Processed 4500/5130...\n",
      "Processed 4600/5130...\n",
      "Processed 4700/5130...\n",
      "Processed 4800/5130...\n",
      "Processed 4900/5130...\n",
      "Processed 5000/5130...\n",
      "Processed 5100/5130...\n",
      "Processing ipg110111.xml... (2/3)\n",
      "Processed 100/5160...\n",
      "Processed 200/5160...\n",
      "Processed 300/5160...\n",
      "Processed 400/5160...\n",
      "Processed 500/5160...\n",
      "Processed 600/5160...\n",
      "Processed 700/5160...\n",
      "Processed 800/5160...\n",
      "Processed 900/5160...\n",
      "Processed 1000/5160...\n",
      "Processed 1100/5160...\n",
      "Processed 1200/5160...\n",
      "Processed 1300/5160...\n",
      "Processed 1400/5160...\n",
      "Processed 1500/5160...\n",
      "Processed 1600/5160...\n",
      "Processed 1700/5160...\n",
      "Processed 1800/5160...\n",
      "Processed 1900/5160...\n",
      "Processed 2000/5160...\n",
      "Processed 2100/5160...\n",
      "Processed 2200/5160...\n",
      "Processed 2300/5160...\n",
      "Processed 2400/5160...\n",
      "Processed 2500/5160...\n",
      "Processed 2600/5160...\n",
      "Processed 2700/5160...\n",
      "Processed 2800/5160...\n",
      "Processed 2900/5160...\n",
      "Processed 3000/5160...\n",
      "Processed 3100/5160...\n",
      "Processed 3200/5160...\n",
      "Processed 3300/5160...\n",
      "Processed 3400/5160...\n",
      "Processed 3500/5160...\n",
      "Processed 3600/5160...\n",
      "Processed 3700/5160...\n",
      "Processed 3800/5160...\n",
      "Processed 3900/5160...\n",
      "Processed 4000/5160...\n",
      "Processed 4100/5160...\n",
      "Processed 4200/5160...\n",
      "Processed 4300/5160...\n",
      "Processed 4400/5160...\n",
      "Processed 4500/5160...\n",
      "Processed 4600/5160...\n",
      "Processed 4700/5160...\n",
      "Processed 4800/5160...\n",
      "Processed 4900/5160...\n",
      "Processed 5000/5160...\n",
      "Processed 5100/5160...\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "folder_path = \"../app/data/patent_grants_2011\"\n",
    "file_names = os.listdir(\"../app/data/patent_grants_2011\")\n",
    "doc_w_exp = {}\n",
    "for i, file in enumerate(file_names):\n",
    "    all_xml_parts = []\n",
    "    if file.endswith(\".xml\"):\n",
    "        print(f\"Processing {file}... ({i + 1}/{len(file_names)})\")\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "        try:\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "                content = file.read()\n",
    "                parts = content.split('<?xml version=\"1.0\" encoding=\"UTF-8\"?>')\n",
    "                parts = [p for p in parts if p.strip()]\n",
    "                all_xml_parts.extend(parts)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file}: {str(e)}\")\n",
    "        # xml_no_dup = remove_duplicate_docs(all_xml_parts)\n",
    "        # print(f\"Num of duplicates removed: {len(all_xml_parts) - len(xml_no_dup)} out of {len(all_xml_parts)}\")\n",
    "        for j, xml in enumerate(all_xml_parts):\n",
    "            if j % 100 == 0 and j > 1:\n",
    "                print(f\"Processed {j}/{len(all_xml_parts)}...\")\n",
    "            if len(xml) <= 2000:\n",
    "                pass\n",
    "            s_tags = re.findall(r\"<s\\d+>.*?</s\\d+>\", xml)\n",
    "            if len(s_tags) > 0 or '<sequence-cwu id=\"SEQLST-0\">' in xml:\n",
    "                pass\n",
    "\n",
    "            heading = extract_experiments_w_heading(xml)\n",
    "\n",
    "            # Process examples based on heading presence\n",
    "            if heading and len(heading) == 1:\n",
    "                #found_heading += 1\n",
    "                examples = extract_examples_start_w_word(\n",
    "                    heading[0].find_next_siblings()\n",
    "                )\n",
    "                if len(examples) == 0:\n",
    "                    soup = BeautifulSoup(xml, \"xml\")\n",
    "                    siblings = soup.find_all([\"heading\", \"p\"])\n",
    "                    examples = extract_examples_start_w_word(siblings)\n",
    "            else:\n",
    "                #not_found_heading += 1\n",
    "                soup = BeautifulSoup(xml, \"xml\")\n",
    "                siblings = soup.find_all([\"heading\", \"p\"])\n",
    "                examples = extract_examples_start_w_word(siblings)\n",
    "\n",
    "            if len(examples) > 0:\n",
    "                doc_num = remove_leadiong_zeros(find_doc_number(xml)[0])\n",
    "                doc_w_exp[doc_num] = examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'nonprophetic_patents'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3621\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3620\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3621\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3622\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx:136\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx:163\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5198\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5206\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'nonprophetic_patents'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[1;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 9>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m df_check \u001b[38;5;241m=\u001b[39mmerged\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m#df_check[\"past\"] = df_check[\"past\"] + df_check[\"Unknown\"]\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m df_check[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal_Extracted\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf_check\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnonprophetic_patents\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m+\u001b[39m df_check[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprophetic_patents\u001b[39m\u001b[38;5;124m\"\u001b[39m]  \u001b[38;5;66;03m#+ df_check[\"Unknown\"]\u001b[39;00m\n\u001b[0;32m     10\u001b[0m df_check[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal_Freilich\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m df_check[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprophetic\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m df_check[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonprophetic\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     11\u001b[0m df_check[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprophetic_error\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msqrt((df_check[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprophetic\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m-\u001b[39m df_check[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprophetic_patents\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:3505\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3503\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   3504\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3505\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3506\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3507\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3623\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3621\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3622\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m-> 3623\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3624\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3625\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3626\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3627\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3628\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'nonprophetic_patents'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# final = dic_to_dic_w_tense_test(doc_w_exp,threshold=0)\n",
    "# df_final = pd.DataFrame(final).T.reset_index()\n",
    "df_final = merged\n",
    "#df_final.columns = [\"patentnumber\",\"past\",\"present\",\"Unknown\"]\n",
    "df_final[\"patentnumber\"] = df_final[\"patentnumber\"].apply(remove_leadiong_zeros)\n",
    "df_check =merged\n",
    "#df_check[\"past\"] = df_check[\"past\"] + df_check[\"Unknown\"]\n",
    "df_check[\"Total_Extracted\"] = df_check[\"nonprophetic_patents\"] + df_check[\"prophetic_patents\"]  #+ df_check[\"Unknown\"]\n",
    "df_check[\"Total_Freilich\"] = df_check[\"prophetic\"] + df_check[\"nonprophetic\"]\n",
    "df_check[\"prophetic_error\"] = np.sqrt((df_check[\"prophetic\"] - df_check[\"prophetic_patents\"])**2)\n",
    "df_check[\"nonprophetic_error\"] = np.sqrt((df_check[\"nonprophetic\"] - df_check[\"nonprophetic_patents\"])**2)\n",
    "df_check[\"Total_Mean_error\"] = np.sqrt((df_check[\"Total_Freilich\"] - df_check[\"Total_Extracted\"])**2)\n",
    "df_check[\"Sum_error\"] = df_check[\"prophetic_error\"] + df_check[\"nonprophetic_error\"] + df_check[\"Total_Mean_error\"]\n",
    "print(f\"Number of exact matches: {len(df_check[(df_check.prophetic_error ==0) & (df_check.nonprophetic_error ==0) & (df_check.Total_Mean_error ==0)])} out of {len(df_check)} , Percentage: {len(df_check[(df_check.prophetic_error ==0) & (df_check.nonprophetic_error ==0) & (df_check.Total_Mean_error ==0)])/len(df_check)*100}\")\n",
    "print(f\"Number of exact num of patent extracted: {len(df_check[(df_check.Total_Extracted == df_check.Total_Freilich)])} out of {len(df_check)}\")\n",
    "print(f\"Avg Total error: {df_check['Total_Mean_error'].mean()}, num of corrects:  {df_check[df_check['Total_Mean_error'] == 0].shape[0]}\")\n",
    "print(f\"Avg Total prophetic error: {df_check['prophetic_error'].mean()}, num of corrects: {df_check[df_check['prophetic_error'] == 0].shape[0]}\")\n",
    "print(f\"Avg Total nonprophetic error: {df_check['nonprophetic_error'].mean()}, num of corrects: {df_check[df_check['nonprophetic_error'] == 0].shape[0]}\")\n",
    "print(f\"{df_check['Sum_error'].sum()}\")\n",
    "print(f\"Sum of number of Unknowns: {df._check['unknown_patents'].sum()}\")\n",
    "print(f\"Number of patets with experiments extracted {len(df_check)}\")\n",
    "df_check.sort_values(\"Sum_error\",ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities.nlp_processing import analyze_sentence_tense\n",
    "def dic_to_dic_w_tense_test(doc_w_exp, threshold=0):\n",
    "    dic = {}\n",
    "    pattern = r\"\\(\\d+\\)\\s*([A-Za-z0-9\\-\\(\\)\\{\\},:;=\\[\\]\\+\\*\\s\\.\\^\\$\\%]+(?:\\.(?:sup|delta|Hz|NMR)[^\\)]*)?)\"\n",
    "\n",
    "    for key, value in doc_w_exp.items():\n",
    "        tense_counts = {\"past\": 0, \"present\": 0, \"unknown\": 0}\n",
    "\n",
    "        if isinstance(value, list) and len(value) == 1:\n",
    "            desc =  \"\".join(value[0][\"content\"]) # value[0][\"title\"] + \".\" +\n",
    "            if len(desc) > threshold:\n",
    "                tense = analyze_sentence_tense(desc)\n",
    "                if tense != \"unknown\":\n",
    "                    tense_counts[tense] += 1\n",
    "                else:\n",
    "                    matches = re.findall(pattern, desc)\n",
    "                    if matches:\n",
    "                        tense_counts[\"past\"] += 1\n",
    "                    else:\n",
    "                        tense_counts[\"unknown\"] += 1\n",
    "                dic[key] = tense_counts\n",
    "\n",
    "        elif isinstance(value, list) and len(value) > 1:\n",
    "            for ls in value:\n",
    "                desc = \"\".join(ls[\"content\"]) #ls[\"title\"] + \".\" + \n",
    "                if len(desc) > threshold:\n",
    "                    if len(desc) > 0:\n",
    "                        tense = analyze_sentence_tense(desc)\n",
    "\n",
    "                        if tense != \"unknown\":\n",
    "                            tense_counts[tense] += 1\n",
    "                        else:\n",
    "                            matches = re.findall(pattern, desc)\n",
    "                            if matches:\n",
    "                                tense_counts[\"past\"] += 1\n",
    "                            else:\n",
    "                                tense_counts[\"unknown\"] += 1\n",
    "            dic[key] = tense_counts\n",
    "\n",
    "        # elif isinstance(value, dict):\n",
    "        #     print(value)\n",
    "        #     for ex, desc in value.items():\n",
    "        #         if len(desc) > threshold:\n",
    "        #             tense = analyze_sentence_tense(desc)\n",
    "        #             if tense != \"unknown\":\n",
    "        #                 tense_counts[tense] += 1\n",
    "        #             else:\n",
    "        #                 matches = re.findall(pattern, desc)\n",
    "        #                 if matches:\n",
    "        #                     tense_counts[\"past\"] += 1\n",
    "        #                 else:\n",
    "        #                     tense_counts[\"unknown\"] += 1\n",
    "        #     dic[key] = tense_counts\n",
    "        # else:\n",
    "        #     print(type(value))\n",
    "        #     print(value)\n",
    "        #     print(key)\n",
    "\n",
    "    return dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found_heading: 7172, not_found_heading: 3713, gibberish: 3111,too_short: 379\n",
      "5000 patents MAE: 0.0, total_error: 0, highest_difference: 0, highest_difference_patent: \n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def extract_examples_start_w_word(xml_siblings):\n",
    "    examples = []\n",
    "    current_example = None\n",
    "    in_example = False\n",
    "\n",
    "    for tag in xml_siblings:\n",
    "        if tag.name == \"heading\":\n",
    "            if (\n",
    "                tag.text.strip().lower().startswith(\"example\")\n",
    "                or tag.text.strip().lower().startswith(\"experiment\")\n",
    "                or tag.text.strip().lower().startswith(\"test\")\n",
    "                or tag.text.strip().lower().startswith(\"trial\")\n",
    "                or \"test\" in tag.text.strip().lower()\n",
    "                or \"experiment\" in tag.text.strip().lower()\n",
    "                or \"example\" in tag.text.strip().lower()\n",
    "                or \"trial\" in tag.text.strip().lower()\n",
    "            ):\n",
    "                in_example = True\n",
    "                current_example = {\n",
    "                    \"number\": tag.text.strip(),\n",
    "                    \"title\": xml_siblings[xml_siblings.index(tag) + 1].text.strip(),\n",
    "                    \"content\": [],\n",
    "                }\n",
    "                examples.append(current_example)\n",
    "        elif tag.name == \"heading\" and (\n",
    "            tag.text.strip().lower().startswith(\"example\")\n",
    "            or tag.text.strip().lower().startswith(\"experiment\")\n",
    "            or tag.text.strip().lower().startswith(\"test\")\n",
    "            or tag.text.strip().lower().startswith(\"trial\")\n",
    "            or \"test\" in tag.text.strip().lower()\n",
    "            or \"experiment\" in tag.text.strip().lower()\n",
    "            or \"example\" in tag.text.strip().lower()\n",
    "            or \"trial\" in tag.text.strip().lower()\n",
    "        ):\n",
    "            in_example = False\n",
    "        # else:\n",
    "        #     # If we hit any other heading, stop collecting content\n",
    "        #     in_example = False\n",
    "        elif in_example and current_example is not None:\n",
    "            current_example[\"content\"].append(tag.text.strip())\n",
    "\n",
    "    return examples\n",
    "num_of_paterns = 5000\n",
    "mae = 0\n",
    "highest_difference = 0\n",
    "found_heading = 0\n",
    "not_found_heading = 0\n",
    "gib = 0\n",
    "short = 0\n",
    "mostdifss = []\n",
    "doc_w_exp = {}\n",
    "\n",
    "for row in test_dataset_df.iterrows():\n",
    "    xml = row[1][\"xml\"]\n",
    "    if len(xml)>2000:\n",
    "        s_tags = re.findall(r'<s\\d+>.*?</s\\d+>', xml)\n",
    "        if len(s_tags) > 0:\n",
    "            #print(f\"Patent {row[1]['patentnumber']} is gibberish\")\n",
    "            gib+=1\n",
    "        else:\n",
    "            heading = extract_experiments_w_heading(xml)\n",
    "            #janetsnumexamples = row[1][\"prophetic\"] + row[1][\"nonprophetic\"]\n",
    "\n",
    "            if heading:\n",
    "                found_heading += 1\n",
    "                examples = extract_examples_start_w_word(heading[0].find_next_siblings())\n",
    "                if len(examples)==0:\n",
    "                    soup = BeautifulSoup(xml, 'xml')\n",
    "                    siblings = soup.find_all(['heading', 'p'])\n",
    "                    examples = extract_examples_start_w_word(siblings)\n",
    "                numexamples = len(examples)\n",
    "            else:\n",
    "                not_found_heading += 1\n",
    "                soup = BeautifulSoup(xml, 'xml')\n",
    "                siblings = soup.find_all(['heading', 'p'])\n",
    "                examples = extract_examples_start_w_word(siblings)\n",
    "                numexamples = len(examples)\n",
    "\n",
    "\n",
    "\n",
    "            # #difference = abs(numexamples-janetsnumexamples)\n",
    "            # mae += difference\n",
    "            # if difference > 0:\n",
    "            #     mostdifss.append([difference, row[1][\"patentnumber\"]])\n",
    "            # if difference > highest_difference:\n",
    "            #     highest_difference = difference\n",
    "            #     highest_difference_patent = row[1][\"patentnumber\"]\n",
    "            if len(examples)>0:\n",
    "                doc_w_exp[row[1][\"patentnumber\"]] = examples\n",
    "\n",
    "    else:\n",
    "        short+=1\n",
    "        #print(f\"skipping {row[1]['patentnumber']}, patent is too short\")\n",
    "        \n",
    "        \n",
    "print(f\"found_heading: {found_heading}, not_found_heading: {not_found_heading}, gibberish: {gib},too_short: {short}\")\n",
    "print(f\"{num_of_paterns} patents MAE: {mae/num_of_paterns}, total_error: {mae}, highest_difference: {highest_difference}, highest_difference_patent: \") #highest_difference_patent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1750"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(doc_w_exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discovered prefixes:\n",
      "content_number\n",
      "example        75.285318\n",
      "comparative     4.797215\n",
      "examples        3.159456\n",
      "reference       2.611387\n",
      "synthesis       2.134245\n",
      "industrial      2.005287\n",
      "preparation     1.966600\n",
      "Name: count, dtype: float64\n",
      "\n",
      "Discovered special phrases:\n",
      "['comparative example 1', 'comparative example 2', 'comparative example 3', 'comparative example 4']\n"
     ]
    }
   ],
   "source": [
    "a = []\n",
    "num_examples = 0\n",
    "for k, pat_exs in doc_w_exp.items():\n",
    "    num_examples += len(pat_exs)\n",
    "    for ex in pat_exs:\n",
    "        ola = \"\".join(ex[\"content\"]) + ex[\"title\"]\n",
    "        if len(ola) == 0:\n",
    "            #print(ex)\n",
    "            continue\n",
    "        a.append({\n",
    "            'patent_number': k,\n",
    "            'content_number': ex[\"number\"]\n",
    "        })\n",
    "df_ex_types = pd.DataFrame(a)\n",
    "val_counts= df_ex_types.content_number.value_counts()\n",
    "valc_df = pd.DataFrame(val_counts).reset_index()\n",
    "\n",
    "# Group by type and sum counts\n",
    "def get_example_type(content_number):\n",
    "    content_lower = content_number.lower()\n",
    "    for prefix in start_w:\n",
    "        if content_lower.startswith(prefix):\n",
    "            return prefix\n",
    "    for special in ls:\n",
    "        if special in content_lower:\n",
    "            return special\n",
    "    return \"other\"\n",
    "\n",
    "# Extract first word from each content_number\n",
    "def get_first_word(text):\n",
    "    return text.lower().split()[0]\n",
    "\n",
    "# Get common prefixes (appearing more than 100 times)\n",
    "common_prefixes = (df_ex_types['content_number']\n",
    "                  .apply(get_first_word)\n",
    "                  .value_counts()\n",
    "                  .loc[lambda x: x > 100])\n",
    "\n",
    "start_w = common_prefixes.index.tolist()\n",
    "print(\"Discovered prefixes:\")\n",
    "print(common_prefixes/df_ex_types.shape[0] * 100)\n",
    "\n",
    "# Function to detect patterns that occur in middle of text\n",
    "def find_common_phrases(df, min_count=50):\n",
    "    # Get all content numbers as lowercase\n",
    "    texts = df['content_number'].str.lower()\n",
    "    # Find phrases with 2 or more words that appear frequently\n",
    "    phrases = texts[texts.str.contains(' .+ ')]  # Contains at least 2 spaces\n",
    "    common_phrases = phrases.value_counts().loc[lambda x: x > min_count]\n",
    "    return common_phrases.index.tolist()\n",
    "\n",
    "ls = find_common_phrases(df_ex_types)\n",
    "print(\"\\nDiscovered special phrases:\")\n",
    "print(ls)\n",
    "\n",
    "# Rest of your grouping code remains the same\n",
    "valc_df['type'] = valc_df['content_number'].apply(get_example_type)\n",
    "type_counts = valc_df.groupby('type')['count'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discovered prefixes (% of unique patents):\n",
      "content_number\n",
      "example        64.571429\n",
      "industrial     17.771429\n",
      "examples       16.914286\n",
      "comparative    13.314286\n",
      "Name: count, dtype: float64\n",
      "\n",
      "Discovered special phrases:\n",
      "['comparative example 1', 'comparative example 2', 'comparative example 3', 'comparative example 4']\n",
      "\n",
      "Number of unique patents per type:\n",
      "type\n",
      "example                  1319\n",
      "other                     476\n",
      "industrial                311\n",
      "comparative               233\n",
      "comparative example 1       5\n",
      "comparative example 2       3\n",
      "Name: patent_number, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "a = []\n",
    "num_examples = 0\n",
    "for k, pat_exs in doc_w_exp.items():\n",
    "    num_examples += len(pat_exs)\n",
    "    for ex in pat_exs:\n",
    "        ola = \"\".join(ex[\"content\"]) + ex[\"title\"]\n",
    "        if len(ola) == 0:\n",
    "            continue\n",
    "        a.append({\n",
    "            'patent_number': k,\n",
    "            'content_number': ex[\"number\"]\n",
    "        })\n",
    "\n",
    "df_ex_types = pd.DataFrame(a)\n",
    "\n",
    "# Function definitions remain the same\n",
    "def get_example_type(content_number):\n",
    "    content_lower = content_number.lower()\n",
    "    for prefix in start_w:\n",
    "        if content_lower.startswith(prefix):\n",
    "            return prefix\n",
    "    for special in ls:\n",
    "        if special in content_lower:\n",
    "            return special\n",
    "    return \"other\"\n",
    "\n",
    "def get_first_word(text):\n",
    "    return text.lower().split()[0]\n",
    "\n",
    "# Get common prefixes (appearing in more than 100 unique patents)\n",
    "common_prefixes = (df_ex_types.groupby('patent_number')['content_number']\n",
    "                  .apply(lambda x: x.apply(get_first_word).unique().tolist())\n",
    "                  .explode()\n",
    "                  .value_counts()\n",
    "                  .loc[lambda x: x > 100])\n",
    "\n",
    "start_w = common_prefixes.index.tolist()\n",
    "print(\"Discovered prefixes (% of unique patents):\")\n",
    "print(common_prefixes/df_ex_types['patent_number'].nunique() * 100)\n",
    "\n",
    "def find_common_phrases(df, min_count=50):\n",
    "    # Get unique patent-phrase combinations\n",
    "    patent_phrases = (df.groupby('patent_number')['content_number']\n",
    "                     .apply(lambda x: x.str.lower().unique().tolist())\n",
    "                     .explode())\n",
    "    phrases = patent_phrases[patent_phrases.str.contains(' .+ ')]\n",
    "    common_phrases = phrases.value_counts().loc[lambda x: x > min_count]\n",
    "    return common_phrases.index.tolist()\n",
    "\n",
    "ls = find_common_phrases(df_ex_types)\n",
    "print(\"\\nDiscovered special phrases:\")\n",
    "print(ls)\n",
    "\n",
    "# Add type column to original dataframe\n",
    "df_ex_types['type'] = df_ex_types['content_number'].apply(get_example_type)\n",
    "\n",
    "# Count unique patents per type\n",
    "type_counts = df_ex_types.groupby('type')['patent_number'].nunique().sort_values(ascending=False)\n",
    "print(\"\\nNumber of unique patents per type:\")\n",
    "print(type_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Unique content numbers: 23329\n",
      "\n",
      "Content number frequencies:\n",
      "content_number\n",
      "Example 1                       6682\n",
      "Example 2                       6024\n",
      "Example 3                       5260\n",
      "Example 4                       4561\n",
      "Example 5                       3855\n",
      "                                ... \n",
      "Example #N.1.1                     1\n",
      "Example #O.1.1                     1\n",
      "Example #X.1.1                     1\n",
      "Example #38                        1\n",
      "VI. Industrial Applicability       1\n",
      "Name: count, Length: 23329, dtype: int64\n",
      "\n",
      "Unique patent numbers: 7965\n",
      "\n",
      "Patent number frequencies:\n",
      "patent_number\n",
      "8952157    1446\n",
      "8987441    1040\n",
      "8969587     996\n",
      "8987242     901\n",
      "9006265     800\n",
      "           ... \n",
      "9034169       1\n",
      "8956508       1\n",
      "8956528       1\n",
      "8956536       1\n",
      "8986522       1\n",
      "Name: count, Length: 7965, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nUnique content numbers:\", len(df_ex_types['content_number'].unique()))\n",
    "print(\"\\nContent number frequencies:\")\n",
    "print(df_ex_types['content_number'].value_counts())\n",
    "\n",
    "print(\"\\nUnique patent numbers:\", len(df_ex_types['patent_number'].unique()))\n",
    "print(\"\\nPatent number frequencies:\")\n",
    "print(df_ex_types['patent_number'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INDUSTRIAL APPLICABILITY  :  30\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of patents with difference > 0: 104\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[4.0, '8926548'],\n",
       " [1.0, '8926595'],\n",
       " [1.0, '8926732'],\n",
       " [1.0, '8926824'],\n",
       " [6.0, '8926862'],\n",
       " [4.0, '8926864'],\n",
       " [1.0, '8926872'],\n",
       " [1.0, '8926955'],\n",
       " [11.0, '8926966'],\n",
       " [7.0, '8926979'],\n",
       " [1.0, '8927015'],\n",
       " [1.0, '8927023'],\n",
       " [5.0, '8927036'],\n",
       " [2.0, '8927098'],\n",
       " [1.0, '8927150'],\n",
       " [2.0, '8927157'],\n",
       " [1.0, '8927236'],\n",
       " [4.0, '8927254'],\n",
       " [1.0, '8927429'],\n",
       " [1.0, '8927469'],\n",
       " [1.0, '8927479'],\n",
       " [1.0, '8927536'],\n",
       " [1.0, '8927541'],\n",
       " [1.0, '8927546'],\n",
       " [2.0, '8927568'],\n",
       " [4.0, '8927583'],\n",
       " [6.0, '8927588'],\n",
       " [3.0, '8927596'],\n",
       " [1.0, '8927617'],\n",
       " [1.0, '8927619'],\n",
       " [2.0, '8927631'],\n",
       " [1.0, '8927642'],\n",
       " [1.0, '8927647'],\n",
       " [1.0, '8927678'],\n",
       " [1.0, '8927681'],\n",
       " [2.0, '8927683'],\n",
       " [1.0, '8927698'],\n",
       " [7.0, '8927710'],\n",
       " [15.0, '8927721'],\n",
       " [1.0, '8927725'],\n",
       " [1.0, '8927730'],\n",
       " [1.0, '8927738'],\n",
       " [1.0, '8927746'],\n",
       " [1.0, '8927750'],\n",
       " [1.0, '8927777'],\n",
       " [1.0, '8927781'],\n",
       " [1.0, '8927801'],\n",
       " [9.0, '8927811'],\n",
       " [1.0, '8932406'],\n",
       " [3.0, '8932470'],\n",
       " [4.0, '8932480'],\n",
       " [2.0, '8932491'],\n",
       " [2.0, '8932492'],\n",
       " [1.0, '8932579'],\n",
       " [3.0, '8932591'],\n",
       " [1.0, '8932615'],\n",
       " [2.0, '8932626'],\n",
       " [2.0, '8932688'],\n",
       " [4.0, '8932705'],\n",
       " [6.0, '8932717'],\n",
       " [1.0, '8932842'],\n",
       " [1.0, '8932853'],\n",
       " [2.0, '8932867'],\n",
       " [1.0, '8932981'],\n",
       " [1.0, '8933000'],\n",
       " [2.0, '8933012'],\n",
       " [23.0, '8933032'],\n",
       " [12.0, '8933042'],\n",
       " [2.0, '8933065'],\n",
       " [1.0, '8933067'],\n",
       " [1.0, '8933071'],\n",
       " [56.0, '8933072'],\n",
       " [1.0, '8933074'],\n",
       " [1.0, '8933094'],\n",
       " [3.0, '8933100'],\n",
       " [2.0, '8933103'],\n",
       " [3.0, '8933105'],\n",
       " [2.0, '8933117'],\n",
       " [15.0, '8933131'],\n",
       " [1.0, '8933135'],\n",
       " [1.0, '8933181'],\n",
       " [1.0, '8933204'],\n",
       " [1.0, '8933221'],\n",
       " [2.0, '8933224'],\n",
       " [1.0, '8933228'],\n",
       " [6.0, '8933229'],\n",
       " [1.0, '8933251'],\n",
       " [1.0, '8933267'],\n",
       " [1.0, '8933294'],\n",
       " [3.0, '8933318'],\n",
       " [1.0, '8934216'],\n",
       " [1.0, '8934637'],\n",
       " [1.0, '8936675'],\n",
       " [1.0, '8936685'],\n",
       " [1.0, '8936706'],\n",
       " [1.0, '8936833'],\n",
       " [1.0, '8936836'],\n",
       " [3.0, '8936849'],\n",
       " [3.0, '8936859'],\n",
       " [3.0, '8936910'],\n",
       " [1.0, '8936939'],\n",
       " [1.0, '8937025'],\n",
       " [1.0, '8937045'],\n",
       " [6.0, '8937055']]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"Number of patents with difference > 0: {len(mostdifss)}\")\n",
    "mostdifss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patentnumber</th>\n",
       "      <th>prophetic</th>\n",
       "      <th>nonprophetic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8285</th>\n",
       "      <td>8933072</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     patentnumber  prophetic  nonprophetic\n",
       "8285      8933072        0.0           9.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# highest difference\n",
    "df2[df2.patentnumber == \"8933072\"][[\"patentnumber\",\"prophetic\",\"nonprophetic\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2897131579.py, line 34)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[16], line 34\u001b[1;36m\u001b[0m\n\u001b[1;33m    Number of exact matches: 2870 out of 3971 , Percentage: 72.27398640141023\u001b[0m\n\u001b[1;37m           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "final = dic_to_dic_w_tense_test(doc_w_exp,threshold=0)\n",
    "df_final = pd.DataFrame(final).T.reset_index()\n",
    "df_final.columns = [\"patentnumber\",\"past\",\"present\",\"Unknown\"]\n",
    "df_final[\"patentnumber\"] = df_final[\"patentnumber\"].apply(remove_leadiong_zeros)\n",
    "df_check = df_final.merge(merged,on= \"patentnumber\",how=\"left\")[[\"patentnumber\",\"present\",\"past\",\"Unknown\",\"prophetic\",\"nonprophetic\",\"allprophetic\",\"someprophetic\"]]\n",
    "#df_check[\"past\"] = df_check[\"past\"] + df_check[\"Unknown\"]\n",
    "df_check[\"Total_Extracted\"] = df_check[\"past\"] + df_check[\"present\"]  #+ df_check[\"Unknown\"]\n",
    "df_check[\"Total_Freilich\"] = df_check[\"prophetic\"] + df_check[\"nonprophetic\"]\n",
    "df_check[\"prophetic_error\"] = np.sqrt((df_check[\"prophetic\"] - df_check[\"present\"])**2)\n",
    "df_check[\"nonprophetic_error\"] = np.sqrt((df_check[\"nonprophetic\"] - df_check[\"past\"])**2)\n",
    "df_check[\"Total_Mean_error\"] = np.sqrt((df_check[\"Total_Freilich\"] - df_check[\"Total_Extracted\"])**2)\n",
    "df_check[\"Sum_error\"] = df_check[\"prophetic_error\"] + df_check[\"nonprophetic_error\"] + df_check[\"Total_Mean_error\"]\n",
    "print(f\"Number of exact matches: {len(df_check[(df_check.prophetic_error ==0) & (df_check.nonprophetic_error ==0) & (df_check.Total_Mean_error ==0)])} out of {len(df_check)} , Percentage: {len(df_check[(df_check.prophetic_error ==0) & (df_check.nonprophetic_error ==0) & (df_check.Total_Mean_error ==0)])/len(df_check)*100}\")\n",
    "print(f\"Number of exact num of patent extracted: {len(df_check[(df_check.Total_Extracted == df_check.Total_Freilich)])} out of {len(df_check)}\")\n",
    "print(f\"Avg Total error: {df_check['Total_Mean_error'].mean()}, num of corrects:  {df_check[df_check['Total_Mean_error'] == 0].shape[0]}\")\n",
    "print(f\"Avg Total prophetic error: {df_check['prophetic_error'].mean()}, num of corrects: {df_check[df_check['prophetic_error'] == 0].shape[0]}\")\n",
    "print(f\"Avg Total nonprophetic error: {df_check['nonprophetic_error'].mean()}, num of corrects: {df_check[df_check['nonprophetic_error'] == 0].shape[0]}\")\n",
    "print(f\"{df_check['Sum_error'].sum()}\")\n",
    "print(f\"Sum of number of Unknowns: {df_check['Unknown'].sum()}\")\n",
    "print(f\"Number of patets with experiments extracted {len(df_check)}\")\n",
    "df_check.sort_values(\"Sum_error\",ascending=False).head(20)\n",
    "\n",
    "\n",
    "# 1000\n",
    "# Number of exact matches: 577 out of 799 , Percentage: 72.21526908635795\n",
    "# Number of exact num of patent extracted: 694 out of 799\n",
    "# Avg Total error: 0.5181476846057572, num of corrects:  694\n",
    "# Avg Total prophetic error: 0.5857321652065082, num of corrects: 652\n",
    "# Avg Total nonprophetic error: 0.9887359198998749, num of corrects: 587\n",
    "# 1672.0\n",
    "# Sum of number of Unknowns: 0\n",
    "# Number of patets with experiments extracted 799\n",
    "\n",
    "# 5000\n",
    "# Number of exact matches: 2870 out of 3971 , Percentage: 72.27398640141023\n",
    "# Number of exact num of patent extracted: 3461 out of 3971\n",
    "# Avg Total error: 0.5560312263913372, num of corrects:  3461\n",
    "# Avg Total prophetic error: 0.8645177537144296, num of corrects: 3209\n",
    "# Avg Total nonprophetic error: 1.2976580206497104, num of corrects: 2942\n",
    "# 10794.0\n",
    "# Sum of number of Unknowns: 0\n",
    "# Number of patets with experiments extracted 3971"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# patent 9102614 injanets says 4 examples but there are 161\n",
    "# patent 9102599 in janets says 1 ,theres more than 1 actually \n",
    "# Some xml files are made with error : e.g. these dont have much information : skipping 9102601, 9102662,9102692\n",
    "# these have gibberish: 9102724,9102705,9102727,9102628"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_siblings_test(xml_siblings):\n",
    "    examples = []\n",
    "\n",
    "    # Find all matching headings directly from xml_siblings\n",
    "    example_headings = [\n",
    "        tag\n",
    "        for tag in xml_siblings\n",
    "        if tag.name == \"heading\"\n",
    "        and any(\n",
    "            keyword in tag.text.strip().lower().replace(\" \", \"\")\n",
    "            for keyword in [\"example\", \"experiment\", \"test\"]\n",
    "        )\n",
    "        # and not any(\n",
    "        #     excluded in tag.text.strip().lower().replace(\" \", \"\")\n",
    "        #     for excluded in [\"reference\", \"preparation\"]\n",
    "        # )\n",
    "    ]\n",
    "\n",
    "    for heading in example_headings:\n",
    "        current_content = []\n",
    "        idx = xml_siblings.index(heading)\n",
    "\n",
    "        # Get title from next heading if available\n",
    "        title = \"\"\n",
    "        if idx + 1 < len(xml_siblings) and xml_siblings[idx + 1].name == \"heading\":\n",
    "            title = xml_siblings[idx + 1].text.strip()\n",
    "\n",
    "        # Collect content until next example heading\n",
    "        i = idx + 1\n",
    "        while i < len(xml_siblings):\n",
    "            if (\n",
    "                xml_siblings[i].name == \"heading\"\n",
    "                and any(\n",
    "                    keyword in xml_siblings[i].text.strip().lower().replace(\" \", \"\")\n",
    "                    for keyword in [\"example\", \"experiment\", \"test\"]\n",
    "                )\n",
    "                # and not any(\n",
    "                #     excluded in xml_siblings[i].text.strip().lower().replace(\" \", \"\")\n",
    "                #     for excluded in [\"reference\", \"preparation\"]\n",
    "                # )\n",
    "            ):\n",
    "                break\n",
    "            if xml_siblings[i].name == \"p\":\n",
    "                current_content.append(xml_siblings[i].text.strip())\n",
    "            i += 1\n",
    "\n",
    "        examples.append(\n",
    "            {\"number\": heading.text.strip(), \"title\": title, \"content\": current_content}\n",
    "        )\n",
    "\n",
    "    return examples if examples else None\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "has more than 1 Examples heading, 349\n",
      "has more than 1 Examples heading, 388\n",
      "has more than 1 Examples heading, 689\n",
      "has more than 1 Examples heading, 768\n",
      "has more than 1 Examples heading, 781\n",
      "has more than 1 Examples heading, 814\n",
      "1000/25081 so far found 771 docs with experiments\n"
     ]
    }
   ],
   "source": [
    "def extract_examples_from_heading(heading):\n",
    "    extracted_examples = []\n",
    "    example_start_w = process_siblings(heading.find_next_siblings())\n",
    "    if example_start_w:\n",
    "        if not example_start_w[0][\"content\"]:\n",
    "            extracted_ex_w_word = extract_examples_start_w_word(heading.find_next_siblings())\n",
    "            if extracted_ex_w_word:\n",
    "                if isinstance(extracted_ex_w_word, list):\n",
    "                    if extracted_ex_w_word and len(extracted_ex_w_word[0][\"content\"])>0:\n",
    "                        extracted_examples.append(extracted_ex_w_word)\n",
    "                elif extracted_ex_w_word[\"content\"]:\n",
    "                    extracted_examples.append(extracted_ex_w_word)\n",
    "        else:\n",
    "            if len(example_start_w[0][\"content\"])>0:\n",
    "                extracted_examples.append(example_start_w)\n",
    "    else:\n",
    "        extracted_ex_w_word = extract_examples_start_w_word(heading.find_next_siblings())\n",
    "        if extracted_ex_w_word:\n",
    "            if isinstance(extracted_ex_w_word, list):\n",
    "                if extracted_ex_w_word and len(extracted_ex_w_word[0][\"content\"])>0:\n",
    "                    extracted_examples.append(extracted_ex_w_word)\n",
    "            elif extracted_ex_w_word[\"content\"]:\n",
    "                if len(extracted_ex_w_word[\"content\"])>0:\n",
    "                    extracted_examples.append(extracted_ex_w_word)\n",
    "        else:\n",
    "            num_dot_examples = extract_num_dot_examples(str(heading.find_next_siblings()))\n",
    "            if num_dot_examples:\n",
    "                extracted_examples.append(num_dot_examples)\n",
    "    return extracted_examples\n",
    "\n",
    "def extract_examples(xml):\n",
    "    heading = extract_experiments_w_heading(xml)\n",
    "    if heading:\n",
    "        if len(heading) > 1:\n",
    "            print(f\"has more than 1 Examples heading, {i}\")\n",
    "        elif len(heading) == 1:\n",
    "            return extract_examples_from_heading(heading[0])\n",
    "        else:\n",
    "            extracted_ex_w_word = extract_examples_w_word(xml)\n",
    "            if extracted_ex_w_word:\n",
    "                if isinstance(extracted_ex_w_word, list):\n",
    "                    if extracted_ex_w_word and extracted_ex_w_word[0][\"content\"]:\n",
    "                        return [extracted_ex_w_word]\n",
    "                elif extracted_ex_w_word[\"content\"]:\n",
    "                    return [extracted_ex_w_word]\n",
    "            else:\n",
    "                example_start_w = process_siblings(heading[0].find_next_siblings())\n",
    "                if example_start_w and example_start_w[0][\"content\"]:\n",
    "                    return [example_start_w]\n",
    "    else: \n",
    "        example_start_w = extract_examples_w_word(xml)\n",
    "        if example_start_w:\n",
    "            return [example_start_w]\n",
    "        else:\n",
    "            num_dot_examples = extract_num_dot_examples(xml)\n",
    "            if num_dot_examples:\n",
    "                return [num_dot_examples]\n",
    "    return []\n",
    "\n",
    "doc_w_exp = {}\n",
    "for i, xml in enumerate(merged[\"xml\"][:1000], start=1):\n",
    "    if i % 1000 == 0:\n",
    "        print(f\"{i}/{len(merged.xml)} so far found {len(doc_w_exp)} docs with experiments\")\n",
    "    \n",
    "    extracted_examples = extract_examples(xml)\n",
    "    if extracted_examples:\n",
    "        doc_w_exp[find_doc_number(xml)[0]] = extracted_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/25081 so far found 54 docs with experiments\n",
      "200/25081 so far found 111 docs with experiments\n",
      "300/25081 so far found 169 docs with experiments\n",
      "has more than 1 Examples heading, 349\n",
      "has more than 1 Examples heading, 388\n",
      "400/25081 so far found 225 docs with experiments\n",
      "500/25081 so far found 294 docs with experiments\n",
      "600/25081 so far found 352 docs with experiments\n",
      "has more than 1 Examples heading, 689\n",
      "700/25081 so far found 395 docs with experiments\n",
      "has more than 1 Examples heading, 768\n",
      "has more than 1 Examples heading, 781\n",
      "800/25081 so far found 463 docs with experiments\n",
      "has more than 1 Examples heading, 814\n",
      "900/25081 so far found 522 docs with experiments\n",
      "1000/25081 so far found 575 docs with experiments\n"
     ]
    }
   ],
   "source": [
    "def extract_examples_from_heading(heading):\n",
    "    length_threshold = 30\n",
    "    extracted_examples = []\n",
    "    example_start_w = process_siblings(heading.find_next_siblings())\n",
    "    if example_start_w:\n",
    "        if not example_start_w[0][\"content\"]:\n",
    "            extracted_ex_w_word = extract_examples_start_w_word(heading.find_next_siblings())\n",
    "            if extracted_ex_w_word:\n",
    "                if isinstance(extracted_ex_w_word, list):\n",
    "                    extracted_ex_w_word = [ex for ex in extracted_ex_w_word if len(ex[\"content\"])>length_threshold]\n",
    "                    extracted_examples.append(extracted_ex_w_word)\n",
    "                elif extracted_ex_w_word[\"content\"]>length_threshold:\n",
    "                    extracted_examples.append(extracted_ex_w_word)\n",
    "        else:\n",
    "            if isinstance(example_start_w, list):\n",
    "                example_start_w = [ex for ex in example_start_w if len(ex[\"content\"])>length_threshold]\n",
    "                extracted_examples.append(example_start_w)\n",
    "    else:\n",
    "        extracted_ex_w_word = extract_examples_start_w_word(heading.find_next_siblings())\n",
    "        if extracted_ex_w_word:\n",
    "            if isinstance(extracted_ex_w_word, list):\n",
    "                extracted_ex_w_word = [ex for ex in extracted_ex_w_word if len(ex[\"content\"])>length_threshold]\n",
    "                extracted_examples.append(extracted_ex_w_word)\n",
    "            elif len(extracted_ex_w_word[\"content\"])>length_threshold:\n",
    "                extracted_examples.append(extracted_ex_w_word)\n",
    "        else:\n",
    "            num_dot_examples = extract_num_dot_examples(str(heading.find_next_siblings()))\n",
    "            if isinstance(num_dot_examples, list):\n",
    "                num_dot_examples = [ex for ex in num_dot_examples if len(ex[\"content\"])>length_threshold]\n",
    "                extracted_examples.append(num_dot_examples)\n",
    "            elif isinstance(num_dot_examples, dict):\n",
    "                num_dot_examples = [ex for ex in num_dot_examples.items() if len(ex[\"content\"])>length_threshold]\n",
    "\n",
    "                extracted_examples.append(num_dot_examples)\n",
    "    return extracted_examples\n",
    "\n",
    "def extract_examples(xml):\n",
    "    length_threshold = 30\n",
    "    heading = extract_experiments_w_heading(xml)\n",
    "    if heading:\n",
    "        if len(heading) > 1:\n",
    "            print(f\"has more than 1 Examples heading, {i}\")\n",
    "        elif len(heading) == 1:\n",
    "            return extract_examples_from_heading(heading[0])\n",
    "        else:\n",
    "            pass\n",
    "        # else:\n",
    "        #     extracted_ex_w_word = extract_examples_w_word(xml)\n",
    "        #     if extracted_ex_w_word:\n",
    "        #         if isinstance(extracted_ex_w_word, list):\n",
    "        #             extracted_ex_w_word = [ex for ex in extracted_ex_w_word if len(ex[\"content\"])>length_threshold]\n",
    "        #             return extracted_ex_w_word if extracted_ex_w_word\n",
    "        #         elif extracted_ex_w_word[\"content\"]>length_threshold:\n",
    "        #             return extracted_ex_w_word if extracted_ex_w_word\n",
    "        #     else:\n",
    "        #         example_start_w = process_siblings(heading[0].find_next_siblings())\n",
    "        #         if isinstance(example_start_w, list):\n",
    "        #             example_start_w = [ex for ex in example_start_w if len(ex[\"content\"])>length_threshold]\n",
    "        #             return example_start_w if example_start_w\n",
    "    else: \n",
    "        example_start_w = extract_examples_w_word(xml)\n",
    "        if isinstance(example_start_w, list):\n",
    "            example_start_w = [ex for ex in example_start_w if len(ex[\"content\"])>length_threshold]\n",
    "            return example_start_w\n",
    "        else:\n",
    "            num_dot_examples = extract_num_dot_examples(xml)\n",
    "            if isinstance(num_dot_examples, list):\n",
    "                num_dot_examples = [ex for ex in num_dot_examples if len(ex[\"content\"])>length_threshold]\n",
    "                return num_dot_examples\n",
    "\n",
    "doc_w_exp = {}\n",
    "for i, xml in enumerate(merged[\"xml\"][:1000], start=1):\n",
    "    if i % 100 == 0:\n",
    "        print(f\"{i}/{len(merged.xml)} so far found {len(doc_w_exp)} docs with experiments\")\n",
    "    \n",
    "    extracted_examples = extract_examples(xml)\n",
    "    if extracted_examples:\n",
    "        doc_w_exp[find_doc_number(xml)[0]] = extracted_examples\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "has more than 1 Examples heading, 349\n",
      "has more than 1 Examples heading, 388\n",
      "has more than 1 Examples heading, 689\n",
      "has more than 1 Examples heading, 768\n",
      "has more than 1 Examples heading, 781\n",
      "has more than 1 Examples heading, 814\n",
      "1000/25081 so far found 769 docs with experiments\n",
      "has more than 1 Examples heading, 1054\n",
      "has more than 1 Examples heading, 1417\n",
      "has more than 1 Examples heading, 1475\n",
      "has more than 1 Examples heading, 1810\n"
     ]
    }
   ],
   "source": [
    "\n",
    "doc_w_exp = {}\n",
    "for i, xml in enumerate(merged[:1999].xml.values, start=1):\n",
    "    if i % 1000 == 0:\n",
    "        print(f\"{i}/{len(merged.xml)} so far found {len(doc_w_exp)} docs with experiments\")\n",
    "    \n",
    "    extracted_examples = extract_examples(xml)\n",
    "    if extracted_examples:\n",
    "        doc_w_exp[find_doc_number(xml)[0]] = extracted_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 18977 patents from doc_w_exp_new_algorithm.pkl\n"
     ]
    }
   ],
   "source": [
    "# save_as_pickle(doc_w_exp,\"doc_w_exp_new_algorithm.pkl\")\n",
    "doc_w_exp = load_from_pickle(\"doc_w_exp_new_algorithm.pkl\")\n",
    "\n",
    "# save_as_json(doc_w_exp, \"doc_w_exp_2015.json\")\n",
    "# doc_w_exp = read_json(\"../data/doc_w_exp_2015.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import html\n",
    "import unicodedata\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def clean_text_updated(text):\n",
    "    \"\"\"\n",
    "    Clean text by removing HTML tags, special characters, extra spaces,\n",
    "    and normalizing the content while keeping meaningful punctuation.\n",
    "\n",
    "    Args:\n",
    "        text (str): Input text to clean\n",
    "\n",
    "    Returns:\n",
    "        str: Cleaned text\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "\n",
    "    # Decode HTML entities & remove HTML tags\n",
    "    text = html.unescape(text)\n",
    "    text = BeautifulSoup(text, \"html.parser\").get_text()\n",
    "\n",
    "    # Normalize Unicode characters (e.g., Ã© â†’ e)\n",
    "    text = unicodedata.normalize(\"NFKC\", text)\n",
    "\n",
    "    # Remove unwanted special characters but keep punctuation\n",
    "    text = re.sub(r\"[^\\w\\s.,!?'\\-]\", \"\", text)\n",
    "\n",
    "    # Normalize spaces: remove multiple spaces, newlines, and tabs\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "def check_tense_nltk_updated(sentence):\n",
    "    words = word_tokenize(sentence)\n",
    "    tagged = pos_tag(words)\n",
    "\n",
    "    # Define tense categories\n",
    "    past = {\"VBD\", \"VBN\"}\n",
    "    present = {\"VB\", \"VBG\", \"VBP\", \"VBZ\"}\n",
    "    future = {\"MD\"}\n",
    "\n",
    "    tenses = {\"past\": 0, \"present\": 0, \"future\": 0}\n",
    "\n",
    "    for i, (word, tag) in enumerate(tagged):\n",
    "        # Count past tense words\n",
    "        if tag in past:\n",
    "            tenses[\"past\"] += 1\n",
    "\n",
    "        # Count present tense words\n",
    "        elif tag in present:\n",
    "            tenses[\"present\"] += 1\n",
    "\n",
    "        # Future tense handling\n",
    "        elif tag in future:\n",
    "            if word.lower() in {\"will\", \"shall\"}:\n",
    "                # Ensure 'will' or 'shall' is followed by a verb (to confirm future tense)\n",
    "                if i + 1 < len(tagged) and tagged[i + 1][1] in {\"VB\", \"VBP\"}:\n",
    "                    tenses[\"future\"] += 1\n",
    "\n",
    "    return max(tenses, key=tenses.get) if max(tenses.values()) > 0 else \"Unknown\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from utilities.nlp_processing import check_tense_nltk,clean_text\n",
    "\n",
    "def tense_with_pattern(description):\n",
    "    future_pattern = r'\\b(?:will|would|shall|should)\\b'\n",
    "    past_pattern = r'\\b(?:was|were|had|did)\\b'\n",
    "\n",
    "    future_matches = re.findall(future_pattern, description)\n",
    "    past_matches = re.findall(past_pattern, description)\n",
    "    if future_matches and past_matches:\n",
    "        if len(future_matches) > len(past_matches):\n",
    "            return \"present\"\n",
    "        else:\n",
    "            return \"past\"\n",
    "    if future_pattern or past_pattern:\n",
    "        if future_pattern:\n",
    "            return \"present\"\n",
    "        else:\n",
    "            return \"past\"\n",
    "    return \"Unkown\"\n",
    "\n",
    "\n",
    "def dic_to_dic_w_tense(doc_w_exp,threshold = 50):\n",
    "    dic = {}\n",
    "    pattern = r'\\(\\d+\\)\\s*([A-Za-z0-9\\-\\(\\)\\{\\},:;=\\[\\]\\+\\*\\s\\.\\^\\$\\%]+(?:\\.(?:sup|delta|Hz|NMR)[^\\)]*)?)'\n",
    "    pattern2 = r'\\(\\d+\\)\\s*(?:[A-Za-z]+\\s*\\d*\\-?[A-Za-z]*[\\(\\{][^)]*[\\)\\}][^;]*|(?:\\.\\d*H\\-?NMR[^;]*|\\d+\\.[a-zA-Z]*\\d*))'\n",
    "\n",
    "\n",
    "    for key, value in doc_w_exp.items():\n",
    "        tense_counts = {\"past\": 0, \"present\": 0,\"Unknown\":0}\n",
    "        \n",
    "        if isinstance(value, list) and len(value) == 1 and len(value[0][0][\"content\"]) > threshold:\n",
    "            desc = value[0][0][\"title\"] + \".\" + \"\".join(value[0][0][\"content\"])\n",
    "            tense = check_tense_nltk_updated(clean_text(desc))\n",
    "            if tense !=  \"Unknown\":\n",
    "                tense_counts[tense] += 1\n",
    "            else:\n",
    "                matches = re.findall(pattern, desc)\n",
    "                if matches:\n",
    "                    tense_counts[\"past\"] += 1\n",
    "                else:\n",
    "                    tense_counts[\"Unknown\"] += 1\n",
    "            dic[key] = tense_counts\n",
    "\n",
    "\n",
    "        elif isinstance(value[0], list) and len(value[0]) > 1:\n",
    "            for ls in value[0]:\n",
    "                if len(ls[\"content\"]) > threshold:\n",
    "                    desc = \"\".join(ls[\"content\"])\n",
    "                    tense = check_tense_nltk_updated(clean_text(desc))\n",
    "                    if tense !=  \"Unknown\":\n",
    "                        tense_counts[tense] += 1\n",
    "                    else:\n",
    "                        matches = re.findall(pattern, desc)\n",
    "                        if matches:\n",
    "                            tense_counts[\"past\"] += 1\n",
    "                        else:\n",
    "                            tense_counts[\"Unknown\"] += 1\n",
    "            dic[key] = tense_counts\n",
    "\n",
    "        elif isinstance(value[0], dict):\n",
    "            for ex, desc in value[0].items():\n",
    "                if len(desc) > threshold:\n",
    "                    tense = check_tense_nltk_updated(clean_text(desc))\n",
    "                    if tense !=  \"Unknown\":\n",
    "                        tense_counts[tense] += 1\n",
    "                    else:\n",
    "                        matches = re.findall(pattern, desc)\n",
    "                        if matches:\n",
    "                            tense_counts[\"past\"] += 1\n",
    "                        else:\n",
    "                            tense_counts[\"Unknown\"] += 1\n",
    "            dic[key] = tense_counts\n",
    "        else:\n",
    "            print(type(value[0]))\n",
    "            print(value[0])\n",
    "\n",
    "    return dic\n",
    "\n",
    "import re\n",
    "\n",
    "def dic_to_dic_w_unknown_tense(doc_w_exp):\n",
    "    dic = {}\n",
    "    pattern = r'\\(\\d+\\)\\s*([A-Za-z0-9\\-\\(\\)\\{\\},:;=\\[\\]\\+\\*\\s\\.\\^\\$\\%]+(?:\\.(?:sup|delta|Hz|NMR)[^\\)]*)?)'\n",
    "    pattern2 = r'\\(\\d+\\)\\s*(?:[A-Za-z]+\\s*\\d*\\-?[A-Za-z]*[\\(\\{][^)]*[\\)\\}][^;]*|(?:\\.\\d*H\\-?NMR[^;]*|\\d+\\.[a-zA-Z]*\\d*))'\n",
    "\n",
    "    for key, value in doc_w_exp.items():\n",
    "        unknown_experiments = []  # Initialize a list to store the experiments classified as Unknown\n",
    "        \n",
    "        if isinstance(value[0], list) and len(value[0]) == 1:\n",
    "            desc = value[0][0][\"title\"] + \".\" + \"\".join(value[0][0][\"content\"])\n",
    "            tense = check_tense_nltk_updated(clean_text(desc))\n",
    "            if tense == \"Unknown\":\n",
    "                unknown_experiments.append(desc)\n",
    "\n",
    "        elif isinstance(value[0], list) and len(value[0]) > 1:\n",
    "            for ls in value[0]:\n",
    "                desc = ls[\"title\"] + \".\" + \"\".join(ls[\"content\"])\n",
    "                tense = check_tense_nltk_updated(clean_text(desc))\n",
    "                if tense == \"Unknown\":\n",
    "                    unknown_experiments.append(desc)\n",
    "\n",
    "        elif isinstance(value[0], dict):\n",
    "            for ex, description in value[0].items():\n",
    "                tense = check_tense_nltk_updated(clean_text(description))\n",
    "                if tense == \"Unknown\":\n",
    "                    unknown_experiments.append(description)\n",
    "\n",
    "        # Only add to the dictionary if there are \"Unknown\" experiments\n",
    "        if unknown_experiments:\n",
    "            dic[key] = unknown_experiments\n",
    "\n",
    "    return dic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1140748136.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [105]\u001b[1;36m\u001b[0m\n\u001b[1;33m    unkown:\u001b[0m\n\u001b[1;37m           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# 8946300 example 1-5\n",
    "# 08946296 example 2 onwards \n",
    "# 08932484 examples 2 to 11\n",
    "\n",
    "\n",
    "# 08933074:  \n",
    "# Synthesis of 5,5-dimethyl-4-(pyridin-4-yl)-3-(4-(quinolin-2-ylmethoxy)phenyl) furan-2(5H)-one (Example 23)\n",
    "# 5,5-dimethyl-4-(pyridin-4-yl)-3-(4-(quinolin-2-ylmethoxy)phenyl) furan-2(5H)-one (Example 23)\n",
    "\n",
    "# 08945522:\n",
    "# Example 2\n",
    "# (69) 3-(3-(Triethoxysilyl)propylamino)phenyl benzoate and\n",
    "# (140) Example 9\n",
    "# (141) 4-(3-(Triethoxysilyl)propylamino)phenyl benzoate and\n",
    "# (142) Example 10\n",
    "# (143) 4-(Bis(3-(triethoxysilyl)propyl)amino)phenyl benzoate\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[8969641,8926966,8951515,8940436,9023999,9011956,9018396,9213027,9108964,9084992]#examples our algorithm didnt find but freilich did \n",
    "# many dont have an overarching examples section but have example 1, example 2 etc\n",
    "# check these xml, the text has examples section:9194008,9018332,9054322,9150561,9213027, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pprint as p\n",
    "# p.pprint(merged[merged.patentnumber == \"9018332\"][\"xml\"].values[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ERRORS IN FREILICH DATASET:\n",
    "patentnumber 8957069: in freilich 108.0\tnonproph and 0 prophetic but there are many more examples than 108, our algorithm: 267 nonprophetic and 1 prophetic and 3 unknown\n",
    "patent number 8946443:we extracted 271\tfreilich: 108, this patent has refrtence examples, are we extracting these or not? \n",
    "patent number 8952010: same issue do we take refrence examples? \n",
    "patent 8933099: there are far more than 33 examples in this patent but freilich says 33, our algorithm found 131\n",
    "what to do with Preparation examples?? e.g 8962612 in freilich has 6 we got 70."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# improvement for our algorithm\n",
    "1. remove duplicates somehow, e.g in patent number 08987242, there some examples are extracted twice just because the name e.g example aa137 appears multiple times.\n",
    "2. better classification of the patents. e.g patent 8987295 has 442, we extract the same number as frelich but we clasify differently.\n",
    "3. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
