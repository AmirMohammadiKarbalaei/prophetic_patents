{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\amoha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\amoha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from utilities.utils_clean import *\n",
    "from utilities.test_dataset_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#download_patents_pto(start_year=2015, end_year=2015, kind='grant',download_path=\"data\")\n",
    "#unzip_files(\"data\",\"patent_grants_2015\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "test_dataset = create_test_dataset_from_freilich( #\"patent_xmls_2010.json\"\n",
    "    year=2015,  # The year you want to analyze\n",
    "    freilich_data_path=\"Freilich.Data.Compressed.xlsb\",  # Path to your Freilich dataset\n",
    "    path_to_all_xmls_for_chosen_year=\"../app/data/patent_grants_2015\"  # Directory containing XML files\n",
    ")\n",
    "\n",
    "# # Optional: Check the results\n",
    "# print(f\"Number of patents extracted: {len(test_dataset)}\")\n",
    "# print(\"Sample document numbers:\", list(test_dataset.keys())[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved as patent_xmls_2015.json\n"
     ]
    }
   ],
   "source": [
    "save_as_json(test_dataset, \"patent_xmls_2015.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patentnumber</th>\n",
       "      <th>xml</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7641702</td>\n",
       "      <td>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\\n&lt;!DOCT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7641704</td>\n",
       "      <td>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\\n&lt;!DOCT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7641709</td>\n",
       "      <td>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\\n&lt;!DOCT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7641721</td>\n",
       "      <td>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\\n&lt;!DOCT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7641723</td>\n",
       "      <td>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\\n&lt;!DOCT...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  patentnumber                                                xml\n",
       "0      7641702  <?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n<!DOCT...\n",
       "1      7641704  <?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n<!DOCT...\n",
       "2      7641709  <?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n<!DOCT...\n",
       "3      7641721  <?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n<!DOCT...\n",
       "4      7641723  <?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n<!DOCT..."
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = read_json(\"patent_xmls_2010.json\")\n",
    "df = pd.DataFrame.from_dict(data,orient=\"index\").reset_index()\n",
    "df.columns = [\"patentnumber\",\"xml\"]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_examples_start_w_word_all(xml_siblings):\n",
    "    \"\"\"Extract examples from XML siblings with improved handling.\n",
    "\n",
    "    Args:\n",
    "        xml_siblings: List of XML sibling elements\n",
    "\n",
    "    Returns:\n",
    "        list: List of dictionaries containing example information\n",
    "    \"\"\"\n",
    "    KEYWORDS = {\"example\", \"experiment\", \"test\", \"trial\"}\n",
    "    PLURALS = {\"examples\", \"experiments\", \"tests\", \"trials\"}\n",
    "    MAX_CONTENT_LENGTH = 4\n",
    "\n",
    "    def is_example_heading(text):\n",
    "        \"\"\"Check if heading indicates start of an example section.\"\"\"\n",
    "        text_lower = text.strip().lower()\n",
    "        return any(keyword in text_lower for keyword in KEYWORDS) and not any(\n",
    "            text_lower == plural for plural in PLURALS\n",
    "        )\n",
    "\n",
    "    examples = []\n",
    "    current_example = None\n",
    "\n",
    "    for i, tag in enumerate(xml_siblings):\n",
    "        if not hasattr(tag, \"name\") or not hasattr(tag, \"text\"):\n",
    "            continue\n",
    "\n",
    "        if tag.name == \"heading\":\n",
    "            text = tag.text.strip()\n",
    "\n",
    "            # Check for example heading\n",
    "            if is_example_heading(text):\n",
    "                # Try to get title from next element\n",
    "                try:\n",
    "                    title = (\n",
    "                        xml_siblings[i + 1].text.strip()\n",
    "                        if i + 1 < len(xml_siblings)\n",
    "                        else \"\"\n",
    "                    )\n",
    "                except (AttributeError, IndexError):\n",
    "                    title = \"\"\n",
    "\n",
    "                current_example = {\"number\": text, \"title\": title, \"content\": []}\n",
    "                examples.append(current_example)\n",
    "\n",
    "            # Handle end of example section\n",
    "            elif text.lower() in PLURALS or text.lower() == \"exampels\":\n",
    "                current_example = None\n",
    "\n",
    "        # Collect content if in example section\n",
    "        elif (\n",
    "            current_example is not None\n",
    "            and len(current_example[\"content\"]) < MAX_CONTENT_LENGTH\n",
    "        ):\n",
    "            try:\n",
    "                content = tag.text.strip()\n",
    "                if content:  # Only append non-empty content\n",
    "                    current_example[\"content\"].append(content)\n",
    "            except AttributeError:\n",
    "                continue\n",
    "\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk import pos_tag, word_tokenize\n",
    "def analyze_sentence_tense(text, threshold=0.5):\n",
    "    text = text.replace(\"  \", \"\").replace(\"\\n\", \" \").replace(\"\\t\", \" \")\n",
    "\n",
    "    # Ensure required NLTK data is available\n",
    "    try:\n",
    "        nltk.data.find(\"taggers/averaged_perceptron_tagger\")\n",
    "    except LookupError:\n",
    "        nltk.download(\"averaged_perceptron_tagger\")\n",
    "        nltk.download(\"punkt\")\n",
    "\n",
    "    # Tokenize and POS tag the text\n",
    "    tokens = word_tokenize(text)\n",
    "    tagged = pos_tag(tokens)\n",
    "\n",
    "    verb_tenses = []\n",
    "\n",
    "    # Time indicators (adverbs, phrases)\n",
    "    # future_time = {'tomorrow', 'soon', 'later', 'in the future'}\n",
    "    # past_time = {'yesterday', 'last', 'ago', 'previously', 'earlier'}\n",
    "    # present_time = {'now', 'currently', 'at the moment', 'as we speak'}\n",
    "\n",
    "    # Check for time-related words\n",
    "    text_lower = text.lower()\n",
    "    # if any(word in text_lower for word in future_time):\n",
    "    #     verb_tenses.append('Future')\n",
    "    # if any(word in text_lower for word in past_time):\n",
    "    #     verb_tenses.append('Past')\n",
    "    # if any(word in text_lower for word in present_time):\n",
    "    #     verb_tenses.append('Present')\n",
    "    # if \"was\" in text_lower or \"were\" in text_lower:\n",
    "    #     return \"past\"\n",
    "\n",
    "    # Helper function to check for auxiliary/modal verbs\n",
    "    def has_auxiliary(aux_list):\n",
    "        return any(aux in text_lower for aux in aux_list)\n",
    "\n",
    "    # Iterate through words with their POS tags\n",
    "    for i, (word, tag) in enumerate(tagged):\n",
    "        if tag.startswith(\"VB\"):  # Checking for verb forms\n",
    "            # Present Continuous: \"is/are + VBG\"\n",
    "            if tag == \"VBG\" and i > 0 and tagged[i - 1][0].lower() in [\"is\", \"are\"]:\n",
    "                verb_tenses.append(\"Present\")  ####\n",
    "\n",
    "            # Past Continuous: \"was/were + VBG\"\n",
    "            elif tag == \"VBG\" and i > 0 and tagged[i - 1][0].lower() in [\"was\", \"were\"]:\n",
    "                verb_tenses.append(\"Present\")\n",
    "\n",
    "            # Future Continuous: \"will be + VBG\"\n",
    "            elif (\n",
    "                tag == \"VBG\"\n",
    "                and i > 1\n",
    "                and tagged[i - 2][0].lower() == \"will\"\n",
    "                and tagged[i - 1][0].lower() == \"be\"\n",
    "            ):\n",
    "                verb_tenses.append(\"Present\")\n",
    "\n",
    "            # \"Going to\" Future: \"am/is/are going to + VB\"\n",
    "            elif (\n",
    "                word.lower() == \"going\"\n",
    "                and i < len(tagged) - 1\n",
    "                and tagged[i + 1][0].lower() == \"to\"\n",
    "            ):\n",
    "                verb_tenses.append(\"Present\")\n",
    "\n",
    "            # Future Simple: \"will + VB\"\n",
    "            elif i > 0 and tagged[i - 1][0].lower() == \"will\":\n",
    "                verb_tenses.append(\"present\")\n",
    "\n",
    "            # Past Simple: \"baked\", \"traveled\" (VBD)\n",
    "            elif tag == \"VBD\":\n",
    "                verb_tenses.append(\"Past\")\n",
    "\n",
    "            # Present Simple: \"walks\", \"runs\", \"eats\" (VBP, VBZ)\n",
    "            elif tag in [\"VBP\", \"VBZ\"]:\n",
    "                verb_tenses.append(\"Present\")\n",
    "\n",
    "            # Past Participle: \"was analyzed\"\n",
    "            elif tag == \"VBN\" and has_auxiliary([\"was\", \"were\"]):\n",
    "                verb_tenses.append(\"Past\")\n",
    "\n",
    "            # Present Perfect: \"has analyzed\"\n",
    "            elif tag == \"VBN\" and has_auxiliary([\"has\", \"have\"]):\n",
    "                verb_tenses.append(\"Present\")\n",
    "\n",
    "            # Future Perfect: \"will have analyzed\"\n",
    "            elif tag == \"VBN\" and has_auxiliary([\"will have\"]):\n",
    "                verb_tenses.append(\"Present\")\n",
    "\n",
    "    # If no tenses were found, return \"unknown\"\n",
    "    # if not verb_tenses:\n",
    "    #     return \"past\"\n",
    "\n",
    "    # Use Counter to determine the most common tense\n",
    "    tense_counts = Counter(verb_tenses)\n",
    "    try:\n",
    "        primary_tense = tense_counts.most_common(1)[0][0]\n",
    "    except IndexError:\n",
    "        return \"unknown\"\n",
    "\n",
    "    # Confidence calculation\n",
    "    total_verbs = sum(tense_counts.values())\n",
    "    # confidence = tense_counts.most_common(1)[0][1] / total_verbs\n",
    "    if total_verbs == 0:\n",
    "        return \"unknown\"\n",
    "    # if total_verbs<10:\n",
    "    #     print(primary_tense)\n",
    "    #     print(text)\n",
    "\n",
    "    # If confidence is too low, return \"unknown\"\n",
    "    # if confidence < threshold:\n",
    "    #     # print(primary_tense)\n",
    "    #     # print(text)\n",
    "\n",
    "    #     return \"past\"\n",
    "\n",
    "    return primary_tense.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'past'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyze_sentence_tense(\"\".join((examples[25][\"content\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of patents:14375\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>patentnumber</th>\n",
       "      <th>appyear</th>\n",
       "      <th>issueyear</th>\n",
       "      <th>inventorcountry</th>\n",
       "      <th>divisional</th>\n",
       "      <th>continuation</th>\n",
       "      <th>prioritydate</th>\n",
       "      <th>priority.formatted</th>\n",
       "      <th>priorityyear</th>\n",
       "      <th>...</th>\n",
       "      <th>yr4</th>\n",
       "      <th>yr8</th>\n",
       "      <th>yr11</th>\n",
       "      <th>claims</th>\n",
       "      <th>wordaverage</th>\n",
       "      <th>litigated</th>\n",
       "      <th>orangebook</th>\n",
       "      <th>forward.cites</th>\n",
       "      <th>industry</th>\n",
       "      <th>subindustry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>408507</td>\n",
       "      <td>7776838</td>\n",
       "      <td>1995.0</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>US</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>19871028.0</td>\n",
       "      <td>32078.0</td>\n",
       "      <td>1987.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>109.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>31.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>408508</td>\n",
       "      <td>7723066</td>\n",
       "      <td>1995.0</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>US</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>19880808.0</td>\n",
       "      <td>32363.0</td>\n",
       "      <td>1988.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>68.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>31.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>408509</td>\n",
       "      <td>7687247</td>\n",
       "      <td>1994.0</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>US</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>19890601.0</td>\n",
       "      <td>32660.0</td>\n",
       "      <td>1989.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>43.33333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>31.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>408510</td>\n",
       "      <td>7662383</td>\n",
       "      <td>2008.0</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>DE</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>19890324.0</td>\n",
       "      <td>32591.0</td>\n",
       "      <td>1989.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>37.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>31.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>408511</td>\n",
       "      <td>7705215</td>\n",
       "      <td>1993.0</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>US</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>19900417.0</td>\n",
       "      <td>32980.0</td>\n",
       "      <td>1990.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>79.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>33.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0 patentnumber  appyear  issueyear inventorcountry  divisional  \\\n",
       "0      408507      7776838   1995.0     2010.0              US         1.0   \n",
       "1      408508      7723066   1995.0     2010.0              US         1.0   \n",
       "2      408509      7687247   1994.0     2010.0              US         1.0   \n",
       "3      408510      7662383   2008.0     2010.0              DE         0.0   \n",
       "4      408511      7705215   1993.0     2010.0              US         0.0   \n",
       "\n",
       "   continuation  prioritydate  priority.formatted  priorityyear  ...  yr4  \\\n",
       "0           1.0    19871028.0             32078.0        1987.0  ...  1.0   \n",
       "1           1.0    19880808.0             32363.0        1988.0  ...  1.0   \n",
       "2           1.0    19890601.0             32660.0        1989.0  ...  1.0   \n",
       "3           1.0    19890324.0             32591.0        1989.0  ...  0.0   \n",
       "4           1.0    19900417.0             32980.0        1990.0  ...  1.0   \n",
       "\n",
       "   yr8  yr11  claims wordaverage  litigated  orangebook  forward.cites  \\\n",
       "0  0.0   0.0    11.0   109.00000        0.0         1.0            0.0   \n",
       "1  0.0   0.0    10.0    68.00000        0.0         0.0            0.0   \n",
       "2  0.0   0.0    18.0    43.33333        0.0         0.0            3.0   \n",
       "3  0.0   0.0     8.0    37.00000        0.0         0.0            0.0   \n",
       "4  0.0   0.0     4.0    79.00000        0.0         0.0            6.0   \n",
       "\n",
       "   industry  subindustry  \n",
       "0       3.0         31.0  \n",
       "1       3.0         31.0  \n",
       "2       3.0         31.0  \n",
       "3       3.0         31.0  \n",
       "4       3.0         33.0  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test_data = load_from_pickle(\"../data/test_dataset_2015.pkl\")\n",
    "\n",
    "# df_test_data = pd.DataFrame(test_data,index=[\"xml\"],).T.reset_index()\n",
    "# df_test_data.columns = [\"patentnumber\",\"xml\"] \n",
    "\n",
    "\n",
    "# df = read_xlsb_file()\n",
    "\n",
    "# df[df.issueyear == 2012].to_csv(\"freilichdataet_2012.csv\")\n",
    "df2 = pd.read_csv(\"freilichdataet_2010.csv\")\n",
    "df2[\"patentnumber\"] = df2[\"patentnumber\"].astype(str).transform(lambda x: x.replace(\".0\", \"\"))\n",
    "df2[\"patentnumber\"] = df2[\"patentnumber\"].apply(remove_leadiong_zeros)\n",
    "print(f\"Number of patents:{len(df2)}\")\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'past'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import pos_tag, word_tokenize\n",
    "def analyze_sentence_tense(text, threshold=0.5):\n",
    "    text = text.replace(\"  \", \"\").replace(\"\\n\", \" \").replace(\"\\t\", \" \")\n",
    "\n",
    "    # Ensure required NLTK data is available\n",
    "    try:\n",
    "        nltk.data.find(\"taggers/averaged_perceptron_tagger\")\n",
    "    except LookupError:\n",
    "        nltk.download(\"averaged_perceptron_tagger\")\n",
    "        nltk.download(\"punkt\")\n",
    "\n",
    "    # Tokenize and POS tag the text\n",
    "    tokens = word_tokenize(text)\n",
    "    tagged = pos_tag(tokens)\n",
    "\n",
    "    verb_tenses = []\n",
    "\n",
    "    # Check for time-related words\n",
    "    text_lower = text.lower()\n",
    "    # if any(word in text_lower for word in future_time):\n",
    "    #     verb_tenses.append('Future')\n",
    "    # if any(word in text_lower for word in past_time):\n",
    "    #     verb_tenses.append('Past')\n",
    "    # if any(word in text_lower for word in present_time):\n",
    "    #     verb_tenses.append('Present')\n",
    "    if \"was\" in text_lower or \"were\" in text_lower:\n",
    "        return \"past\"\n",
    "    # if text.strip().startswith(('Prepared', 'Obtained', 'Synthesized', 'Isolated')):\n",
    "    #     return \"past\"\n",
    "    def has_passive_voice(tagged):\n",
    "        \"\"\"Check if sentence contains passive voice construction\"\"\"\n",
    "        for i, (word, tag) in enumerate(tagged):\n",
    "            # Check for past participle\n",
    "            if tag == 'VBN':\n",
    "                # Look for forms of \"be\" verb before it\n",
    "                if i > 0 and tagged[i-1][0].lower() in ['was', 'were', 'is', 'are', 'be']:\n",
    "                    return True\n",
    "                # Check if VBN starts the sentence (implied passive)\n",
    "                if i == 0:\n",
    "                    return True\n",
    "        return False\n",
    "    def is_patent_procedure(text, tagged):\n",
    "        \"\"\"Check if text matches patent procedure patterns\"\"\"\n",
    "        # Common patent procedure starters\n",
    "        procedure_starters = {\n",
    "            'prepared', 'obtained', 'synthesized', 'isolated', 'dissolved', 'mixed',\n",
    "            'combined', 'heated', 'cooled', 'filtered', 'purified', 'separated'\n",
    "        }\n",
    "        \n",
    "        # Check if starts with procedure word\n",
    "        first_word = text.strip().split()[0].lower()\n",
    "        if first_word in procedure_starters:\n",
    "            return True\n",
    "        \n",
    "        # Check for chemical procedure patterns\n",
    "        procedure_patterns = [\n",
    "            'according to', 'following the procedure', 'as described',\n",
    "            'using the method', 'following example'\n",
    "        ]\n",
    "        if any(pattern in text.lower() for pattern in procedure_patterns):\n",
    "            return True\n",
    "            \n",
    "        return False\n",
    "    # Helper function to check for auxiliary/modal verbs\n",
    "    def has_auxiliary(aux_list):\n",
    "        return any(aux in text_lower for aux in aux_list)\n",
    "    if has_passive_voice(tagged):\n",
    "        return \"past\"\n",
    "    if is_patent_procedure(text, tagged):\n",
    "        return \"past\"\n",
    "\n",
    "    # Iterate through words with their POS tags\n",
    "    for i, (word, tag) in enumerate(tagged):\n",
    "        if tag.startswith(\"VB\"):  # Checking for verb forms\n",
    "            # Present Continuous: \"is/are + VBG\"\n",
    "            if tag == \"VBG\" and i > 0 and tagged[i - 1][0].lower() in [\"is\", \"are\"]:\n",
    "                verb_tenses.append(\"Present\")  ####\n",
    "\n",
    "            # Past Continuous: \"was/were + VBG\"\n",
    "            elif tag == \"VBG\" and i > 0 and tagged[i - 1][0].lower() in [\"was\", \"were\"]:\n",
    "                verb_tenses.append(\"Present\")\n",
    "\n",
    "            # Future Continuous: \"will be + VBG\"\n",
    "            elif (\n",
    "                tag == \"VBG\"\n",
    "                and i > 1\n",
    "                and tagged[i - 2][0].lower() == \"will\"\n",
    "                and tagged[i - 1][0].lower() == \"be\"\n",
    "            ):\n",
    "                verb_tenses.append(\"Present\")\n",
    "\n",
    "            # \"Going to\" Future: \"am/is/are going to + VB\"\n",
    "            elif (\n",
    "                word.lower() == \"going\"\n",
    "                and i < len(tagged) - 1\n",
    "                and tagged[i + 1][0].lower() == \"to\"\n",
    "            ):\n",
    "                verb_tenses.append(\"Present\")\n",
    "\n",
    "            # Future Simple: \"will + VB\"\n",
    "            elif i > 0 and tagged[i - 1][0].lower() == \"will\":\n",
    "                verb_tenses.append(\"present\")\n",
    "\n",
    "            # Past Simple: \"baked\", \"traveled\" (VBD)\n",
    "            elif tag == \"VBD\":\n",
    "                verb_tenses.append(\"Past\")\n",
    "\n",
    "            # Present Simple: \"walks\", \"runs\", \"eats\" (VBP, VBZ)\n",
    "            elif tag in [\"VBP\", \"VBZ\"]:\n",
    "                verb_tenses.append(\"Present\")\n",
    "\n",
    "            # Past Participle: \"was analyzed\"\n",
    "            elif tag == \"VBN\" and has_auxiliary([\"was\", \"were\"]):\n",
    "                verb_tenses.append(\"Past\")\n",
    "\n",
    "            # Present Perfect: \"has analyzed\"\n",
    "            elif tag == \"VBN\" and has_auxiliary([\"has\", \"have\"]):\n",
    "                verb_tenses.append(\"Present\")\n",
    "\n",
    "            # Future Perfect: \"will have analyzed\"\n",
    "            elif tag == \"VBN\" and has_auxiliary([\"will have\"]):\n",
    "                verb_tenses.append(\"Present\")\n",
    "\n",
    "    # If no tenses were found, return \"unknown\"\n",
    "    if not verb_tenses:\n",
    "        print(tagged)\n",
    "        return \"unknown\"\n",
    "\n",
    "    # Use Counter to determine the most common tense\n",
    "    tense_counts = Counter(verb_tenses)\n",
    "    primary_tense = tense_counts.most_common(1)[0][0]\n",
    "\n",
    "    # Confidence calculation\n",
    "    total_verbs = sum(tense_counts.values())\n",
    "    # confidence = tense_counts.most_common(1)[0][1] / total_verbs\n",
    "    if total_verbs == 0:\n",
    "        return \"unknown\"\n",
    "    # if total_verbs<10:\n",
    "    #     print(primary_tense)\n",
    "    #     print(text)\n",
    "\n",
    "    # If confidence is too low, return \"unknown\"\n",
    "    # if confidence < threshold:\n",
    "    #     # print(primary_tense)\n",
    "    #     # print(text)\n",
    "\n",
    "    #     return \"past\"\n",
    "\n",
    "    return primary_tense.lower()\n",
    "analyze_sentence_tense(\"Prepared according to the procedure described above for EXAMPLE 26, except using 3-methoxyphenylacetic acid in place of formic acid. MS (ES+): m/z 481.09 [MH+]..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of patents in the database for 2010:  792\n",
      "Number of patets: 241\n",
      "Number of exact matches: 92 out of 241 , Percentage:      38.17%\n",
      "Number of exact num of patent extracted: 151 out of 241,     62.66%\n",
      "Avg Total error: 1.01, num of corrects:        151\n",
      "Avg Total prophetic error: 2.79, num of corrects:160,        66.39%\n",
      "Avg Total nonprophetic error: 4.54, num of corrects: 106,      43.98%\n",
      "2010.0\n",
      "Sum of number of Unknowns: 322\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>patentnumber</th>\n",
       "      <th>year</th>\n",
       "      <th>prophetic_patents</th>\n",
       "      <th>nonprophetic_patents</th>\n",
       "      <th>unknown_patents</th>\n",
       "      <th>mixed_tense_percentage</th>\n",
       "      <th>all_proph</th>\n",
       "      <th>some_proph</th>\n",
       "      <th>no_proph</th>\n",
       "      <th>prophetic</th>\n",
       "      <th>nonprophetic</th>\n",
       "      <th>issueyear</th>\n",
       "      <th>Total_Extracted</th>\n",
       "      <th>Total_Freilich</th>\n",
       "      <th>prophetic_error</th>\n",
       "      <th>nonprophetic_error</th>\n",
       "      <th>Total_Mean_error</th>\n",
       "      <th>Sum_error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>190</td>\n",
       "      <td>7645785</td>\n",
       "      <td>2010</td>\n",
       "      <td>64</td>\n",
       "      <td>148</td>\n",
       "      <td>168</td>\n",
       "      <td>6%</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>376.0</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>380</td>\n",
       "      <td>380.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>228.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>288.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>242</td>\n",
       "      <td>7645891</td>\n",
       "      <td>2010</td>\n",
       "      <td>1</td>\n",
       "      <td>135</td>\n",
       "      <td>14</td>\n",
       "      <td>0%</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>122.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>150</td>\n",
       "      <td>150.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>107.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>228.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>188</td>\n",
       "      <td>7645781</td>\n",
       "      <td>2010</td>\n",
       "      <td>0</td>\n",
       "      <td>91</td>\n",
       "      <td>0</td>\n",
       "      <td>0%</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>88.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>91</td>\n",
       "      <td>91.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>176.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>611</td>\n",
       "      <td>7642267</td>\n",
       "      <td>2010</td>\n",
       "      <td>0</td>\n",
       "      <td>54</td>\n",
       "      <td>0</td>\n",
       "      <td>0%</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>43.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>54</td>\n",
       "      <td>54.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>86.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>546</td>\n",
       "      <td>7642073</td>\n",
       "      <td>2010</td>\n",
       "      <td>0</td>\n",
       "      <td>42</td>\n",
       "      <td>0</td>\n",
       "      <td>0%</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>34.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>42</td>\n",
       "      <td>42.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>68.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>614</td>\n",
       "      <td>7642276</td>\n",
       "      <td>2010</td>\n",
       "      <td>10</td>\n",
       "      <td>92</td>\n",
       "      <td>40</td>\n",
       "      <td>0%</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>141.0</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>142</td>\n",
       "      <td>141.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>60.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>646</td>\n",
       "      <td>7642342</td>\n",
       "      <td>2010</td>\n",
       "      <td>1</td>\n",
       "      <td>48</td>\n",
       "      <td>5</td>\n",
       "      <td>0%</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>54</td>\n",
       "      <td>49.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>57.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>456</td>\n",
       "      <td>7641905</td>\n",
       "      <td>2010</td>\n",
       "      <td>0</td>\n",
       "      <td>43</td>\n",
       "      <td>5</td>\n",
       "      <td>0%</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>25.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>48</td>\n",
       "      <td>43.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>55.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>239</td>\n",
       "      <td>7645886</td>\n",
       "      <td>2010</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>0%</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>26.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>29</td>\n",
       "      <td>29.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>52.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>191</td>\n",
       "      <td>7645786</td>\n",
       "      <td>2010</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>42</td>\n",
       "      <td>0%</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>53</td>\n",
       "      <td>53.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>52.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>152</td>\n",
       "      <td>7645715</td>\n",
       "      <td>2010</td>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>0%</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>31</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>44.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>39</td>\n",
       "      <td>7645336</td>\n",
       "      <td>2010</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0%</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>13</td>\n",
       "      <td>12.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>26.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>62</td>\n",
       "      <td>7645426</td>\n",
       "      <td>2010</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0%</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>14</td>\n",
       "      <td>12.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>207</td>\n",
       "      <td>7645813</td>\n",
       "      <td>2010</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>0%</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>24</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>491</td>\n",
       "      <td>7641968</td>\n",
       "      <td>2010</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>5%</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>20</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>204</td>\n",
       "      <td>7645804</td>\n",
       "      <td>2010</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0%</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>13</td>\n",
       "      <td>13.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>79</td>\n",
       "      <td>7645459</td>\n",
       "      <td>2010</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0%</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>11.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>20</td>\n",
       "      <td>20.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>182</td>\n",
       "      <td>7645772</td>\n",
       "      <td>2010</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0%</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>13</td>\n",
       "      <td>12.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>608</td>\n",
       "      <td>7642262</td>\n",
       "      <td>2010</td>\n",
       "      <td>0</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>0%</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>70</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>389</td>\n",
       "      <td>7641723</td>\n",
       "      <td>2010</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0%</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>16</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     index patentnumber  year  prophetic_patents  nonprophetic_patents  \\\n",
       "65     190      7645785  2010                 64                   148   \n",
       "94     242      7645891  2010                  1                   135   \n",
       "63     188      7645781  2010                  0                    91   \n",
       "186    611      7642267  2010                  0                    54   \n",
       "155    546      7642073  2010                  0                    42   \n",
       "187    614      7642276  2010                 10                    92   \n",
       "208    646      7642342  2010                  1                    48   \n",
       "133    456      7641905  2010                  0                    43   \n",
       "92     239      7645886  2010                  0                    29   \n",
       "66     191      7645786  2010                  5                     6   \n",
       "44     152      7645715  2010                  0                    31   \n",
       "2       39      7645336  2010                  0                    13   \n",
       "10      62      7645426  2010                  0                    14   \n",
       "75     207      7645813  2010                  0                    24   \n",
       "143    491      7641968  2010                  0                    20   \n",
       "74     204      7645804  2010                  0                    13   \n",
       "20      79      7645459  2010                  0                    20   \n",
       "58     182      7645772  2010                  0                    12   \n",
       "183    608      7642262  2010                  0                    70   \n",
       "110    389      7641723  2010                  0                    16   \n",
       "\n",
       "     unknown_patents mixed_tense_percentage  all_proph  some_proph  no_proph  \\\n",
       "65               168                     6%          0           1         0   \n",
       "94                14                     0%          0           1         0   \n",
       "63                 0                     0%          0           0         1   \n",
       "186                0                     0%          0           0         1   \n",
       "155                0                     0%          0           0         1   \n",
       "187               40                     0%          0           1         0   \n",
       "208                5                     0%          0           1         0   \n",
       "133                5                     0%          0           0         1   \n",
       "92                 0                     0%          0           0         1   \n",
       "66                42                     0%          0           1         0   \n",
       "44                 0                     0%          0           0         1   \n",
       "2                  0                     0%          0           0         1   \n",
       "10                 0                     0%          0           0         1   \n",
       "75                 0                     0%          0           0         1   \n",
       "143                0                     5%          0           0         1   \n",
       "74                 0                     0%          0           0         1   \n",
       "20                 0                     0%          0           0         1   \n",
       "58                 1                     0%          0           0         1   \n",
       "183                0                     0%          0           0         1   \n",
       "110                0                     0%          0           0         1   \n",
       "\n",
       "     prophetic  nonprophetic  issueyear  Total_Extracted  Total_Freilich  \\\n",
       "65         4.0         376.0     2010.0              380           380.0   \n",
       "94       122.0          28.0     2010.0              150           150.0   \n",
       "63        88.0           3.0     2010.0               91            91.0   \n",
       "186       43.0          11.0     2010.0               54            54.0   \n",
       "155       34.0           8.0     2010.0               42            42.0   \n",
       "187        0.0         141.0     2010.0              142           141.0   \n",
       "208       27.0          22.0     2010.0               54            49.0   \n",
       "133       25.0          18.0     2010.0               48            43.0   \n",
       "92        26.0           3.0     2010.0               29            29.0   \n",
       "66         0.0          53.0     2010.0               53            53.0   \n",
       "44         0.0           9.0     2010.0               31             9.0   \n",
       "2         12.0           0.0     2010.0               13            12.0   \n",
       "10        10.0           2.0     2010.0               14            12.0   \n",
       "75         0.0          12.0     2010.0               24            12.0   \n",
       "143        0.0           8.0     2010.0               20             8.0   \n",
       "74        11.0           2.0     2010.0               13            13.0   \n",
       "20        11.0           9.0     2010.0               20            20.0   \n",
       "58        10.0           2.0     2010.0               13            12.0   \n",
       "183        0.0          60.0     2010.0               70            60.0   \n",
       "110        0.0           8.0     2010.0               16             8.0   \n",
       "\n",
       "     prophetic_error  nonprophetic_error  Total_Mean_error  Sum_error  \n",
       "65              60.0               228.0               0.0      288.0  \n",
       "94             121.0               107.0               0.0      228.0  \n",
       "63              88.0                88.0               0.0      176.0  \n",
       "186             43.0                43.0               0.0       86.0  \n",
       "155             34.0                34.0               0.0       68.0  \n",
       "187             10.0                49.0               1.0       60.0  \n",
       "208             26.0                26.0               5.0       57.0  \n",
       "133             25.0                25.0               5.0       55.0  \n",
       "92              26.0                26.0               0.0       52.0  \n",
       "66               5.0                47.0               0.0       52.0  \n",
       "44               0.0                22.0              22.0       44.0  \n",
       "2               12.0                13.0               1.0       26.0  \n",
       "10              10.0                12.0               2.0       24.0  \n",
       "75               0.0                12.0              12.0       24.0  \n",
       "143              0.0                12.0              12.0       24.0  \n",
       "74              11.0                11.0               0.0       22.0  \n",
       "20              11.0                11.0               0.0       22.0  \n",
       "58              10.0                10.0               1.0       21.0  \n",
       "183              0.0                10.0              10.0       20.0  \n",
       "110              0.0                 8.0               8.0       16.0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sqlite3 as sql\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "def test_algorithm(year):\n",
    "    conn = sql.connect(\"../app/db/patents.db\")\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Execute query and fetch results\n",
    "    stats = cursor.execute(\"select * from patent_statistics\").fetchall()\n",
    "    stats_df = pd.DataFrame(stats, columns=[\"index\", \"patentnumber\",\"year\", \"prophetic_patents\", \"nonprophetic_patents\",\"unknown_patents\",\"mixed_tense_percentage\",\"all_proph\",\"some_proph\",\"no_proph\"])\n",
    "    print(f\"Number of patents in the database for {year}:  {len(stats_df[stats_df.year == year])}\")\n",
    "    conn.close()\n",
    "\n",
    "    df_year = df2[df2.issueyear == year][[\"patentnumber\",\"prophetic\",\"nonprophetic\",\"issueyear\"]]\n",
    "    df_year[\"patentnumber\"] = df_year[\"patentnumber\"].astype(str).transform(lambda x: x.replace(\".0\", \"\"))\n",
    "    \n",
    "\n",
    "    merged = stats_df.merge(df_year, on=\"patentnumber\")\n",
    "    print(f\"Number of patets: {len(merged)}\")\n",
    "    # final = dic_to_dic_w_tense_test(doc_w_exp,threshold=0)\n",
    "    # df_final = pd.DataFrame(final).T.reset_index()\n",
    "    df_final = merged\n",
    "    #df_final.columns = [\"patentnumber\",\"past\",\"present\",\"Unknown\"]\n",
    "    df_final[\"patentnumber\"] = df_final[\"patentnumber\"].apply(remove_leadiong_zeros)\n",
    "    df_check =merged\n",
    "    #df_check[\"past\"] = df_check[\"past\"] + df_check[\"Unknown\"]\n",
    "    df_check[\"Total_Extracted\"] = df_check[\"nonprophetic_patents\"] + df_check[\"prophetic_patents\"]  + df_check[\"unknown_patents\"]\n",
    "    df_check[\"Total_Freilich\"] = df_check[\"prophetic\"] + df_check[\"nonprophetic\"]\n",
    "    df_check[\"prophetic_error\"] = np.sqrt((df_check[\"prophetic\"] - df_check[\"prophetic_patents\"])**2)\n",
    "    df_check[\"nonprophetic_error\"] = np.sqrt((df_check[\"nonprophetic\"] - df_check[\"nonprophetic_patents\"])**2)\n",
    "    df_check[\"Total_Mean_error\"] = np.sqrt((df_check[\"Total_Freilich\"] - df_check[\"Total_Extracted\"])**2)\n",
    "    df_check[\"Sum_error\"] = df_check[\"prophetic_error\"] + df_check[\"nonprophetic_error\"] + df_check[\"Total_Mean_error\"]\n",
    "    same_as_janet = df_check[(df_check.prophetic_error ==0) & (df_check.nonprophetic_error ==0) & (df_check.Total_Mean_error ==0)]\n",
    "    same_total_as_janet = df_check[(df_check.Total_Extracted == df_check.Total_Freilich)]\n",
    "    same_proph_as_janet = df_check[df_check['prophetic_error'] == 0].shape[0]\n",
    "    same_nonproph_as_janet = df_check[df_check['nonprophetic_error'] == 0].shape[0]\n",
    "\n",
    "    print(f\"Number of exact matches: {len(same_as_janet)} out of {len(df_check)} , Percentage:      {round((len(same_as_janet)/len(df_check))*100,2)}%\")\n",
    "    print(f\"Number of exact num of patent extracted: {len(same_total_as_janet)} out of {len(df_check)},     {round((len(same_total_as_janet)/len(df_check)) *100, 2)}%\")\n",
    "    print(f\"Avg Total error: {round(df_check['Total_Mean_error'].mean(),2)}, num of corrects:        {df_check[df_check['Total_Mean_error'] == 0].shape[0]}\")\n",
    "    print(f\"Avg Total prophetic error: {round(df_check['prophetic_error'].mean(),2)}, num of corrects:{df_check[df_check['prophetic_error'] == 0].shape[0]},        {round((same_proph_as_janet/len(df_check))*100,2)}%\")\n",
    "    print(f\"Avg Total nonprophetic error: {round(df_check['nonprophetic_error'].mean(),2)}, num of corrects: {df_check[df_check['nonprophetic_error'] == 0].shape[0]},      {round((same_nonproph_as_janet/len(df_check))*100,2)}%\")\n",
    "    print(f\"{df_check['Sum_error'].sum()}\")\n",
    "    print(f\"Sum of number of Unknowns: {df_check['unknown_patents'].sum()}\")\n",
    "    return df_check\n",
    "df_check = test_algorithm(2010)\n",
    "df_check.sort_values(\"Sum_error\",ascending=False).head(20)\n",
    "# 2011\n",
    "# 943\n",
    "# Number of exact matches: 628 out of 943 , Percentage: 66.59597030752916\n",
    "# Number of exact num of patent extracted: 796 out of 943\n",
    "# Avg Total error: 1.6246023329798516, num of corrects:  796\n",
    "# Avg Total prophetic error: 1.1018027571580065, num of corrects: 721\n",
    "# Avg Total nonprophetic error: 2.482502651113468, num of corrects: 631\n",
    "# 4912.0\n",
    "# Sum of number of Unknowns: 1065\n",
    "# Number of patets with experiments extracted 943\n",
    "\n",
    "\n",
    "# 2015\n",
    "# 996\n",
    "# Number of exact matches: 636 out of 996 , Percentage: 63.85542168674698\n",
    "# Number of exact num of patent extracted: 828 out of 996\n",
    "# Avg Total error: 0.7710843373493976, num of corrects:  828\n",
    "# Avg Total prophetic error: 1.3744979919678715, num of corrects: 736\n",
    "# Avg Total nonprophetic error: 2.0973895582329316, num of corrects: 642\n",
    "# 4226.0\n",
    "# Sum of number of Unknowns: 461\n",
    "# Number of patets with experiments extracted 996"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of patents we extarcted less examples than janets: 30\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>patentnumber</th>\n",
       "      <th>year</th>\n",
       "      <th>prophetic_patents</th>\n",
       "      <th>nonprophetic_patents</th>\n",
       "      <th>unknown_patents</th>\n",
       "      <th>mixed_tense_percentage</th>\n",
       "      <th>all_proph</th>\n",
       "      <th>some_proph</th>\n",
       "      <th>no_proph</th>\n",
       "      <th>prophetic</th>\n",
       "      <th>nonprophetic</th>\n",
       "      <th>issueyear</th>\n",
       "      <th>Total_Extracted</th>\n",
       "      <th>Total_Freilich</th>\n",
       "      <th>prophetic_error</th>\n",
       "      <th>nonprophetic_error</th>\n",
       "      <th>Total_Mean_error</th>\n",
       "      <th>Sum_error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>550</th>\n",
       "      <td>2360</td>\n",
       "      <td>8932615</td>\n",
       "      <td>2015</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>50%</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599</th>\n",
       "      <td>2446</td>\n",
       "      <td>8932832</td>\n",
       "      <td>2015</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>33%</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>830</th>\n",
       "      <td>3043</td>\n",
       "      <td>8927015</td>\n",
       "      <td>2015</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>17%</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>24</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>862</th>\n",
       "      <td>3115</td>\n",
       "      <td>8927205</td>\n",
       "      <td>2015</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>50%</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>934</th>\n",
       "      <td>3225</td>\n",
       "      <td>8927541</td>\n",
       "      <td>2015</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>55%</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>20</td>\n",
       "      <td>21.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1135</th>\n",
       "      <td>3753</td>\n",
       "      <td>8956670</td>\n",
       "      <td>2015</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>43%</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>7</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1211</th>\n",
       "      <td>3888</td>\n",
       "      <td>8957046</td>\n",
       "      <td>2015</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>67%</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>3</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1360</th>\n",
       "      <td>4264</td>\n",
       "      <td>8951342</td>\n",
       "      <td>2015</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>38%</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>16</td>\n",
       "      <td>29.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>26.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2234</th>\n",
       "      <td>6159</td>\n",
       "      <td>8962507</td>\n",
       "      <td>2015</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0%</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2291</th>\n",
       "      <td>6228</td>\n",
       "      <td>8962633</td>\n",
       "      <td>2015</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>25%</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>4</td>\n",
       "      <td>26.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>44.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2791</th>\n",
       "      <td>7476</td>\n",
       "      <td>8968708</td>\n",
       "      <td>2015</td>\n",
       "      <td>4</td>\n",
       "      <td>34</td>\n",
       "      <td>6</td>\n",
       "      <td>7%</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>44</td>\n",
       "      <td>45.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2925</th>\n",
       "      <td>7701</td>\n",
       "      <td>8969280</td>\n",
       "      <td>2015</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>50%</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2973</th>\n",
       "      <td>7755</td>\n",
       "      <td>8969370</td>\n",
       "      <td>2015</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>32%</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>19</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3396</th>\n",
       "      <td>8637</td>\n",
       "      <td>8987465</td>\n",
       "      <td>2015</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0%</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>3</td>\n",
       "      <td>11.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3432</th>\n",
       "      <td>8683</td>\n",
       "      <td>8987536</td>\n",
       "      <td>2015</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>67%</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>6</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3448</th>\n",
       "      <td>8757</td>\n",
       "      <td>8988516</td>\n",
       "      <td>2015</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>93%</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>14</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4059</th>\n",
       "      <td>10158</td>\n",
       "      <td>9005984</td>\n",
       "      <td>2015</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>50%</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4120</th>\n",
       "      <td>10250</td>\n",
       "      <td>9006226</td>\n",
       "      <td>2015</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>100%</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4136</th>\n",
       "      <td>10268</td>\n",
       "      <td>9006263</td>\n",
       "      <td>2015</td>\n",
       "      <td>18</td>\n",
       "      <td>411</td>\n",
       "      <td>2</td>\n",
       "      <td>2%</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>427.0</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>431</td>\n",
       "      <td>433.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4253</th>\n",
       "      <td>10411</td>\n",
       "      <td>9006499</td>\n",
       "      <td>2015</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0%</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>13</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4325</th>\n",
       "      <td>10828</td>\n",
       "      <td>9011831</td>\n",
       "      <td>2015</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0%</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>5</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4505</th>\n",
       "      <td>11110</td>\n",
       "      <td>9012466</td>\n",
       "      <td>2015</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0%</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>3</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4582</th>\n",
       "      <td>11216</td>\n",
       "      <td>9012645</td>\n",
       "      <td>2015</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>14%</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>7</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4592</th>\n",
       "      <td>11227</td>\n",
       "      <td>9012662</td>\n",
       "      <td>2015</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>0%</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>21</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4838</th>\n",
       "      <td>11875</td>\n",
       "      <td>8999710</td>\n",
       "      <td>2015</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0%</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5016</th>\n",
       "      <td>12128</td>\n",
       "      <td>9000275</td>\n",
       "      <td>2015</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>67%</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>6</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5043</th>\n",
       "      <td>12422</td>\n",
       "      <td>8992471</td>\n",
       "      <td>2015</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>38%</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>8</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5087</th>\n",
       "      <td>12511</td>\n",
       "      <td>8992908</td>\n",
       "      <td>2015</td>\n",
       "      <td>7</td>\n",
       "      <td>38</td>\n",
       "      <td>4</td>\n",
       "      <td>37%</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>49</td>\n",
       "      <td>61.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>28.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5136</th>\n",
       "      <td>12578</td>\n",
       "      <td>8993017</td>\n",
       "      <td>2015</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0%</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5301</th>\n",
       "      <td>12848</td>\n",
       "      <td>8993660</td>\n",
       "      <td>2015</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0%</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      index patentnumber  year  prophetic_patents  nonprophetic_patents  \\\n",
       "550    2360      8932615  2015                  1                     1   \n",
       "599    2446      8932832  2015                  0                     3   \n",
       "830    3043      8927015  2015                  0                    24   \n",
       "862    3115      8927205  2015                  0                     4   \n",
       "934    3225      8927541  2015                 10                    10   \n",
       "1135   3753      8956670  2015                  5                     2   \n",
       "1211   3888      8957046  2015                  0                     3   \n",
       "1360   4264      8951342  2015                  3                    13   \n",
       "2234   6159      8962507  2015                  0                     4   \n",
       "2291   6228      8962633  2015                  1                     3   \n",
       "2791   7476      8968708  2015                  4                    34   \n",
       "2925   7701      8969280  2015                  0                     2   \n",
       "2973   7755      8969370  2015                  1                    18   \n",
       "3396   8637      8987465  2015                  0                     2   \n",
       "3432   8683      8987536  2015                  0                     6   \n",
       "3448   8757      8988516  2015                 14                     0   \n",
       "4059  10158      9005984  2015                  0                     4   \n",
       "4120  10250      9006226  2015                  0                     3   \n",
       "4136  10268      9006263  2015                 18                   411   \n",
       "4253  10411      9006499  2015                  0                    13   \n",
       "4325  10828      9011831  2015                  0                     5   \n",
       "4505  11110      9012466  2015                  0                     3   \n",
       "4582  11216      9012645  2015                  0                     7   \n",
       "4592  11227      9012662  2015                  0                    21   \n",
       "4838  11875      8999710  2015                  0                     2   \n",
       "5016  12128      9000275  2015                  0                     6   \n",
       "5043  12422      8992471  2015                  0                     8   \n",
       "5087  12511      8992908  2015                  7                    38   \n",
       "5136  12578      8993017  2015                  0                     4   \n",
       "5301  12848      8993660  2015                  1                     0   \n",
       "\n",
       "      unknown_patents mixed_tense_percentage  all_proph  some_proph  no_proph  \\\n",
       "550                 0                    50%          0           1         0   \n",
       "599                 0                    33%          0           0         1   \n",
       "830                 0                    17%          0           0         1   \n",
       "862                 0                    50%          0           0         1   \n",
       "934                 0                    55%          0           1         0   \n",
       "1135                0                    43%          0           1         0   \n",
       "1211                0                    67%          0           0         1   \n",
       "1360                0                    38%          0           1         0   \n",
       "2234                0                     0%          0           0         1   \n",
       "2291                0                    25%          0           1         0   \n",
       "2791                6                     7%          0           1         0   \n",
       "2925                0                    50%          0           0         1   \n",
       "2973                0                    32%          0           1         0   \n",
       "3396                1                     0%          0           0         1   \n",
       "3432                0                    67%          0           0         1   \n",
       "3448                0                    93%          1           0         0   \n",
       "4059                0                    50%          0           0         1   \n",
       "4120                0                   100%          0           0         1   \n",
       "4136                2                     2%          0           1         0   \n",
       "4253                0                     0%          0           0         1   \n",
       "4325                0                     0%          0           0         1   \n",
       "4505                0                     0%          0           0         1   \n",
       "4582                0                    14%          0           0         1   \n",
       "4592                0                     0%          0           0         1   \n",
       "4838                0                     0%          0           0         1   \n",
       "5016                0                    67%          0           0         1   \n",
       "5043                0                    38%          0           0         1   \n",
       "5087                4                    37%          0           1         0   \n",
       "5136                0                     0%          0           0         1   \n",
       "5301                0                     0%          1           0         0   \n",
       "\n",
       "      prophetic  nonprophetic  issueyear  Total_Extracted  Total_Freilich  \\\n",
       "550         1.0           2.0     2015.0                2             3.0   \n",
       "599         0.0           4.0     2015.0                3             4.0   \n",
       "830         1.0          24.0     2015.0               24            25.0   \n",
       "862         0.0           5.0     2015.0                4             5.0   \n",
       "934        20.0           1.0     2015.0               20            21.0   \n",
       "1135        2.0           6.0     2015.0                7             8.0   \n",
       "1211        0.0          14.0     2015.0                3            14.0   \n",
       "1360       13.0          16.0     2015.0               16            29.0   \n",
       "2234        1.0           4.0     2015.0                4             5.0   \n",
       "2291        2.0          24.0     2015.0                4            26.0   \n",
       "2791        4.0          41.0     2015.0               44            45.0   \n",
       "2925        0.0           3.0     2015.0                2             3.0   \n",
       "2973        0.0          20.0     2015.0               19            20.0   \n",
       "3396        7.0           4.0     2015.0                3            11.0   \n",
       "3432        1.0           6.0     2015.0                6             7.0   \n",
       "3448       15.0           0.0     2015.0               14            15.0   \n",
       "4059        0.0           5.0     2015.0                4             5.0   \n",
       "4120        2.0           2.0     2015.0                3             4.0   \n",
       "4136        6.0         427.0     2015.0              431           433.0   \n",
       "4253        0.0          14.0     2015.0               13            14.0   \n",
       "4325        1.0           5.0     2015.0                5             6.0   \n",
       "4505        0.0          11.0     2015.0                3            11.0   \n",
       "4582        0.0           8.0     2015.0                7             8.0   \n",
       "4592        0.0          22.0     2015.0               21            22.0   \n",
       "4838        0.0           3.0     2015.0                2             3.0   \n",
       "5016        0.0           7.0     2015.0                6             7.0   \n",
       "5043        0.0           9.0     2015.0                8             9.0   \n",
       "5087       15.0          46.0     2015.0               49            61.0   \n",
       "5136        0.0           5.0     2015.0                4             5.0   \n",
       "5301        1.0           1.0     2015.0                1             2.0   \n",
       "\n",
       "      prophetic_error  nonprophetic_error  Total_Mean_error  Sum_error  \n",
       "550               0.0                 1.0               1.0        2.0  \n",
       "599               0.0                 1.0               1.0        2.0  \n",
       "830               1.0                 0.0               1.0        2.0  \n",
       "862               0.0                 1.0               1.0        2.0  \n",
       "934              10.0                 9.0               1.0       20.0  \n",
       "1135              3.0                 4.0               1.0        8.0  \n",
       "1211              0.0                11.0              11.0       22.0  \n",
       "1360             10.0                 3.0              13.0       26.0  \n",
       "2234              1.0                 0.0               1.0        2.0  \n",
       "2291              1.0                21.0              22.0       44.0  \n",
       "2791              0.0                 7.0               1.0        8.0  \n",
       "2925              0.0                 1.0               1.0        2.0  \n",
       "2973              1.0                 2.0               1.0        4.0  \n",
       "3396              7.0                 2.0               8.0       17.0  \n",
       "3432              1.0                 0.0               1.0        2.0  \n",
       "3448              1.0                 0.0               1.0        2.0  \n",
       "4059              0.0                 1.0               1.0        2.0  \n",
       "4120              2.0                 1.0               1.0        4.0  \n",
       "4136             12.0                16.0               2.0       30.0  \n",
       "4253              0.0                 1.0               1.0        2.0  \n",
       "4325              1.0                 0.0               1.0        2.0  \n",
       "4505              0.0                 8.0               8.0       16.0  \n",
       "4582              0.0                 1.0               1.0        2.0  \n",
       "4592              0.0                 1.0               1.0        2.0  \n",
       "4838              0.0                 1.0               1.0        2.0  \n",
       "5016              0.0                 1.0               1.0        2.0  \n",
       "5043              0.0                 1.0               1.0        2.0  \n",
       "5087              8.0                 8.0              12.0       28.0  \n",
       "5136              0.0                 1.0               1.0        2.0  \n",
       "5301              0.0                 1.0               1.0        2.0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"Num of patents we extarcted less examples than janets: {len(df_check[(df_check.Total_Extracted<df_check.Total_Freilich)])}\")\n",
    "df_check[(df_check.Total_Extracted<df_check.Total_Freilich)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of patents we extarcted more examples than janets: 1993\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>patentnumber</th>\n",
       "      <th>year</th>\n",
       "      <th>prophetic_patents</th>\n",
       "      <th>nonprophetic_patents</th>\n",
       "      <th>unknown_patents</th>\n",
       "      <th>mixed_tense_percentage</th>\n",
       "      <th>all_proph</th>\n",
       "      <th>some_proph</th>\n",
       "      <th>no_proph</th>\n",
       "      <th>prophetic</th>\n",
       "      <th>nonprophetic</th>\n",
       "      <th>issueyear</th>\n",
       "      <th>Total_Extracted</th>\n",
       "      <th>Total_Freilich</th>\n",
       "      <th>prophetic_error</th>\n",
       "      <th>nonprophetic_error</th>\n",
       "      <th>Total_Mean_error</th>\n",
       "      <th>Sum_error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1155</td>\n",
       "      <td>8935891</td>\n",
       "      <td>2015</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>0%</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>8</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1175</td>\n",
       "      <td>8936159</td>\n",
       "      <td>2015</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>14%</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>7</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1185</td>\n",
       "      <td>8936359</td>\n",
       "      <td>2015</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>17%</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>6</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1195</td>\n",
       "      <td>8936517</td>\n",
       "      <td>2015</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>14%</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>21</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1200</td>\n",
       "      <td>8936619</td>\n",
       "      <td>2015</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>12%</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>16</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5374</th>\n",
       "      <td>12943</td>\n",
       "      <td>8993823</td>\n",
       "      <td>2015</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>7%</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>14</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5376</th>\n",
       "      <td>12948</td>\n",
       "      <td>8993833</td>\n",
       "      <td>2015</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>31%</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>16</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5377</th>\n",
       "      <td>12949</td>\n",
       "      <td>8993837</td>\n",
       "      <td>2015</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>55%</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>11</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5382</th>\n",
       "      <td>12954</td>\n",
       "      <td>8993843</td>\n",
       "      <td>2015</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>75%</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5386</th>\n",
       "      <td>12967</td>\n",
       "      <td>8993882</td>\n",
       "      <td>2015</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>100%</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1993 rows Ã— 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      index patentnumber  year  prophetic_patents  nonprophetic_patents  \\\n",
       "0      1155      8935891  2015                  0                     6   \n",
       "6      1175      8936159  2015                  0                     6   \n",
       "7      1185      8936359  2015                  0                     6   \n",
       "8      1195      8936517  2015                 10                     7   \n",
       "10     1200      8936619  2015                  1                    14   \n",
       "...     ...          ...   ...                ...                   ...   \n",
       "5374  12943      8993823  2015                  1                     9   \n",
       "5376  12948      8993833  2015                  2                    14   \n",
       "5377  12949      8993837  2015                  0                    10   \n",
       "5382  12954      8993843  2015                  2                     2   \n",
       "5386  12967      8993882  2015                  2                     3   \n",
       "\n",
       "      unknown_patents mixed_tense_percentage  all_proph  some_proph  no_proph  \\\n",
       "0                   2                     0%          0           0         1   \n",
       "6                   1                    14%          0           0         1   \n",
       "7                   0                    17%          0           0         1   \n",
       "8                   4                    14%          0           1         0   \n",
       "10                  1                    12%          0           1         0   \n",
       "...               ...                    ...        ...         ...       ...   \n",
       "5374                4                     7%          0           1         0   \n",
       "5376                0                    31%          0           1         0   \n",
       "5377                1                    55%          0           0         1   \n",
       "5382                0                    75%          0           1         0   \n",
       "5386                0                   100%          0           1         0   \n",
       "\n",
       "      prophetic  nonprophetic  issueyear  Total_Extracted  Total_Freilich  \\\n",
       "0           0.0           6.0     2015.0                8             6.0   \n",
       "6           0.0           6.0     2015.0                7             6.0   \n",
       "7           0.0           3.0     2015.0                6             3.0   \n",
       "8          10.0           7.0     2015.0               21            17.0   \n",
       "10          0.0          15.0     2015.0               16            15.0   \n",
       "...         ...           ...        ...              ...             ...   \n",
       "5374        0.0           7.0     2015.0               14             7.0   \n",
       "5376        0.0          10.0     2015.0               16            10.0   \n",
       "5377        0.0          10.0     2015.0               11            10.0   \n",
       "5382        0.0           3.0     2015.0                4             3.0   \n",
       "5386        0.0           4.0     2015.0                5             4.0   \n",
       "\n",
       "      prophetic_error  nonprophetic_error  Total_Mean_error  Sum_error  \n",
       "0                 0.0                 0.0               2.0        2.0  \n",
       "6                 0.0                 0.0               1.0        1.0  \n",
       "7                 0.0                 3.0               3.0        6.0  \n",
       "8                 0.0                 0.0               4.0        4.0  \n",
       "10                1.0                 1.0               1.0        3.0  \n",
       "...               ...                 ...               ...        ...  \n",
       "5374              1.0                 2.0               7.0       10.0  \n",
       "5376              2.0                 4.0               6.0       12.0  \n",
       "5377              0.0                 0.0               1.0        1.0  \n",
       "5382              2.0                 1.0               1.0        4.0  \n",
       "5386              2.0                 1.0               1.0        4.0  \n",
       "\n",
       "[1993 rows x 19 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"Num of patents we extarcted more examples than janets: {len(df_check[(df_check.Total_Extracted>df_check.Total_Freilich)])}\")\n",
    "df_check[(df_check.Total_Extracted>df_check.Total_Freilich)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sql.connect(\"../app/db/patents.db\")\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Execute query and fetch results\n",
    "stats = cursor.execute(\"select * from patent_statistics\").fetchall()\n",
    "stats_df = pd.DataFrame(stats, columns=[\"index\", \"patentnumber\",\"year\", \"prophetic_patents\", \"nonprophetic_patents\",\"unknown_patents\",\"mixed_tense_percentage\",\"all_proph\",\"some_proph\",\"no_proph\"])\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>patentnumber</th>\n",
       "      <th>year</th>\n",
       "      <th>prophetic_patents</th>\n",
       "      <th>nonprophetic_patents</th>\n",
       "      <th>unknown_patents</th>\n",
       "      <th>mixed_tense_percentage</th>\n",
       "      <th>all_proph</th>\n",
       "      <th>some_proph</th>\n",
       "      <th>no_proph</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>7650659</td>\n",
       "      <td>2010</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>0%</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>7650666</td>\n",
       "      <td>2010</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0%</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>7650742</td>\n",
       "      <td>2010</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0%</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>7650751</td>\n",
       "      <td>2010</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0%</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>7650756</td>\n",
       "      <td>2010</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0%</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1144</th>\n",
       "      <td>1145</td>\n",
       "      <td>7644361</td>\n",
       "      <td>2010</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>100%</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1145</th>\n",
       "      <td>1146</td>\n",
       "      <td>7644376</td>\n",
       "      <td>2010</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0%</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1146</th>\n",
       "      <td>1147</td>\n",
       "      <td>7644383</td>\n",
       "      <td>2010</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>33%</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1147</th>\n",
       "      <td>1148</td>\n",
       "      <td>7644409</td>\n",
       "      <td>2010</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11%</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1148</th>\n",
       "      <td>1149</td>\n",
       "      <td>7644442</td>\n",
       "      <td>2010</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0%</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>786 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      index patentnumber  year  prophetic_patents  nonprophetic_patents  \\\n",
       "0         1      7650659  2010                  1                    24   \n",
       "1         2      7650666  2010                  0                     1   \n",
       "2         3      7650742  2010                  2                     0   \n",
       "3         4      7650751  2010                  0                     0   \n",
       "4         5      7650756  2010                  1                     0   \n",
       "...     ...          ...   ...                ...                   ...   \n",
       "1144   1145      7644361  2010                  1                     0   \n",
       "1145   1146      7644376  2010                  4                     0   \n",
       "1146   1147      7644383  2010                  2                     1   \n",
       "1147   1148      7644409  2010                  9                     0   \n",
       "1148   1149      7644442  2010                  3                     0   \n",
       "\n",
       "      unknown_patents mixed_tense_percentage  all_proph  some_proph  no_proph  \n",
       "0                   0                     0%          0           1         0  \n",
       "1                   0                     0%          0           0         1  \n",
       "2                   0                     0%          1           0         0  \n",
       "3                   1                     0%          0           0         1  \n",
       "4                   0                     0%          1           0         0  \n",
       "...               ...                    ...        ...         ...       ...  \n",
       "1144                0                   100%          1           0         0  \n",
       "1145                0                     0%          1           0         0  \n",
       "1146                0                    33%          0           1         0  \n",
       "1147                0                    11%          1           0         0  \n",
       "1148                0                     0%          1           0         0  \n",
       "\n",
       "[786 rows x 10 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_df = stats_df[\n",
    "    (stats_df.year == 2010) & \n",
    "    (~stats_df.patentnumber.isin(df_check.patentnumber.values))\n",
    "]\n",
    "filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patentnumber</th>\n",
       "      <th>xml</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7641702</td>\n",
       "      <td>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\\n&lt;!DOCT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7641704</td>\n",
       "      <td>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\\n&lt;!DOCT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7641709</td>\n",
       "      <td>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\\n&lt;!DOCT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7641721</td>\n",
       "      <td>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\\n&lt;!DOCT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7641723</td>\n",
       "      <td>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\\n&lt;!DOCT...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  patentnumber                                                xml\n",
       "0      7641702  <?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n<!DOCT...\n",
       "1      7641704  <?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n<!DOCT...\n",
       "2      7641709  <?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n<!DOCT...\n",
       "3      7641721  <?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n<!DOCT...\n",
       "4      7641723  <?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n<!DOCT..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset = read_json(\"patent_xmls_2010.json\")\n",
    "test_dataset_df = pd.DataFrame.from_dict(test_dataset, orient='index').reset_index()\n",
    "test_dataset_df.columns = ['patentnumber', 'xml']\n",
    "test_dataset_df.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m merged \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241m.\u001b[39mmerge(df2, on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpatentnumber\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(merged))\n\u001b[0;32m      3\u001b[0m merged\u001b[38;5;241m.\u001b[39mhead()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "merged = df.merge(df2, on=\"patentnumber\")\n",
    "print(len(merged))\n",
    "merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ipg110104.xml... (1/3)\n",
      "Processed 100/5130...\n",
      "Processed 200/5130...\n",
      "Processed 300/5130...\n",
      "Processed 400/5130...\n",
      "Processed 500/5130...\n",
      "Processed 600/5130...\n",
      "Processed 700/5130...\n",
      "Processed 800/5130...\n",
      "Processed 900/5130...\n",
      "Processed 1000/5130...\n",
      "Processed 1100/5130...\n",
      "Processed 1200/5130...\n",
      "Processed 1300/5130...\n",
      "Processed 1400/5130...\n",
      "Processed 1500/5130...\n",
      "Processed 1600/5130...\n",
      "Processed 1700/5130...\n",
      "Processed 1800/5130...\n",
      "Processed 1900/5130...\n",
      "Processed 2000/5130...\n",
      "Processed 2100/5130...\n",
      "Processed 2200/5130...\n",
      "Processed 2300/5130...\n",
      "Processed 2400/5130...\n",
      "Processed 2500/5130...\n",
      "Processed 2600/5130...\n",
      "Processed 2700/5130...\n",
      "Processed 2800/5130...\n",
      "Processed 2900/5130...\n",
      "Processed 3000/5130...\n",
      "Processed 3100/5130...\n",
      "Processed 3200/5130...\n",
      "Processed 3300/5130...\n",
      "Processed 3400/5130...\n",
      "Processed 3500/5130...\n",
      "Processed 3600/5130...\n",
      "Processed 3700/5130...\n",
      "Processed 3800/5130...\n",
      "Processed 3900/5130...\n",
      "Processed 4000/5130...\n",
      "Processed 4100/5130...\n",
      "Processed 4200/5130...\n",
      "Processed 4300/5130...\n",
      "Processed 4400/5130...\n",
      "Processed 4500/5130...\n",
      "Processed 4600/5130...\n",
      "Processed 4700/5130...\n",
      "Processed 4800/5130...\n",
      "Processed 4900/5130...\n",
      "Processed 5000/5130...\n",
      "Processed 5100/5130...\n",
      "Processing ipg110111.xml... (2/3)\n",
      "Processed 100/5160...\n",
      "Processed 200/5160...\n",
      "Processed 300/5160...\n",
      "Processed 400/5160...\n",
      "Processed 500/5160...\n",
      "Processed 600/5160...\n",
      "Processed 700/5160...\n",
      "Processed 800/5160...\n",
      "Processed 900/5160...\n",
      "Processed 1000/5160...\n",
      "Processed 1100/5160...\n",
      "Processed 1200/5160...\n",
      "Processed 1300/5160...\n",
      "Processed 1400/5160...\n",
      "Processed 1500/5160...\n",
      "Processed 1600/5160...\n",
      "Processed 1700/5160...\n",
      "Processed 1800/5160...\n",
      "Processed 1900/5160...\n",
      "Processed 2000/5160...\n",
      "Processed 2100/5160...\n",
      "Processed 2200/5160...\n",
      "Processed 2300/5160...\n",
      "Processed 2400/5160...\n",
      "Processed 2500/5160...\n",
      "Processed 2600/5160...\n",
      "Processed 2700/5160...\n",
      "Processed 2800/5160...\n",
      "Processed 2900/5160...\n",
      "Processed 3000/5160...\n",
      "Processed 3100/5160...\n",
      "Processed 3200/5160...\n",
      "Processed 3300/5160...\n",
      "Processed 3400/5160...\n",
      "Processed 3500/5160...\n",
      "Processed 3600/5160...\n",
      "Processed 3700/5160...\n",
      "Processed 3800/5160...\n",
      "Processed 3900/5160...\n",
      "Processed 4000/5160...\n",
      "Processed 4100/5160...\n",
      "Processed 4200/5160...\n",
      "Processed 4300/5160...\n",
      "Processed 4400/5160...\n",
      "Processed 4500/5160...\n",
      "Processed 4600/5160...\n",
      "Processed 4700/5160...\n",
      "Processed 4800/5160...\n",
      "Processed 4900/5160...\n",
      "Processed 5000/5160...\n",
      "Processed 5100/5160...\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "folder_path = \"../app/data/patent_grants_2011\"\n",
    "file_names = os.listdir(\"../app/data/patent_grants_2011\")\n",
    "doc_w_exp = {}\n",
    "for i, file in enumerate(file_names):\n",
    "    all_xml_parts = []\n",
    "    if file.endswith(\".xml\"):\n",
    "        print(f\"Processing {file}... ({i + 1}/{len(file_names)})\")\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "        try:\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "                content = file.read()\n",
    "                parts = content.split('<?xml version=\"1.0\" encoding=\"UTF-8\"?>')\n",
    "                parts = [p for p in parts if p.strip()]\n",
    "                all_xml_parts.extend(parts)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file}: {str(e)}\")\n",
    "        # xml_no_dup = remove_duplicate_docs(all_xml_parts)\n",
    "        # print(f\"Num of duplicates removed: {len(all_xml_parts) - len(xml_no_dup)} out of {len(all_xml_parts)}\")\n",
    "        for j, xml in enumerate(all_xml_parts):\n",
    "            if j % 100 == 0 and j > 1:\n",
    "                print(f\"Processed {j}/{len(all_xml_parts)}...\")\n",
    "            if len(xml) <= 2000:\n",
    "                pass\n",
    "            s_tags = re.findall(r\"<s\\d+>.*?</s\\d+>\", xml)\n",
    "            if len(s_tags) > 0 or '<sequence-cwu id=\"SEQLST-0\">' in xml:\n",
    "                pass\n",
    "\n",
    "            heading = extract_experiments_w_heading(xml)\n",
    "\n",
    "            # Process examples based on heading presence\n",
    "            if heading and len(heading) == 1:\n",
    "                #found_heading += 1\n",
    "                examples = extract_examples_start_w_word(\n",
    "                    heading[0].find_next_siblings()\n",
    "                )\n",
    "                if len(examples) == 0:\n",
    "                    soup = BeautifulSoup(xml, \"xml\")\n",
    "                    siblings = soup.find_all([\"heading\", \"p\"])\n",
    "                    examples = extract_examples_start_w_word(siblings)\n",
    "            else:\n",
    "                #not_found_heading += 1\n",
    "                soup = BeautifulSoup(xml, \"xml\")\n",
    "                siblings = soup.find_all([\"heading\", \"p\"])\n",
    "                examples = extract_examples_start_w_word(siblings)\n",
    "\n",
    "            if len(examples) > 0:\n",
    "                doc_num = remove_leadiong_zeros(find_doc_number(xml)[0])\n",
    "                doc_w_exp[doc_num] = examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'nonprophetic_patents'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3621\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3620\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3621\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3622\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx:136\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx:163\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5198\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5206\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'nonprophetic_patents'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[1;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 9>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m df_check \u001b[38;5;241m=\u001b[39mmerged\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m#df_check[\"past\"] = df_check[\"past\"] + df_check[\"Unknown\"]\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m df_check[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal_Extracted\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf_check\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnonprophetic_patents\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m+\u001b[39m df_check[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprophetic_patents\u001b[39m\u001b[38;5;124m\"\u001b[39m]  \u001b[38;5;66;03m#+ df_check[\"Unknown\"]\u001b[39;00m\n\u001b[0;32m     10\u001b[0m df_check[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal_Freilich\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m df_check[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprophetic\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m df_check[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonprophetic\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     11\u001b[0m df_check[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprophetic_error\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msqrt((df_check[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprophetic\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m-\u001b[39m df_check[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprophetic_patents\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:3505\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3503\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   3504\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3505\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3506\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3507\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3623\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3621\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3622\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m-> 3623\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3624\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3625\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3626\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3627\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3628\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'nonprophetic_patents'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# final = dic_to_dic_w_tense_test(doc_w_exp,threshold=0)\n",
    "# df_final = pd.DataFrame(final).T.reset_index()\n",
    "df_final = merged\n",
    "#df_final.columns = [\"patentnumber\",\"past\",\"present\",\"Unknown\"]\n",
    "df_final[\"patentnumber\"] = df_final[\"patentnumber\"].apply(remove_leadiong_zeros)\n",
    "df_check =merged\n",
    "#df_check[\"past\"] = df_check[\"past\"] + df_check[\"Unknown\"]\n",
    "df_check[\"Total_Extracted\"] = df_check[\"nonprophetic_patents\"] + df_check[\"prophetic_patents\"]  #+ df_check[\"Unknown\"]\n",
    "df_check[\"Total_Freilich\"] = df_check[\"prophetic\"] + df_check[\"nonprophetic\"]\n",
    "df_check[\"prophetic_error\"] = np.sqrt((df_check[\"prophetic\"] - df_check[\"prophetic_patents\"])**2)\n",
    "df_check[\"nonprophetic_error\"] = np.sqrt((df_check[\"nonprophetic\"] - df_check[\"nonprophetic_patents\"])**2)\n",
    "df_check[\"Total_Mean_error\"] = np.sqrt((df_check[\"Total_Freilich\"] - df_check[\"Total_Extracted\"])**2)\n",
    "df_check[\"Sum_error\"] = df_check[\"prophetic_error\"] + df_check[\"nonprophetic_error\"] + df_check[\"Total_Mean_error\"]\n",
    "print(f\"Number of exact matches: {len(df_check[(df_check.prophetic_error ==0) & (df_check.nonprophetic_error ==0) & (df_check.Total_Mean_error ==0)])} out of {len(df_check)} , Percentage: {len(df_check[(df_check.prophetic_error ==0) & (df_check.nonprophetic_error ==0) & (df_check.Total_Mean_error ==0)])/len(df_check)*100}\")\n",
    "print(f\"Number of exact num of patent extracted: {len(df_check[(df_check.Total_Extracted == df_check.Total_Freilich)])} out of {len(df_check)}\")\n",
    "print(f\"Avg Total error: {df_check['Total_Mean_error'].mean()}, num of corrects:  {df_check[df_check['Total_Mean_error'] == 0].shape[0]}\")\n",
    "print(f\"Avg Total prophetic error: {df_check['prophetic_error'].mean()}, num of corrects: {df_check[df_check['prophetic_error'] == 0].shape[0]}\")\n",
    "print(f\"Avg Total nonprophetic error: {df_check['nonprophetic_error'].mean()}, num of corrects: {df_check[df_check['nonprophetic_error'] == 0].shape[0]}\")\n",
    "print(f\"{df_check['Sum_error'].sum()}\")\n",
    "print(f\"Sum of number of Unknowns: {df._check['unknown_patents'].sum()}\")\n",
    "print(f\"Number of patets with experiments extracted {len(df_check)}\")\n",
    "df_check.sort_values(\"Sum_error\",ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities.nlp_processing import analyze_sentence_tense\n",
    "def dic_to_dic_w_tense_test(doc_w_exp, threshold=0):\n",
    "    dic = {}\n",
    "    pattern = r\"\\(\\d+\\)\\s*([A-Za-z0-9\\-\\(\\)\\{\\},:;=\\[\\]\\+\\*\\s\\.\\^\\$\\%]+(?:\\.(?:sup|delta|Hz|NMR)[^\\)]*)?)\"\n",
    "\n",
    "    for key, value in doc_w_exp.items():\n",
    "        tense_counts = {\"past\": 0, \"present\": 0, \"unknown\": 0}\n",
    "\n",
    "        if isinstance(value, list) and len(value) == 1:\n",
    "            desc =  \"\".join(value[0][\"content\"]) # value[0][\"title\"] + \".\" +\n",
    "            if len(desc) > threshold:\n",
    "                tense = analyze_sentence_tense(desc)\n",
    "                if tense != \"unknown\":\n",
    "                    tense_counts[tense] += 1\n",
    "                else:\n",
    "                    matches = re.findall(pattern, desc)\n",
    "                    if matches:\n",
    "                        tense_counts[\"past\"] += 1\n",
    "                    else:\n",
    "                        tense_counts[\"unknown\"] += 1\n",
    "                dic[key] = tense_counts\n",
    "\n",
    "        elif isinstance(value, list) and len(value) > 1:\n",
    "            for ls in value:\n",
    "                desc = \"\".join(ls[\"content\"]) #ls[\"title\"] + \".\" + \n",
    "                if len(desc) > threshold:\n",
    "                    if len(desc) > 0:\n",
    "                        tense = analyze_sentence_tense(desc)\n",
    "\n",
    "                        if tense != \"unknown\":\n",
    "                            tense_counts[tense] += 1\n",
    "                        else:\n",
    "                            matches = re.findall(pattern, desc)\n",
    "                            if matches:\n",
    "                                tense_counts[\"past\"] += 1\n",
    "                            else:\n",
    "                                tense_counts[\"unknown\"] += 1\n",
    "            dic[key] = tense_counts\n",
    "\n",
    "        # elif isinstance(value, dict):\n",
    "        #     print(value)\n",
    "        #     for ex, desc in value.items():\n",
    "        #         if len(desc) > threshold:\n",
    "        #             tense = analyze_sentence_tense(desc)\n",
    "        #             if tense != \"unknown\":\n",
    "        #                 tense_counts[tense] += 1\n",
    "        #             else:\n",
    "        #                 matches = re.findall(pattern, desc)\n",
    "        #                 if matches:\n",
    "        #                     tense_counts[\"past\"] += 1\n",
    "        #                 else:\n",
    "        #                     tense_counts[\"unknown\"] += 1\n",
    "        #     dic[key] = tense_counts\n",
    "        # else:\n",
    "        #     print(type(value))\n",
    "        #     print(value)\n",
    "        #     print(key)\n",
    "\n",
    "    return dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found_heading: 7172, not_found_heading: 3713, gibberish: 3111,too_short: 379\n",
      "5000 patents MAE: 0.0, total_error: 0, highest_difference: 0, highest_difference_patent: \n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def extract_examples_start_w_word(xml_siblings):\n",
    "    examples = []\n",
    "    current_example = None\n",
    "    in_example = False\n",
    "\n",
    "    for tag in xml_siblings:\n",
    "        if tag.name == \"heading\":\n",
    "            if (\n",
    "                tag.text.strip().lower().startswith(\"example\")\n",
    "                or tag.text.strip().lower().startswith(\"experiment\")\n",
    "                or tag.text.strip().lower().startswith(\"test\")\n",
    "                or tag.text.strip().lower().startswith(\"trial\")\n",
    "                or \"test\" in tag.text.strip().lower()\n",
    "                or \"experiment\" in tag.text.strip().lower()\n",
    "                or \"example\" in tag.text.strip().lower()\n",
    "                or \"trial\" in tag.text.strip().lower()\n",
    "            ):\n",
    "                in_example = True\n",
    "                current_example = {\n",
    "                    \"number\": tag.text.strip(),\n",
    "                    \"title\": xml_siblings[xml_siblings.index(tag) + 1].text.strip(),\n",
    "                    \"content\": [],\n",
    "                }\n",
    "                examples.append(current_example)\n",
    "        elif tag.name == \"heading\" and (\n",
    "            tag.text.strip().lower().startswith(\"example\")\n",
    "            or tag.text.strip().lower().startswith(\"experiment\")\n",
    "            or tag.text.strip().lower().startswith(\"test\")\n",
    "            or tag.text.strip().lower().startswith(\"trial\")\n",
    "            or \"test\" in tag.text.strip().lower()\n",
    "            or \"experiment\" in tag.text.strip().lower()\n",
    "            or \"example\" in tag.text.strip().lower()\n",
    "            or \"trial\" in tag.text.strip().lower()\n",
    "        ):\n",
    "            in_example = False\n",
    "        # else:\n",
    "        #     # If we hit any other heading, stop collecting content\n",
    "        #     in_example = False\n",
    "        elif in_example and current_example is not None:\n",
    "            current_example[\"content\"].append(tag.text.strip())\n",
    "\n",
    "    return examples\n",
    "num_of_paterns = 5000\n",
    "mae = 0\n",
    "highest_difference = 0\n",
    "found_heading = 0\n",
    "not_found_heading = 0\n",
    "gib = 0\n",
    "short = 0\n",
    "mostdifss = []\n",
    "doc_w_exp = {}\n",
    "\n",
    "for row in test_dataset_df.iterrows():\n",
    "    xml = row[1][\"xml\"]\n",
    "    if len(xml)>2000:\n",
    "        s_tags = re.findall(r'<s\\d+>.*?</s\\d+>', xml)\n",
    "        if len(s_tags) > 0:\n",
    "            #print(f\"Patent {row[1]['patentnumber']} is gibberish\")\n",
    "            gib+=1\n",
    "        else:\n",
    "            heading = extract_experiments_w_heading(xml)\n",
    "            #janetsnumexamples = row[1][\"prophetic\"] + row[1][\"nonprophetic\"]\n",
    "\n",
    "            if heading:\n",
    "                found_heading += 1\n",
    "                examples = extract_examples_start_w_word(heading[0].find_next_siblings())\n",
    "                if len(examples)==0:\n",
    "                    soup = BeautifulSoup(xml, 'xml')\n",
    "                    siblings = soup.find_all(['heading', 'p'])\n",
    "                    examples = extract_examples_start_w_word(siblings)\n",
    "                numexamples = len(examples)\n",
    "            else:\n",
    "                not_found_heading += 1\n",
    "                soup = BeautifulSoup(xml, 'xml')\n",
    "                siblings = soup.find_all(['heading', 'p'])\n",
    "                examples = extract_examples_start_w_word(siblings)\n",
    "                numexamples = len(examples)\n",
    "\n",
    "\n",
    "\n",
    "            # #difference = abs(numexamples-janetsnumexamples)\n",
    "            # mae += difference\n",
    "            # if difference > 0:\n",
    "            #     mostdifss.append([difference, row[1][\"patentnumber\"]])\n",
    "            # if difference > highest_difference:\n",
    "            #     highest_difference = difference\n",
    "            #     highest_difference_patent = row[1][\"patentnumber\"]\n",
    "            if len(examples)>0:\n",
    "                doc_w_exp[row[1][\"patentnumber\"]] = examples\n",
    "\n",
    "    else:\n",
    "        short+=1\n",
    "        #print(f\"skipping {row[1]['patentnumber']}, patent is too short\")\n",
    "        \n",
    "        \n",
    "print(f\"found_heading: {found_heading}, not_found_heading: {not_found_heading}, gibberish: {gib},too_short: {short}\")\n",
    "print(f\"{num_of_paterns} patents MAE: {mae/num_of_paterns}, total_error: {mae}, highest_difference: {highest_difference}, highest_difference_patent: \") #highest_difference_patent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1750"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(doc_w_exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discovered prefixes:\n",
      "content_number\n",
      "example        75.285318\n",
      "comparative     4.797215\n",
      "examples        3.159456\n",
      "reference       2.611387\n",
      "synthesis       2.134245\n",
      "industrial      2.005287\n",
      "preparation     1.966600\n",
      "Name: count, dtype: float64\n",
      "\n",
      "Discovered special phrases:\n",
      "['comparative example 1', 'comparative example 2', 'comparative example 3', 'comparative example 4']\n"
     ]
    }
   ],
   "source": [
    "a = []\n",
    "num_examples = 0\n",
    "for k, pat_exs in doc_w_exp.items():\n",
    "    num_examples += len(pat_exs)\n",
    "    for ex in pat_exs:\n",
    "        ola = \"\".join(ex[\"content\"]) + ex[\"title\"]\n",
    "        if len(ola) == 0:\n",
    "            #print(ex)\n",
    "            continue\n",
    "        a.append({\n",
    "            'patent_number': k,\n",
    "            'content_number': ex[\"number\"]\n",
    "        })\n",
    "df_ex_types = pd.DataFrame(a)\n",
    "val_counts= df_ex_types.content_number.value_counts()\n",
    "valc_df = pd.DataFrame(val_counts).reset_index()\n",
    "\n",
    "# Group by type and sum counts\n",
    "def get_example_type(content_number):\n",
    "    content_lower = content_number.lower()\n",
    "    for prefix in start_w:\n",
    "        if content_lower.startswith(prefix):\n",
    "            return prefix\n",
    "    for special in ls:\n",
    "        if special in content_lower:\n",
    "            return special\n",
    "    return \"other\"\n",
    "\n",
    "# Extract first word from each content_number\n",
    "def get_first_word(text):\n",
    "    return text.lower().split()[0]\n",
    "\n",
    "# Get common prefixes (appearing more than 100 times)\n",
    "common_prefixes = (df_ex_types['content_number']\n",
    "                  .apply(get_first_word)\n",
    "                  .value_counts()\n",
    "                  .loc[lambda x: x > 100])\n",
    "\n",
    "start_w = common_prefixes.index.tolist()\n",
    "print(\"Discovered prefixes:\")\n",
    "print(common_prefixes/df_ex_types.shape[0] * 100)\n",
    "\n",
    "# Function to detect patterns that occur in middle of text\n",
    "def find_common_phrases(df, min_count=50):\n",
    "    # Get all content numbers as lowercase\n",
    "    texts = df['content_number'].str.lower()\n",
    "    # Find phrases with 2 or more words that appear frequently\n",
    "    phrases = texts[texts.str.contains(' .+ ')]  # Contains at least 2 spaces\n",
    "    common_phrases = phrases.value_counts().loc[lambda x: x > min_count]\n",
    "    return common_phrases.index.tolist()\n",
    "\n",
    "ls = find_common_phrases(df_ex_types)\n",
    "print(\"\\nDiscovered special phrases:\")\n",
    "print(ls)\n",
    "\n",
    "# Rest of your grouping code remains the same\n",
    "valc_df['type'] = valc_df['content_number'].apply(get_example_type)\n",
    "type_counts = valc_df.groupby('type')['count'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discovered prefixes (% of unique patents):\n",
      "content_number\n",
      "example        64.571429\n",
      "industrial     17.771429\n",
      "examples       16.914286\n",
      "comparative    13.314286\n",
      "Name: count, dtype: float64\n",
      "\n",
      "Discovered special phrases:\n",
      "['comparative example 1', 'comparative example 2', 'comparative example 3', 'comparative example 4']\n",
      "\n",
      "Number of unique patents per type:\n",
      "type\n",
      "example                  1319\n",
      "other                     476\n",
      "industrial                311\n",
      "comparative               233\n",
      "comparative example 1       5\n",
      "comparative example 2       3\n",
      "Name: patent_number, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "a = []\n",
    "num_examples = 0\n",
    "for k, pat_exs in doc_w_exp.items():\n",
    "    num_examples += len(pat_exs)\n",
    "    for ex in pat_exs:\n",
    "        ola = \"\".join(ex[\"content\"]) + ex[\"title\"]\n",
    "        if len(ola) == 0:\n",
    "            continue\n",
    "        a.append({\n",
    "            'patent_number': k,\n",
    "            'content_number': ex[\"number\"]\n",
    "        })\n",
    "\n",
    "df_ex_types = pd.DataFrame(a)\n",
    "\n",
    "# Function definitions remain the same\n",
    "def get_example_type(content_number):\n",
    "    content_lower = content_number.lower()\n",
    "    for prefix in start_w:\n",
    "        if content_lower.startswith(prefix):\n",
    "            return prefix\n",
    "    for special in ls:\n",
    "        if special in content_lower:\n",
    "            return special\n",
    "    return \"other\"\n",
    "\n",
    "def get_first_word(text):\n",
    "    return text.lower().split()[0]\n",
    "\n",
    "# Get common prefixes (appearing in more than 100 unique patents)\n",
    "common_prefixes = (df_ex_types.groupby('patent_number')['content_number']\n",
    "                  .apply(lambda x: x.apply(get_first_word).unique().tolist())\n",
    "                  .explode()\n",
    "                  .value_counts()\n",
    "                  .loc[lambda x: x > 100])\n",
    "\n",
    "start_w = common_prefixes.index.tolist()\n",
    "print(\"Discovered prefixes (% of unique patents):\")\n",
    "print(common_prefixes/df_ex_types['patent_number'].nunique() * 100)\n",
    "\n",
    "def find_common_phrases(df, min_count=50):\n",
    "    # Get unique patent-phrase combinations\n",
    "    patent_phrases = (df.groupby('patent_number')['content_number']\n",
    "                     .apply(lambda x: x.str.lower().unique().tolist())\n",
    "                     .explode())\n",
    "    phrases = patent_phrases[patent_phrases.str.contains(' .+ ')]\n",
    "    common_phrases = phrases.value_counts().loc[lambda x: x > min_count]\n",
    "    return common_phrases.index.tolist()\n",
    "\n",
    "ls = find_common_phrases(df_ex_types)\n",
    "print(\"\\nDiscovered special phrases:\")\n",
    "print(ls)\n",
    "\n",
    "# Add type column to original dataframe\n",
    "df_ex_types['type'] = df_ex_types['content_number'].apply(get_example_type)\n",
    "\n",
    "# Count unique patents per type\n",
    "type_counts = df_ex_types.groupby('type')['patent_number'].nunique().sort_values(ascending=False)\n",
    "print(\"\\nNumber of unique patents per type:\")\n",
    "print(type_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Unique content numbers: 23329\n",
      "\n",
      "Content number frequencies:\n",
      "content_number\n",
      "Example 1                       6682\n",
      "Example 2                       6024\n",
      "Example 3                       5260\n",
      "Example 4                       4561\n",
      "Example 5                       3855\n",
      "                                ... \n",
      "Example #N.1.1                     1\n",
      "Example #O.1.1                     1\n",
      "Example #X.1.1                     1\n",
      "Example #38                        1\n",
      "VI. Industrial Applicability       1\n",
      "Name: count, Length: 23329, dtype: int64\n",
      "\n",
      "Unique patent numbers: 7965\n",
      "\n",
      "Patent number frequencies:\n",
      "patent_number\n",
      "8952157    1446\n",
      "8987441    1040\n",
      "8969587     996\n",
      "8987242     901\n",
      "9006265     800\n",
      "           ... \n",
      "9034169       1\n",
      "8956508       1\n",
      "8956528       1\n",
      "8956536       1\n",
      "8986522       1\n",
      "Name: count, Length: 7965, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nUnique content numbers:\", len(df_ex_types['content_number'].unique()))\n",
    "print(\"\\nContent number frequencies:\")\n",
    "print(df_ex_types['content_number'].value_counts())\n",
    "\n",
    "print(\"\\nUnique patent numbers:\", len(df_ex_types['patent_number'].unique()))\n",
    "print(\"\\nPatent number frequencies:\")\n",
    "print(df_ex_types['patent_number'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INDUSTRIAL APPLICABILITY  :  30\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of patents with difference > 0: 104\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[4.0, '8926548'],\n",
       " [1.0, '8926595'],\n",
       " [1.0, '8926732'],\n",
       " [1.0, '8926824'],\n",
       " [6.0, '8926862'],\n",
       " [4.0, '8926864'],\n",
       " [1.0, '8926872'],\n",
       " [1.0, '8926955'],\n",
       " [11.0, '8926966'],\n",
       " [7.0, '8926979'],\n",
       " [1.0, '8927015'],\n",
       " [1.0, '8927023'],\n",
       " [5.0, '8927036'],\n",
       " [2.0, '8927098'],\n",
       " [1.0, '8927150'],\n",
       " [2.0, '8927157'],\n",
       " [1.0, '8927236'],\n",
       " [4.0, '8927254'],\n",
       " [1.0, '8927429'],\n",
       " [1.0, '8927469'],\n",
       " [1.0, '8927479'],\n",
       " [1.0, '8927536'],\n",
       " [1.0, '8927541'],\n",
       " [1.0, '8927546'],\n",
       " [2.0, '8927568'],\n",
       " [4.0, '8927583'],\n",
       " [6.0, '8927588'],\n",
       " [3.0, '8927596'],\n",
       " [1.0, '8927617'],\n",
       " [1.0, '8927619'],\n",
       " [2.0, '8927631'],\n",
       " [1.0, '8927642'],\n",
       " [1.0, '8927647'],\n",
       " [1.0, '8927678'],\n",
       " [1.0, '8927681'],\n",
       " [2.0, '8927683'],\n",
       " [1.0, '8927698'],\n",
       " [7.0, '8927710'],\n",
       " [15.0, '8927721'],\n",
       " [1.0, '8927725'],\n",
       " [1.0, '8927730'],\n",
       " [1.0, '8927738'],\n",
       " [1.0, '8927746'],\n",
       " [1.0, '8927750'],\n",
       " [1.0, '8927777'],\n",
       " [1.0, '8927781'],\n",
       " [1.0, '8927801'],\n",
       " [9.0, '8927811'],\n",
       " [1.0, '8932406'],\n",
       " [3.0, '8932470'],\n",
       " [4.0, '8932480'],\n",
       " [2.0, '8932491'],\n",
       " [2.0, '8932492'],\n",
       " [1.0, '8932579'],\n",
       " [3.0, '8932591'],\n",
       " [1.0, '8932615'],\n",
       " [2.0, '8932626'],\n",
       " [2.0, '8932688'],\n",
       " [4.0, '8932705'],\n",
       " [6.0, '8932717'],\n",
       " [1.0, '8932842'],\n",
       " [1.0, '8932853'],\n",
       " [2.0, '8932867'],\n",
       " [1.0, '8932981'],\n",
       " [1.0, '8933000'],\n",
       " [2.0, '8933012'],\n",
       " [23.0, '8933032'],\n",
       " [12.0, '8933042'],\n",
       " [2.0, '8933065'],\n",
       " [1.0, '8933067'],\n",
       " [1.0, '8933071'],\n",
       " [56.0, '8933072'],\n",
       " [1.0, '8933074'],\n",
       " [1.0, '8933094'],\n",
       " [3.0, '8933100'],\n",
       " [2.0, '8933103'],\n",
       " [3.0, '8933105'],\n",
       " [2.0, '8933117'],\n",
       " [15.0, '8933131'],\n",
       " [1.0, '8933135'],\n",
       " [1.0, '8933181'],\n",
       " [1.0, '8933204'],\n",
       " [1.0, '8933221'],\n",
       " [2.0, '8933224'],\n",
       " [1.0, '8933228'],\n",
       " [6.0, '8933229'],\n",
       " [1.0, '8933251'],\n",
       " [1.0, '8933267'],\n",
       " [1.0, '8933294'],\n",
       " [3.0, '8933318'],\n",
       " [1.0, '8934216'],\n",
       " [1.0, '8934637'],\n",
       " [1.0, '8936675'],\n",
       " [1.0, '8936685'],\n",
       " [1.0, '8936706'],\n",
       " [1.0, '8936833'],\n",
       " [1.0, '8936836'],\n",
       " [3.0, '8936849'],\n",
       " [3.0, '8936859'],\n",
       " [3.0, '8936910'],\n",
       " [1.0, '8936939'],\n",
       " [1.0, '8937025'],\n",
       " [1.0, '8937045'],\n",
       " [6.0, '8937055']]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"Number of patents with difference > 0: {len(mostdifss)}\")\n",
    "mostdifss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patentnumber</th>\n",
       "      <th>prophetic</th>\n",
       "      <th>nonprophetic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8285</th>\n",
       "      <td>8933072</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     patentnumber  prophetic  nonprophetic\n",
       "8285      8933072        0.0           9.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# highest difference\n",
    "df2[df2.patentnumber == \"8933072\"][[\"patentnumber\",\"prophetic\",\"nonprophetic\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2897131579.py, line 34)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[16], line 34\u001b[1;36m\u001b[0m\n\u001b[1;33m    Number of exact matches: 2870 out of 3971 , Percentage: 72.27398640141023\u001b[0m\n\u001b[1;37m           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "final = dic_to_dic_w_tense_test(doc_w_exp,threshold=0)\n",
    "df_final = pd.DataFrame(final).T.reset_index()\n",
    "df_final.columns = [\"patentnumber\",\"past\",\"present\",\"Unknown\"]\n",
    "df_final[\"patentnumber\"] = df_final[\"patentnumber\"].apply(remove_leadiong_zeros)\n",
    "df_check = df_final.merge(merged,on= \"patentnumber\",how=\"left\")[[\"patentnumber\",\"present\",\"past\",\"Unknown\",\"prophetic\",\"nonprophetic\",\"allprophetic\",\"someprophetic\"]]\n",
    "#df_check[\"past\"] = df_check[\"past\"] + df_check[\"Unknown\"]\n",
    "df_check[\"Total_Extracted\"] = df_check[\"past\"] + df_check[\"present\"]  #+ df_check[\"Unknown\"]\n",
    "df_check[\"Total_Freilich\"] = df_check[\"prophetic\"] + df_check[\"nonprophetic\"]\n",
    "df_check[\"prophetic_error\"] = np.sqrt((df_check[\"prophetic\"] - df_check[\"present\"])**2)\n",
    "df_check[\"nonprophetic_error\"] = np.sqrt((df_check[\"nonprophetic\"] - df_check[\"past\"])**2)\n",
    "df_check[\"Total_Mean_error\"] = np.sqrt((df_check[\"Total_Freilich\"] - df_check[\"Total_Extracted\"])**2)\n",
    "df_check[\"Sum_error\"] = df_check[\"prophetic_error\"] + df_check[\"nonprophetic_error\"] + df_check[\"Total_Mean_error\"]\n",
    "print(f\"Number of exact matches: {len(df_check[(df_check.prophetic_error ==0) & (df_check.nonprophetic_error ==0) & (df_check.Total_Mean_error ==0)])} out of {len(df_check)} , Percentage: {len(df_check[(df_check.prophetic_error ==0) & (df_check.nonprophetic_error ==0) & (df_check.Total_Mean_error ==0)])/len(df_check)*100}\")\n",
    "print(f\"Number of exact num of patent extracted: {len(df_check[(df_check.Total_Extracted == df_check.Total_Freilich)])} out of {len(df_check)}\")\n",
    "print(f\"Avg Total error: {df_check['Total_Mean_error'].mean()}, num of corrects:  {df_check[df_check['Total_Mean_error'] == 0].shape[0]}\")\n",
    "print(f\"Avg Total prophetic error: {df_check['prophetic_error'].mean()}, num of corrects: {df_check[df_check['prophetic_error'] == 0].shape[0]}\")\n",
    "print(f\"Avg Total nonprophetic error: {df_check['nonprophetic_error'].mean()}, num of corrects: {df_check[df_check['nonprophetic_error'] == 0].shape[0]}\")\n",
    "print(f\"{df_check['Sum_error'].sum()}\")\n",
    "print(f\"Sum of number of Unknowns: {df_check['Unknown'].sum()}\")\n",
    "print(f\"Number of patets with experiments extracted {len(df_check)}\")\n",
    "df_check.sort_values(\"Sum_error\",ascending=False).head(20)\n",
    "\n",
    "\n",
    "# 1000\n",
    "# Number of exact matches: 577 out of 799 , Percentage: 72.21526908635795\n",
    "# Number of exact num of patent extracted: 694 out of 799\n",
    "# Avg Total error: 0.5181476846057572, num of corrects:  694\n",
    "# Avg Total prophetic error: 0.5857321652065082, num of corrects: 652\n",
    "# Avg Total nonprophetic error: 0.9887359198998749, num of corrects: 587\n",
    "# 1672.0\n",
    "# Sum of number of Unknowns: 0\n",
    "# Number of patets with experiments extracted 799\n",
    "\n",
    "# 5000\n",
    "# Number of exact matches: 2870 out of 3971 , Percentage: 72.27398640141023\n",
    "# Number of exact num of patent extracted: 3461 out of 3971\n",
    "# Avg Total error: 0.5560312263913372, num of corrects:  3461\n",
    "# Avg Total prophetic error: 0.8645177537144296, num of corrects: 3209\n",
    "# Avg Total nonprophetic error: 1.2976580206497104, num of corrects: 2942\n",
    "# 10794.0\n",
    "# Sum of number of Unknowns: 0\n",
    "# Number of patets with experiments extracted 3971"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# patent 9102614 injanets says 4 examples but there are 161\n",
    "# patent 9102599 in janets says 1 ,theres more than 1 actually \n",
    "# Some xml files are made with error : e.g. these dont have much information : skipping 9102601, 9102662,9102692\n",
    "# these have gibberish: 9102724,9102705,9102727,9102628"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_siblings_test(xml_siblings):\n",
    "    examples = []\n",
    "\n",
    "    # Find all matching headings directly from xml_siblings\n",
    "    example_headings = [\n",
    "        tag\n",
    "        for tag in xml_siblings\n",
    "        if tag.name == \"heading\"\n",
    "        and any(\n",
    "            keyword in tag.text.strip().lower().replace(\" \", \"\")\n",
    "            for keyword in [\"example\", \"experiment\", \"test\"]\n",
    "        )\n",
    "        # and not any(\n",
    "        #     excluded in tag.text.strip().lower().replace(\" \", \"\")\n",
    "        #     for excluded in [\"reference\", \"preparation\"]\n",
    "        # )\n",
    "    ]\n",
    "\n",
    "    for heading in example_headings:\n",
    "        current_content = []\n",
    "        idx = xml_siblings.index(heading)\n",
    "\n",
    "        # Get title from next heading if available\n",
    "        title = \"\"\n",
    "        if idx + 1 < len(xml_siblings) and xml_siblings[idx + 1].name == \"heading\":\n",
    "            title = xml_siblings[idx + 1].text.strip()\n",
    "\n",
    "        # Collect content until next example heading\n",
    "        i = idx + 1\n",
    "        while i < len(xml_siblings):\n",
    "            if (\n",
    "                xml_siblings[i].name == \"heading\"\n",
    "                and any(\n",
    "                    keyword in xml_siblings[i].text.strip().lower().replace(\" \", \"\")\n",
    "                    for keyword in [\"example\", \"experiment\", \"test\"]\n",
    "                )\n",
    "                # and not any(\n",
    "                #     excluded in xml_siblings[i].text.strip().lower().replace(\" \", \"\")\n",
    "                #     for excluded in [\"reference\", \"preparation\"]\n",
    "                # )\n",
    "            ):\n",
    "                break\n",
    "            if xml_siblings[i].name == \"p\":\n",
    "                current_content.append(xml_siblings[i].text.strip())\n",
    "            i += 1\n",
    "\n",
    "        examples.append(\n",
    "            {\"number\": heading.text.strip(), \"title\": title, \"content\": current_content}\n",
    "        )\n",
    "\n",
    "    return examples if examples else None\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "has more than 1 Examples heading, 349\n",
      "has more than 1 Examples heading, 388\n",
      "has more than 1 Examples heading, 689\n",
      "has more than 1 Examples heading, 768\n",
      "has more than 1 Examples heading, 781\n",
      "has more than 1 Examples heading, 814\n",
      "1000/25081 so far found 771 docs with experiments\n"
     ]
    }
   ],
   "source": [
    "def extract_examples_from_heading(heading):\n",
    "    extracted_examples = []\n",
    "    example_start_w = process_siblings(heading.find_next_siblings())\n",
    "    if example_start_w:\n",
    "        if not example_start_w[0][\"content\"]:\n",
    "            extracted_ex_w_word = extract_examples_start_w_word(heading.find_next_siblings())\n",
    "            if extracted_ex_w_word:\n",
    "                if isinstance(extracted_ex_w_word, list):\n",
    "                    if extracted_ex_w_word and len(extracted_ex_w_word[0][\"content\"])>0:\n",
    "                        extracted_examples.append(extracted_ex_w_word)\n",
    "                elif extracted_ex_w_word[\"content\"]:\n",
    "                    extracted_examples.append(extracted_ex_w_word)\n",
    "        else:\n",
    "            if len(example_start_w[0][\"content\"])>0:\n",
    "                extracted_examples.append(example_start_w)\n",
    "    else:\n",
    "        extracted_ex_w_word = extract_examples_start_w_word(heading.find_next_siblings())\n",
    "        if extracted_ex_w_word:\n",
    "            if isinstance(extracted_ex_w_word, list):\n",
    "                if extracted_ex_w_word and len(extracted_ex_w_word[0][\"content\"])>0:\n",
    "                    extracted_examples.append(extracted_ex_w_word)\n",
    "            elif extracted_ex_w_word[\"content\"]:\n",
    "                if len(extracted_ex_w_word[\"content\"])>0:\n",
    "                    extracted_examples.append(extracted_ex_w_word)\n",
    "        else:\n",
    "            num_dot_examples = extract_num_dot_examples(str(heading.find_next_siblings()))\n",
    "            if num_dot_examples:\n",
    "                extracted_examples.append(num_dot_examples)\n",
    "    return extracted_examples\n",
    "\n",
    "def extract_examples(xml):\n",
    "    heading = extract_experiments_w_heading(xml)\n",
    "    if heading:\n",
    "        if len(heading) > 1:\n",
    "            print(f\"has more than 1 Examples heading, {i}\")\n",
    "        elif len(heading) == 1:\n",
    "            return extract_examples_from_heading(heading[0])\n",
    "        else:\n",
    "            extracted_ex_w_word = extract_examples_w_word(xml)\n",
    "            if extracted_ex_w_word:\n",
    "                if isinstance(extracted_ex_w_word, list):\n",
    "                    if extracted_ex_w_word and extracted_ex_w_word[0][\"content\"]:\n",
    "                        return [extracted_ex_w_word]\n",
    "                elif extracted_ex_w_word[\"content\"]:\n",
    "                    return [extracted_ex_w_word]\n",
    "            else:\n",
    "                example_start_w = process_siblings(heading[0].find_next_siblings())\n",
    "                if example_start_w and example_start_w[0][\"content\"]:\n",
    "                    return [example_start_w]\n",
    "    else: \n",
    "        example_start_w = extract_examples_w_word(xml)\n",
    "        if example_start_w:\n",
    "            return [example_start_w]\n",
    "        else:\n",
    "            num_dot_examples = extract_num_dot_examples(xml)\n",
    "            if num_dot_examples:\n",
    "                return [num_dot_examples]\n",
    "    return []\n",
    "\n",
    "doc_w_exp = {}\n",
    "for i, xml in enumerate(merged[\"xml\"][:1000], start=1):\n",
    "    if i % 1000 == 0:\n",
    "        print(f\"{i}/{len(merged.xml)} so far found {len(doc_w_exp)} docs with experiments\")\n",
    "    \n",
    "    extracted_examples = extract_examples(xml)\n",
    "    if extracted_examples:\n",
    "        doc_w_exp[find_doc_number(xml)[0]] = extracted_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/25081 so far found 54 docs with experiments\n",
      "200/25081 so far found 111 docs with experiments\n",
      "300/25081 so far found 169 docs with experiments\n",
      "has more than 1 Examples heading, 349\n",
      "has more than 1 Examples heading, 388\n",
      "400/25081 so far found 225 docs with experiments\n",
      "500/25081 so far found 294 docs with experiments\n",
      "600/25081 so far found 352 docs with experiments\n",
      "has more than 1 Examples heading, 689\n",
      "700/25081 so far found 395 docs with experiments\n",
      "has more than 1 Examples heading, 768\n",
      "has more than 1 Examples heading, 781\n",
      "800/25081 so far found 463 docs with experiments\n",
      "has more than 1 Examples heading, 814\n",
      "900/25081 so far found 522 docs with experiments\n",
      "1000/25081 so far found 575 docs with experiments\n"
     ]
    }
   ],
   "source": [
    "def extract_examples_from_heading(heading):\n",
    "    length_threshold = 30\n",
    "    extracted_examples = []\n",
    "    example_start_w = process_siblings(heading.find_next_siblings())\n",
    "    if example_start_w:\n",
    "        if not example_start_w[0][\"content\"]:\n",
    "            extracted_ex_w_word = extract_examples_start_w_word(heading.find_next_siblings())\n",
    "            if extracted_ex_w_word:\n",
    "                if isinstance(extracted_ex_w_word, list):\n",
    "                    extracted_ex_w_word = [ex for ex in extracted_ex_w_word if len(ex[\"content\"])>length_threshold]\n",
    "                    extracted_examples.append(extracted_ex_w_word)\n",
    "                elif extracted_ex_w_word[\"content\"]>length_threshold:\n",
    "                    extracted_examples.append(extracted_ex_w_word)\n",
    "        else:\n",
    "            if isinstance(example_start_w, list):\n",
    "                example_start_w = [ex for ex in example_start_w if len(ex[\"content\"])>length_threshold]\n",
    "                extracted_examples.append(example_start_w)\n",
    "    else:\n",
    "        extracted_ex_w_word = extract_examples_start_w_word(heading.find_next_siblings())\n",
    "        if extracted_ex_w_word:\n",
    "            if isinstance(extracted_ex_w_word, list):\n",
    "                extracted_ex_w_word = [ex for ex in extracted_ex_w_word if len(ex[\"content\"])>length_threshold]\n",
    "                extracted_examples.append(extracted_ex_w_word)\n",
    "            elif len(extracted_ex_w_word[\"content\"])>length_threshold:\n",
    "                extracted_examples.append(extracted_ex_w_word)\n",
    "        else:\n",
    "            num_dot_examples = extract_num_dot_examples(str(heading.find_next_siblings()))\n",
    "            if isinstance(num_dot_examples, list):\n",
    "                num_dot_examples = [ex for ex in num_dot_examples if len(ex[\"content\"])>length_threshold]\n",
    "                extracted_examples.append(num_dot_examples)\n",
    "            elif isinstance(num_dot_examples, dict):\n",
    "                num_dot_examples = [ex for ex in num_dot_examples.items() if len(ex[\"content\"])>length_threshold]\n",
    "\n",
    "                extracted_examples.append(num_dot_examples)\n",
    "    return extracted_examples\n",
    "\n",
    "def extract_examples(xml):\n",
    "    length_threshold = 30\n",
    "    heading = extract_experiments_w_heading(xml)\n",
    "    if heading:\n",
    "        if len(heading) > 1:\n",
    "            print(f\"has more than 1 Examples heading, {i}\")\n",
    "        elif len(heading) == 1:\n",
    "            return extract_examples_from_heading(heading[0])\n",
    "        else:\n",
    "            pass\n",
    "        # else:\n",
    "        #     extracted_ex_w_word = extract_examples_w_word(xml)\n",
    "        #     if extracted_ex_w_word:\n",
    "        #         if isinstance(extracted_ex_w_word, list):\n",
    "        #             extracted_ex_w_word = [ex for ex in extracted_ex_w_word if len(ex[\"content\"])>length_threshold]\n",
    "        #             return extracted_ex_w_word if extracted_ex_w_word\n",
    "        #         elif extracted_ex_w_word[\"content\"]>length_threshold:\n",
    "        #             return extracted_ex_w_word if extracted_ex_w_word\n",
    "        #     else:\n",
    "        #         example_start_w = process_siblings(heading[0].find_next_siblings())\n",
    "        #         if isinstance(example_start_w, list):\n",
    "        #             example_start_w = [ex for ex in example_start_w if len(ex[\"content\"])>length_threshold]\n",
    "        #             return example_start_w if example_start_w\n",
    "    else: \n",
    "        example_start_w = extract_examples_w_word(xml)\n",
    "        if isinstance(example_start_w, list):\n",
    "            example_start_w = [ex for ex in example_start_w if len(ex[\"content\"])>length_threshold]\n",
    "            return example_start_w\n",
    "        else:\n",
    "            num_dot_examples = extract_num_dot_examples(xml)\n",
    "            if isinstance(num_dot_examples, list):\n",
    "                num_dot_examples = [ex for ex in num_dot_examples if len(ex[\"content\"])>length_threshold]\n",
    "                return num_dot_examples\n",
    "\n",
    "doc_w_exp = {}\n",
    "for i, xml in enumerate(merged[\"xml\"][:1000], start=1):\n",
    "    if i % 100 == 0:\n",
    "        print(f\"{i}/{len(merged.xml)} so far found {len(doc_w_exp)} docs with experiments\")\n",
    "    \n",
    "    extracted_examples = extract_examples(xml)\n",
    "    if extracted_examples:\n",
    "        doc_w_exp[find_doc_number(xml)[0]] = extracted_examples\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "has more than 1 Examples heading, 349\n",
      "has more than 1 Examples heading, 388\n",
      "has more than 1 Examples heading, 689\n",
      "has more than 1 Examples heading, 768\n",
      "has more than 1 Examples heading, 781\n",
      "has more than 1 Examples heading, 814\n",
      "1000/25081 so far found 769 docs with experiments\n",
      "has more than 1 Examples heading, 1054\n",
      "has more than 1 Examples heading, 1417\n",
      "has more than 1 Examples heading, 1475\n",
      "has more than 1 Examples heading, 1810\n"
     ]
    }
   ],
   "source": [
    "\n",
    "doc_w_exp = {}\n",
    "for i, xml in enumerate(merged[:1999].xml.values, start=1):\n",
    "    if i % 1000 == 0:\n",
    "        print(f\"{i}/{len(merged.xml)} so far found {len(doc_w_exp)} docs with experiments\")\n",
    "    \n",
    "    extracted_examples = extract_examples(xml)\n",
    "    if extracted_examples:\n",
    "        doc_w_exp[find_doc_number(xml)[0]] = extracted_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 18977 patents from doc_w_exp_new_algorithm.pkl\n"
     ]
    }
   ],
   "source": [
    "# save_as_pickle(doc_w_exp,\"doc_w_exp_new_algorithm.pkl\")\n",
    "doc_w_exp = load_from_pickle(\"doc_w_exp_new_algorithm.pkl\")\n",
    "\n",
    "# save_as_json(doc_w_exp, \"doc_w_exp_2015.json\")\n",
    "# doc_w_exp = read_json(\"../data/doc_w_exp_2015.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import html\n",
    "import unicodedata\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def clean_text_updated(text):\n",
    "    \"\"\"\n",
    "    Clean text by removing HTML tags, special characters, extra spaces,\n",
    "    and normalizing the content while keeping meaningful punctuation.\n",
    "\n",
    "    Args:\n",
    "        text (str): Input text to clean\n",
    "\n",
    "    Returns:\n",
    "        str: Cleaned text\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "\n",
    "    # Decode HTML entities & remove HTML tags\n",
    "    text = html.unescape(text)\n",
    "    text = BeautifulSoup(text, \"html.parser\").get_text()\n",
    "\n",
    "    # Normalize Unicode characters (e.g., Ã© â†’ e)\n",
    "    text = unicodedata.normalize(\"NFKC\", text)\n",
    "\n",
    "    # Remove unwanted special characters but keep punctuation\n",
    "    text = re.sub(r\"[^\\w\\s.,!?'\\-]\", \"\", text)\n",
    "\n",
    "    # Normalize spaces: remove multiple spaces, newlines, and tabs\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "def check_tense_nltk_updated(sentence):\n",
    "    words = word_tokenize(sentence)\n",
    "    tagged = pos_tag(words)\n",
    "\n",
    "    # Define tense categories\n",
    "    past = {\"VBD\", \"VBN\"}\n",
    "    present = {\"VB\", \"VBG\", \"VBP\", \"VBZ\"}\n",
    "    future = {\"MD\"}\n",
    "\n",
    "    tenses = {\"past\": 0, \"present\": 0, \"future\": 0}\n",
    "\n",
    "    for i, (word, tag) in enumerate(tagged):\n",
    "        # Count past tense words\n",
    "        if tag in past:\n",
    "            tenses[\"past\"] += 1\n",
    "\n",
    "        # Count present tense words\n",
    "        elif tag in present:\n",
    "            tenses[\"present\"] += 1\n",
    "\n",
    "        # Future tense handling\n",
    "        elif tag in future:\n",
    "            if word.lower() in {\"will\", \"shall\"}:\n",
    "                # Ensure 'will' or 'shall' is followed by a verb (to confirm future tense)\n",
    "                if i + 1 < len(tagged) and tagged[i + 1][1] in {\"VB\", \"VBP\"}:\n",
    "                    tenses[\"future\"] += 1\n",
    "\n",
    "    return max(tenses, key=tenses.get) if max(tenses.values()) > 0 else \"Unknown\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from utilities.nlp_processing import check_tense_nltk,clean_text\n",
    "\n",
    "def tense_with_pattern(description):\n",
    "    future_pattern = r'\\b(?:will|would|shall|should)\\b'\n",
    "    past_pattern = r'\\b(?:was|were|had|did)\\b'\n",
    "\n",
    "    future_matches = re.findall(future_pattern, description)\n",
    "    past_matches = re.findall(past_pattern, description)\n",
    "    if future_matches and past_matches:\n",
    "        if len(future_matches) > len(past_matches):\n",
    "            return \"present\"\n",
    "        else:\n",
    "            return \"past\"\n",
    "    if future_pattern or past_pattern:\n",
    "        if future_pattern:\n",
    "            return \"present\"\n",
    "        else:\n",
    "            return \"past\"\n",
    "    return \"Unkown\"\n",
    "\n",
    "\n",
    "def dic_to_dic_w_tense(doc_w_exp,threshold = 50):\n",
    "    dic = {}\n",
    "    pattern = r'\\(\\d+\\)\\s*([A-Za-z0-9\\-\\(\\)\\{\\},:;=\\[\\]\\+\\*\\s\\.\\^\\$\\%]+(?:\\.(?:sup|delta|Hz|NMR)[^\\)]*)?)'\n",
    "    pattern2 = r'\\(\\d+\\)\\s*(?:[A-Za-z]+\\s*\\d*\\-?[A-Za-z]*[\\(\\{][^)]*[\\)\\}][^;]*|(?:\\.\\d*H\\-?NMR[^;]*|\\d+\\.[a-zA-Z]*\\d*))'\n",
    "\n",
    "\n",
    "    for key, value in doc_w_exp.items():\n",
    "        tense_counts = {\"past\": 0, \"present\": 0,\"Unknown\":0}\n",
    "        \n",
    "        if isinstance(value, list) and len(value) == 1 and len(value[0][0][\"content\"]) > threshold:\n",
    "            desc = value[0][0][\"title\"] + \".\" + \"\".join(value[0][0][\"content\"])\n",
    "            tense = check_tense_nltk_updated(clean_text(desc))\n",
    "            if tense !=  \"Unknown\":\n",
    "                tense_counts[tense] += 1\n",
    "            else:\n",
    "                matches = re.findall(pattern, desc)\n",
    "                if matches:\n",
    "                    tense_counts[\"past\"] += 1\n",
    "                else:\n",
    "                    tense_counts[\"Unknown\"] += 1\n",
    "            dic[key] = tense_counts\n",
    "\n",
    "\n",
    "        elif isinstance(value[0], list) and len(value[0]) > 1:\n",
    "            for ls in value[0]:\n",
    "                if len(ls[\"content\"]) > threshold:\n",
    "                    desc = \"\".join(ls[\"content\"])\n",
    "                    tense = check_tense_nltk_updated(clean_text(desc))\n",
    "                    if tense !=  \"Unknown\":\n",
    "                        tense_counts[tense] += 1\n",
    "                    else:\n",
    "                        matches = re.findall(pattern, desc)\n",
    "                        if matches:\n",
    "                            tense_counts[\"past\"] += 1\n",
    "                        else:\n",
    "                            tense_counts[\"Unknown\"] += 1\n",
    "            dic[key] = tense_counts\n",
    "\n",
    "        elif isinstance(value[0], dict):\n",
    "            for ex, desc in value[0].items():\n",
    "                if len(desc) > threshold:\n",
    "                    tense = check_tense_nltk_updated(clean_text(desc))\n",
    "                    if tense !=  \"Unknown\":\n",
    "                        tense_counts[tense] += 1\n",
    "                    else:\n",
    "                        matches = re.findall(pattern, desc)\n",
    "                        if matches:\n",
    "                            tense_counts[\"past\"] += 1\n",
    "                        else:\n",
    "                            tense_counts[\"Unknown\"] += 1\n",
    "            dic[key] = tense_counts\n",
    "        else:\n",
    "            print(type(value[0]))\n",
    "            print(value[0])\n",
    "\n",
    "    return dic\n",
    "\n",
    "import re\n",
    "\n",
    "def dic_to_dic_w_unknown_tense(doc_w_exp):\n",
    "    dic = {}\n",
    "    pattern = r'\\(\\d+\\)\\s*([A-Za-z0-9\\-\\(\\)\\{\\},:;=\\[\\]\\+\\*\\s\\.\\^\\$\\%]+(?:\\.(?:sup|delta|Hz|NMR)[^\\)]*)?)'\n",
    "    pattern2 = r'\\(\\d+\\)\\s*(?:[A-Za-z]+\\s*\\d*\\-?[A-Za-z]*[\\(\\{][^)]*[\\)\\}][^;]*|(?:\\.\\d*H\\-?NMR[^;]*|\\d+\\.[a-zA-Z]*\\d*))'\n",
    "\n",
    "    for key, value in doc_w_exp.items():\n",
    "        unknown_experiments = []  # Initialize a list to store the experiments classified as Unknown\n",
    "        \n",
    "        if isinstance(value[0], list) and len(value[0]) == 1:\n",
    "            desc = value[0][0][\"title\"] + \".\" + \"\".join(value[0][0][\"content\"])\n",
    "            tense = check_tense_nltk_updated(clean_text(desc))\n",
    "            if tense == \"Unknown\":\n",
    "                unknown_experiments.append(desc)\n",
    "\n",
    "        elif isinstance(value[0], list) and len(value[0]) > 1:\n",
    "            for ls in value[0]:\n",
    "                desc = ls[\"title\"] + \".\" + \"\".join(ls[\"content\"])\n",
    "                tense = check_tense_nltk_updated(clean_text(desc))\n",
    "                if tense == \"Unknown\":\n",
    "                    unknown_experiments.append(desc)\n",
    "\n",
    "        elif isinstance(value[0], dict):\n",
    "            for ex, description in value[0].items():\n",
    "                tense = check_tense_nltk_updated(clean_text(description))\n",
    "                if tense == \"Unknown\":\n",
    "                    unknown_experiments.append(description)\n",
    "\n",
    "        # Only add to the dictionary if there are \"Unknown\" experiments\n",
    "        if unknown_experiments:\n",
    "            dic[key] = unknown_experiments\n",
    "\n",
    "    return dic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1140748136.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [105]\u001b[1;36m\u001b[0m\n\u001b[1;33m    unkown:\u001b[0m\n\u001b[1;37m           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# 8946300 example 1-5\n",
    "# 08946296 example 2 onwards \n",
    "# 08932484 examples 2 to 11\n",
    "\n",
    "\n",
    "# 08933074:  \n",
    "# Synthesis of 5,5-dimethyl-4-(pyridin-4-yl)-3-(4-(quinolin-2-ylmethoxy)phenyl) furan-2(5H)-one (Example 23)\n",
    "# 5,5-dimethyl-4-(pyridin-4-yl)-3-(4-(quinolin-2-ylmethoxy)phenyl) furan-2(5H)-one (Example 23)\n",
    "\n",
    "# 08945522:\n",
    "# Example 2\n",
    "# (69) 3-(3-(Triethoxysilyl)propylamino)phenyl benzoate and\n",
    "# (140) Example 9\n",
    "# (141) 4-(3-(Triethoxysilyl)propylamino)phenyl benzoate and\n",
    "# (142) Example 10\n",
    "# (143) 4-(Bis(3-(triethoxysilyl)propyl)amino)phenyl benzoate\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[8969641,8926966,8951515,8940436,9023999,9011956,9018396,9213027,9108964,9084992]#examples our algorithm didnt find but freilich did \n",
    "# many dont have an overarching examples section but have example 1, example 2 etc\n",
    "# check these xml, the text has examples section:9194008,9018332,9054322,9150561,9213027, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pprint as p\n",
    "# p.pprint(merged[merged.patentnumber == \"9018332\"][\"xml\"].values[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ERRORS IN FREILICH DATASET:\n",
    "patentnumber 8957069: in freilich 108.0\tnonproph and 0 prophetic but there are many more examples than 108, our algorithm: 267 nonprophetic and 1 prophetic and 3 unknown\n",
    "patent number 8946443:we extracted 271\tfreilich: 108, this patent has refrtence examples, are we extracting these or not? \n",
    "patent number 8952010: same issue do we take refrence examples? \n",
    "patent 8933099: there are far more than 33 examples in this patent but freilich says 33, our algorithm found 131\n",
    "what to do with Preparation examples?? e.g 8962612 in freilich has 6 we got 70."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# improvement for our algorithm\n",
    "1. remove duplicates somehow, e.g in patent number 08987242, there some examples are extracted twice just because the name e.g example aa137 appears multiple times.\n",
    "2. better classification of the patents. e.g patent 8987295 has 442, we extract the same number as frelich but we clasify differently.\n",
    "3. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
