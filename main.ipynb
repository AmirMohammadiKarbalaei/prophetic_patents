{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TO DO\n",
    "1. extract all text untill next example\n",
    "2. clean the collected urls to only include lates version ( removes duplication)\n",
    "https://ppubs.uspto.gov/pubwebapp/static/pages/ppubsbasic.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collection import collect_urls,download_files,unzip_files\n",
    "from utils import extract_examples_start_w_word,find_doc_number,process_siblings,extract_num_dot_examples,extract_examples_w_word,extract_experiments_w_heading,save_as_json,remove_duplicate_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import etree\n",
    "import requests\n",
    "\n",
    "# url =\"https://bulkdata.uspto.gov/data/patent/application/redbook/fulltext/2023/\"\n",
    "# rp = requests.get(url, timeout=10)\n",
    "# parser = etree.XMLParser(recover=True)\n",
    "# root = etree.fromstring(rp.text.encode(), parser)\n",
    "# href_values = root.findall(\".//a[@href]\") \n",
    "# urls = [href.get('href') for href in href_values if href.get('href').endswith('.zip')]\n",
    "# for ur in urls[:10]:\n",
    "#     print(url + ur)\n",
    "\n",
    "start_year = int(input(\"Enter the year you want to start from: \"))\n",
    "end_year = int(input(\"Enter the year you want to end at: \"))\n",
    "\n",
    "\n",
    "def fetch_urls_from_pto(start_year,end_year):\n",
    "    urls = []\n",
    "    for year in range(start_year, end_year + 1):\n",
    "        url = f\"https://bulkdata.uspto.gov/data/patent/application/redbook/fulltext/{year}/\"\n",
    "        rp = requests.get(url, timeout=10)\n",
    "        parser = etree.XMLParser(recover=True)\n",
    "        root = etree.fromstring(rp.text.encode(), parser)\n",
    "        href_values = root.findall(\".//a[@href]\")\n",
    "        urls.extend([href.get('href') for href in href_values if href.get('href').endswith('.zip')])\n",
    "    return urls\n",
    "\n",
    "urls = fetch_urls_from_pto(start_year,end_year)\n",
    "print(len(urls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "156\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import re\n",
    "from collections import defaultdict\n",
    "latest_versions = {}\n",
    "\n",
    "# Regex to extract date and optional revision number\n",
    "pattern = re.compile(r\"(ipa\\d{6})(?:_r(\\d+))?\\.zip\")\n",
    "\n",
    "for file in urls:\n",
    "    match = pattern.match(file)\n",
    "    if match:\n",
    "        base_name, revision = match.groups()\n",
    "        revision = int(revision) if revision else 0  # Default to 0 if no revision\n",
    "        \n",
    "        # Extract the current highest revision number for the base_name\n",
    "        current_revision_match = re.search(r'_r(\\d+)', latest_versions.get(base_name, \"\"))\n",
    "        current_revision = int(current_revision_match.group(1)) if current_revision_match else 0\n",
    "        \n",
    "        # Update if the new file has a higher revision\n",
    "        if base_name not in latest_versions or revision > current_revision:\n",
    "            latest_versions[base_name] = file\n",
    "\n",
    "# Get the final list of unique latest versions\n",
    "latest_files = sorted(latest_versions.values())\n",
    "\n",
    "print(len(latest_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded ipa220106.zip ------- 1 / 156\n",
      "Downloaded ipa220113.zip ------- 2 / 156\n",
      "Downloaded ipa220120.zip ------- 3 / 156\n",
      "Downloaded ipa220127.zip ------- 4 / 156\n",
      "Downloaded ipa220203.zip ------- 5 / 156\n",
      "Downloaded ipa220210.zip ------- 6 / 156\n",
      "Downloaded ipa220217.zip ------- 7 / 156\n",
      "Downloaded ipa220224.zip ------- 8 / 156\n",
      "Downloaded ipa220303.zip ------- 9 / 156\n",
      "Downloaded ipa220310.zip ------- 10 / 156\n",
      "Downloaded ipa220317.zip ------- 11 / 156\n",
      "Downloaded ipa220324.zip ------- 12 / 156\n",
      "Downloaded ipa220331.zip ------- 13 / 156\n",
      "Downloaded ipa220407.zip ------- 14 / 156\n",
      "Downloaded ipa220414.zip ------- 15 / 156\n",
      "Downloaded ipa220421.zip ------- 16 / 156\n",
      "Downloaded ipa220428.zip ------- 17 / 156\n",
      "Downloaded ipa220505.zip ------- 18 / 156\n",
      "Downloaded ipa220512.zip ------- 19 / 156\n",
      "Downloaded ipa220519.zip ------- 20 / 156\n",
      "Downloaded ipa220526.zip ------- 21 / 156\n",
      "Downloaded ipa220602.zip ------- 22 / 156\n",
      "Downloaded ipa220609.zip ------- 23 / 156\n",
      "Downloaded ipa220616.zip ------- 24 / 156\n",
      "Downloaded ipa220623.zip ------- 25 / 156\n",
      "Downloaded ipa220630.zip ------- 26 / 156\n",
      "Downloaded ipa220707.zip ------- 27 / 156\n",
      "Downloaded ipa220714.zip ------- 28 / 156\n",
      "Downloaded ipa220721.zip ------- 29 / 156\n",
      "Downloaded ipa220728.zip ------- 30 / 156\n"
     ]
    },
    {
     "ename": "ConnectTimeout",
     "evalue": "HTTPSConnectionPool(host='bulkdata.uspto.gov', port=443): Max retries exceeded with url: /data/patent/application/redbook/fulltext/2022/ipa220804.zip (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001B7489FDCD0>, 'Connection to bulkdata.uspto.gov timed out. (connect timeout=10)'))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTimeoutError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\urllib3\\connection.py:199\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 199\u001b[0m     sock \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_connection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    200\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dns_host\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    201\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m        \u001b[49m\u001b[43msource_address\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msource_address\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    203\u001b[0m \u001b[43m        \u001b[49m\u001b[43msocket_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    204\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m socket\u001b[38;5;241m.\u001b[39mgaierror \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\urllib3\\util\\connection.py:85\u001b[0m, in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 85\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     87\u001b[0m     \u001b[38;5;66;03m# Break explicitly a reference cycle\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\urllib3\\util\\connection.py:73\u001b[0m, in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[0;32m     72\u001b[0m     sock\u001b[38;5;241m.\u001b[39mbind(source_address)\n\u001b[1;32m---> 73\u001b[0m \u001b[43msock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43msa\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;66;03m# Break explicitly a reference cycle\u001b[39;00m\n",
      "\u001b[1;31mTimeoutError\u001b[0m: timed out",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mConnectTimeoutError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\urllib3\\connectionpool.py:789\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    788\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[1;32m--> 789\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    802\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    804\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\urllib3\\connectionpool.py:490\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    489\u001b[0m         new_e \u001b[38;5;241m=\u001b[39m _wrap_proxy_error(new_e, conn\u001b[38;5;241m.\u001b[39mproxy\u001b[38;5;241m.\u001b[39mscheme)\n\u001b[1;32m--> 490\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m new_e\n\u001b[0;32m    492\u001b[0m \u001b[38;5;66;03m# conn.request() calls http.client.*.request, not the method in\u001b[39;00m\n\u001b[0;32m    493\u001b[0m \u001b[38;5;66;03m# urllib3.request. It also calls makefile (recv) on the socket.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\urllib3\\connectionpool.py:466\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    465\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 466\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    467\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\urllib3\\connectionpool.py:1095\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[1;34m(self, conn)\u001b[0m\n\u001b[0;32m   1094\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mis_closed:\n\u001b[1;32m-> 1095\u001b[0m     \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1097\u001b[0m \u001b[38;5;66;03m# TODO revise this, see https://github.com/urllib3/urllib3/issues/2791\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\urllib3\\connection.py:693\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    692\u001b[0m sock: socket\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m|\u001b[39m ssl\u001b[38;5;241m.\u001b[39mSSLSocket\n\u001b[1;32m--> 693\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m sock \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_new_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    694\u001b[0m server_hostname: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\urllib3\\connection.py:208\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SocketTimeout \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 208\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ConnectTimeoutError(\n\u001b[0;32m    209\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    210\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m timed out. (connect timeout=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    211\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mConnectTimeoutError\u001b[0m: (<urllib3.connection.HTTPSConnection object at 0x000001B7489FDCD0>, 'Connection to bulkdata.uspto.gov timed out. (connect timeout=10)')",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\requests\\adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\urllib3\\connectionpool.py:843\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    841\u001b[0m     new_e \u001b[38;5;241m=\u001b[39m ProtocolError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection aborted.\u001b[39m\u001b[38;5;124m\"\u001b[39m, new_e)\n\u001b[1;32m--> 843\u001b[0m retries \u001b[38;5;241m=\u001b[39m \u001b[43mretries\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mincrement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    844\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_e\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m    845\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    846\u001b[0m retries\u001b[38;5;241m.\u001b[39msleep()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\urllib3\\util\\retry.py:519\u001b[0m, in \u001b[0;36mRetry.increment\u001b[1;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[0;32m    518\u001b[0m     reason \u001b[38;5;241m=\u001b[39m error \u001b[38;5;129;01mor\u001b[39;00m ResponseError(cause)\n\u001b[1;32m--> 519\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MaxRetryError(_pool, url, reason) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mreason\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    521\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncremented Retry for (url=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m): \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, url, new_retry)\n",
      "\u001b[1;31mMaxRetryError\u001b[0m: HTTPSConnectionPool(host='bulkdata.uspto.gov', port=443): Max retries exceeded with url: /data/patent/application/redbook/fulltext/2022/ipa220804.zip (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001B7489FDCD0>, 'Connection to bulkdata.uspto.gov timed out. (connect timeout=10)'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mConnectTimeout\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 17\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m year \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(start_year, end_year \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m     16\u001b[0m         url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://bulkdata.uspto.gov/data/patent/application/redbook/fulltext/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00myear\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 17\u001b[0m         files\u001b[38;5;241m.\u001b[39mappend(\u001b[43mdownload_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpatents_data\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlatest_files\u001b[49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[1;32mIn[14], line 9\u001b[0m, in \u001b[0;36mdownload_files\u001b[1;34m(main_url, download_path, files)\u001b[0m\n\u001b[0;32m      5\u001b[0m url \u001b[38;5;241m=\u001b[39m main_url \u001b[38;5;241m+\u001b[39m file_name\n\u001b[0;32m      8\u001b[0m zip_file_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(download_path, file_name) \u001b[38;5;66;03m#.split(\".\")[-1]\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(zip_file_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m response\u001b[38;5;241m.\u001b[39miter_content(chunk_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8192\u001b[39m):\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\requests\\api.py:73\u001b[0m, in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \n\u001b[0;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mget\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\requests\\api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\requests\\sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[0;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\requests\\adapters.py:688\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    685\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e\u001b[38;5;241m.\u001b[39mreason, ConnectTimeoutError):\n\u001b[0;32m    686\u001b[0m     \u001b[38;5;66;03m# TODO: Remove this in 3.0.0: see #2811\u001b[39;00m\n\u001b[0;32m    687\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e\u001b[38;5;241m.\u001b[39mreason, NewConnectionError):\n\u001b[1;32m--> 688\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ConnectTimeout(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[0;32m    690\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e\u001b[38;5;241m.\u001b[39mreason, ResponseError):\n\u001b[0;32m    691\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m RetryError(e, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "\u001b[1;31mConnectTimeout\u001b[0m: HTTPSConnectionPool(host='bulkdata.uspto.gov', port=443): Max retries exceeded with url: /data/patent/application/redbook/fulltext/2022/ipa220804.zip (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001B7489FDCD0>, 'Connection to bulkdata.uspto.gov timed out. (connect timeout=10)'))"
     ]
    }
   ],
   "source": [
    "def download_files(main_url, download_path,files):\n",
    "    if not os.path.exists(download_path):\n",
    "        os.makedirs(download_path)\n",
    "    for index,file_name in enumerate(files):\n",
    "        url = main_url + file_name\n",
    "\n",
    "\n",
    "        zip_file_path = os.path.join(download_path, file_name) #.split(\".\")[-1]\n",
    "        response = requests.get(url, stream=True, timeout=10)\n",
    "        with open(zip_file_path, \"wb\") as f:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                f.write(chunk)\n",
    "        print(f\"Downloaded {file_name} ------- {index + 1} / {len(files)}\")\n",
    "files = []\n",
    "for year in range(start_year, end_year + 1):\n",
    "        url = f\"https://bulkdata.uspto.gov/data/patent/application/redbook/fulltext/{year}/\"\n",
    "        files.append(download_files(url, \"patents_data\", latest_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download path set to: patents_data\n",
      "https://bulkdata.uspto.gov/data/patent/application/redbook/fulltext/2023/\n"
     ]
    },
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 3 column 1 (char 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\requests\\models.py:974\u001b[0m, in \u001b[0;36mResponse.json\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    973\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 974\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcomplexjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    975\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m JSONDecodeError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    976\u001b[0m     \u001b[38;5;66;03m# Catch JSON-related errors and raise as requests.JSONDecodeError\u001b[39;00m\n\u001b[0;32m    977\u001b[0m     \u001b[38;5;66;03m# This aliases json.JSONDecodeError and simplejson.JSONDecodeError\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\amoha\\anaconda3\\envs\\cuda\\Lib\\json\\__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[1;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[1;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\amoha\\anaconda3\\envs\\cuda\\Lib\\json\\decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[1;34m(self, s, _w)\u001b[0m\n\u001b[0;32m    333\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;124;03mcontaining a JSON document).\u001b[39;00m\n\u001b[0;32m    335\u001b[0m \n\u001b[0;32m    336\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 337\u001b[0m obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    338\u001b[0m end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n",
      "File \u001b[1;32mc:\\Users\\amoha\\anaconda3\\envs\\cuda\\Lib\\json\\decoder.py:355\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[1;34m(self, s, idx)\u001b[0m\n\u001b[0;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m--> 355\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpecting value\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, err\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    356\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj, end\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m: Expecting value: line 3 column 1 (char 6)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 34\u001b[0m\n\u001b[0;32m     31\u001b[0m     os\u001b[38;5;241m.\u001b[39mmakedirs(download_path, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFallback: Download path set to: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdownload_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 34\u001b[0m files \u001b[38;5;241m=\u001b[39m \u001b[43mcollect_urls\u001b[49m\u001b[43m(\u001b[49m\u001b[43myear\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m files:\n\u001b[0;32m     36\u001b[0m     total_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(file[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfileSize\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m1024\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m1024\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m files)\n",
      "Cell \u001b[1;32mIn[10], line 16\u001b[0m, in \u001b[0;36mcollect_urls\u001b[1;34m(year)\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m []\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m#print(f\"Urls collected for the period from {start_date} to {end_date} successfully\")\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproductFiles\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\requests\\models.py:978\u001b[0m, in \u001b[0;36mResponse.json\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    974\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m complexjson\u001b[38;5;241m.\u001b[39mloads(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    975\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m JSONDecodeError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    976\u001b[0m     \u001b[38;5;66;03m# Catch JSON-related errors and raise as requests.JSONDecodeError\u001b[39;00m\n\u001b[0;32m    977\u001b[0m     \u001b[38;5;66;03m# This aliases json.JSONDecodeError and simplejson.JSONDecodeError\u001b[39;00m\n\u001b[1;32m--> 978\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m RequestsJSONDecodeError(e\u001b[38;5;241m.\u001b[39mmsg, e\u001b[38;5;241m.\u001b[39mdoc, e\u001b[38;5;241m.\u001b[39mpos)\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m: Expecting value: line 3 column 1 (char 6)"
     ]
    }
   ],
   "source": [
    "\"yearly url: https://bulkdata.uspto.gov/data/patent/application/redbook/fulltext/2025/\"\n",
    "import requests\n",
    "import os\n",
    "\n",
    "def collect_urls(year):\n",
    "    url = f\"https://bulkdata.uspto.gov/data/patent/application/redbook/fulltext/{year}/\"\n",
    "    # params = {\n",
    "    #     \"data\": f'{{\"name\":\"APPXML\",\"fromDate\":\"{start_date}\",\"toDate\":\"{end_date}\"}}'\n",
    "    # }\n",
    "    print(url)\n",
    "    response = requests.get(url, timeout=10)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Error: {response.status_code}\")\n",
    "        return []\n",
    "    #print(f\"Urls collected for the period from {start_date} to {end_date} successfully\")\n",
    "    return response.json()[\"productFiles\"]\n",
    "\n",
    "# start_date = input(\"Enter start date YYYY-MM: \")\n",
    "# end_date = input(\"Enter end date YYYY-MM: \")\n",
    "year = input(\"Enter year: \")\n",
    "download_path = \"patents_data\"\n",
    "unzip_path = \"unzipped_patents_data\"\n",
    "\n",
    "try:\n",
    "    os.makedirs(download_path, exist_ok=True)\n",
    "    print(f\"Download path set to: {download_path}\")\n",
    "except PermissionError:\n",
    "    print(\"Error: No permission to create directory on D drive\")\n",
    "    # Fallback to current directory\n",
    "    download_path = \"patents_data\"\n",
    "    os.makedirs(download_path, exist_ok=True)\n",
    "    print(f\"Fallback: Download path set to: {download_path}\")\n",
    "\n",
    "files = collect_urls(year)\n",
    "if files:\n",
    "    total_size = sum(file[\"fileSize\"] / 1024 / 1024 for file in files)\n",
    "    print(\n",
    "        f\"\\nTotal file size: {round(total_size / 1024, 2)} GB for  {year}\\n\"\n",
    "    )\n",
    "    download_files(files, download_path)\n",
    "    unzip_files(download_path, unzip_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unzipping files:  12%|█▎        | 1/8 [00:08<01:01,  8.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unzipped ipa230105.zip to unzipped_patents_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unzipping files:  25%|██▌       | 2/8 [00:12<00:36,  6.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unzipped ipa230112.zip to unzipped_patents_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unzipping files:  38%|███▊      | 3/8 [00:15<00:21,  4.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unzipped ipa230112_r1.zip to unzipped_patents_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unzipping files:  50%|█████     | 4/8 [00:26<00:28,  7.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unzipped ipa230119.zip to unzipped_patents_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unzipping files:  62%|██████▎   | 5/8 [00:38<00:26,  8.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unzipped ipa230119_r1.zip to unzipped_patents_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unzipping files:  75%|███████▌  | 6/8 [00:45<00:16,  8.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unzipped ipa230126.zip to unzipped_patents_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unzipping files:  88%|████████▊ | 7/8 [00:53<00:08,  8.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unzipped ipa230126_r1.zip to unzipped_patents_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unzipping files: 100%|██████████| 8/8 [01:02<00:00,  7.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unzipped ipa230126_r2.zip to unzipped_patents_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "unzip_path = \"unzipped_patents_data\"\n",
    "download_path = \"patents_data\"\n",
    "unzip_files(download_path, unzip_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def process_xml_files(directory_path):\n",
    "    \"\"\"\n",
    "    Process all XML files in the given directory and split them into parts.\n",
    "    \n",
    "    Args:\n",
    "        directory_path (str): Path to directory containing XML files\n",
    "        \n",
    "    Returns:\n",
    "        list: Combined list of XML parts from all files\n",
    "    \"\"\"\n",
    "    all_xml_parts = []\n",
    "    \n",
    "    # Get all XML files in directory\n",
    "    xml_files = [f for f in os.listdir(directory_path) if f.endswith('.xml')]\n",
    "    \n",
    "    # Process each file\n",
    "    for xml_file in xml_files:\n",
    "        file_path = os.path.join(directory_path, xml_file)\n",
    "        try:\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "                content = file.read()\n",
    "                parts = content.split('<?xml version=\"1.0\" encoding=\"UTF-8\"?>')\n",
    "                # Remove empty parts and extend master list\n",
    "                parts = [p for p in parts if p.strip()]\n",
    "                all_xml_parts.extend(parts)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {xml_file}: {str(e)}\")\n",
    "    \n",
    "    return all_xml_parts\n",
    "xml_parts = process_xml_files(\"unzipped_patents_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30085"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(xml_parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"unzipped_patents_data\\\\ipa230105.xml\", \"r\", encoding=\"utf-8\") as file:\n",
    "#     content = file.read()\n",
    "# xml_parts = content.split('<?xml version=\"1.0\" encoding=\"UTF-8\"?>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "xml_no_dup = remove_duplicate_docs(xml_parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from bs4 import BeautifulSoup\n",
    "\n",
    "# def extract_experiments_w_heading(text):\n",
    "#     \"\"\"Extracts the 'Examples' section and its experiments from a patent text.\"\"\"\n",
    "\n",
    "#     # Use BeautifulSoup to parse the structure (for HTML-like patents)\n",
    "#     soup = BeautifulSoup(text, \"html.parser\")\n",
    "\n",
    "#     # Find the \"EXAMPLES\" section heading\n",
    "#     examples_heading = soup.find(\n",
    "#         lambda tag: tag.name == \"heading\"\n",
    "#         and (\n",
    "#             \"EXAMPLES\" in tag.text.upper()\n",
    "#             or \"EXAMPLE\" == tag.text.upper()\n",
    "#             or \"EXPERIMENT\" == tag.text.upper()\n",
    "#             or \"EXPERIMENTS\" in tag.text.upper()\n",
    "#         )\n",
    "#     )\n",
    "#     if not examples_heading:\n",
    "#         # print(\"No 'Examples' section found.\")\n",
    "#         return None\n",
    "\n",
    "\n",
    "#     return examples_heading "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/8048 so far found 19 docs with experiments\n",
      "200/8048 so far found 33 docs with experiments\n",
      "300/8048 so far found 36 docs with experiments\n",
      "400/8048 so far found 41 docs with experiments\n",
      "500/8048 so far found 45 docs with experiments\n",
      "600/8048 so far found 48 docs with experiments\n",
      "700/8048 so far found 49 docs with experiments\n",
      "800/8048 so far found 94 docs with experiments\n",
      "900/8048 so far found 151 docs with experiments\n",
      "1000/8048 so far found 192 docs with experiments\n",
      "1100/8048 so far found 238 docs with experiments\n",
      "1200/8048 so far found 257 docs with experiments\n",
      "1300/8048 so far found 264 docs with experiments\n",
      "1400/8048 so far found 269 docs with experiments\n",
      "1500/8048 so far found 303 docs with experiments\n",
      "1600/8048 so far found 308 docs with experiments\n",
      "1700/8048 so far found 313 docs with experiments\n",
      "1800/8048 so far found 331 docs with experiments\n",
      "1900/8048 so far found 337 docs with experiments\n",
      "2000/8048 so far found 342 docs with experiments\n",
      "2100/8048 so far found 345 docs with experiments\n",
      "2200/8048 so far found 346 docs with experiments\n",
      "2300/8048 so far found 348 docs with experiments\n",
      "2400/8048 so far found 375 docs with experiments\n",
      "2500/8048 so far found 450 docs with experiments\n",
      "2600/8048 so far found 500 docs with experiments\n",
      "2700/8048 so far found 550 docs with experiments\n",
      "2800/8048 so far found 606 docs with experiments\n",
      "2900/8048 so far found 658 docs with experiments\n",
      "3000/8048 so far found 700 docs with experiments\n",
      "3100/8048 so far found 726 docs with experiments\n",
      "3200/8048 so far found 740 docs with experiments\n",
      "3300/8048 so far found 741 docs with experiments\n",
      "3400/8048 so far found 741 docs with experiments\n",
      "3500/8048 so far found 742 docs with experiments\n",
      "3600/8048 so far found 744 docs with experiments\n",
      "3700/8048 so far found 747 docs with experiments\n",
      "3800/8048 so far found 748 docs with experiments\n",
      "3900/8048 so far found 754 docs with experiments\n",
      "4000/8048 so far found 780 docs with experiments\n",
      "4100/8048 so far found 783 docs with experiments\n",
      "4200/8048 so far found 784 docs with experiments\n",
      "4300/8048 so far found 790 docs with experiments\n",
      "4400/8048 so far found 802 docs with experiments\n",
      "4500/8048 so far found 802 docs with experiments\n",
      "4600/8048 so far found 806 docs with experiments\n",
      "4700/8048 so far found 806 docs with experiments\n",
      "4800/8048 so far found 809 docs with experiments\n",
      "4900/8048 so far found 810 docs with experiments\n",
      "5000/8048 so far found 811 docs with experiments\n",
      "5100/8048 so far found 817 docs with experiments\n",
      "5200/8048 so far found 819 docs with experiments\n",
      "5300/8048 so far found 827 docs with experiments\n",
      "5400/8048 so far found 834 docs with experiments\n",
      "5500/8048 so far found 837 docs with experiments\n",
      "5600/8048 so far found 844 docs with experiments\n",
      "5700/8048 so far found 846 docs with experiments\n",
      "5800/8048 so far found 852 docs with experiments\n",
      "5900/8048 so far found 866 docs with experiments\n",
      "6000/8048 so far found 868 docs with experiments\n",
      "6100/8048 so far found 869 docs with experiments\n",
      "6200/8048 so far found 870 docs with experiments\n",
      "6300/8048 so far found 872 docs with experiments\n",
      "6400/8048 so far found 886 docs with experiments\n",
      "6500/8048 so far found 921 docs with experiments\n",
      "6600/8048 so far found 926 docs with experiments\n",
      "6700/8048 so far found 930 docs with experiments\n",
      "6800/8048 so far found 931 docs with experiments\n",
      "6900/8048 so far found 932 docs with experiments\n",
      "7000/8048 so far found 935 docs with experiments\n",
      "7100/8048 so far found 935 docs with experiments\n",
      "7200/8048 so far found 939 docs with experiments\n",
      "7300/8048 so far found 941 docs with experiments\n",
      "7400/8048 so far found 943 docs with experiments\n",
      "7500/8048 so far found 949 docs with experiments\n",
      "7600/8048 so far found 953 docs with experiments\n",
      "7700/8048 so far found 954 docs with experiments\n",
      "7800/8048 so far found 954 docs with experiments\n",
      "7900/8048 so far found 955 docs with experiments\n",
      "8000/8048 so far found 957 docs with experiments\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'100/7824 so far found 20 docs with experiments\\n200/7824 so far found 34 docs with experiments\\n300/7824 so far found 36 docs with experiments\\n400/7824 so far found 42 docs with experiments\\n500/7824 so far found 45 docs with experiments\\n600/7824 so far found 49 docs with experiments\\n700/7824 so far found 51 docs with experiments\\n800/7824 so far found 98 docs with experiments\\n900/7824 so far found 162 docs with experiments\\n1000/7824 so far found 217 docs with experiments\\n1100/7824 so far found 253 docs with experiments\\n1200/7824 so far found 261 docs with experiments\\n1300/7824 so far found 266 docs with experiments\\n1400/7824 so far found 295 docs with experiments\\n1500/7824 so far found 304 docs with experiments\\n1600/7824 so far found 309 docs with experiments\\n1700/7824 so far found 328 docs with experiments\\n1800/7824 so far found 333 docs with experiments\\n1900/7824 so far found 339 docs with experiments\\n2000/7824 so far found 342 docs with experiments\\n2100/7824 so far found 343 docs with experiments\\n2200/7824 so far found 345 docs with experiments\\n2300/7824 so far found 358 docs with experiments\\n2400/7824 so far found 439 docs with experiments\\n2500/7824 so far found 510 docs with experiments\\nno dup 949  out of 7824 in 13.5 minutes'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "doc_w_exp = {}\n",
    "i = 0\n",
    "for xml in xml_no_dup:\n",
    "    i += 1\n",
    "    if i % 100 == 0:\n",
    "        print(f\"{i}/{len(xml_no_dup)} so far found {len(doc_w_exp)} docs with experiments\")\n",
    "    heading = extract_experiments_w_heading(xml)\n",
    "    if heading:\n",
    "        extracted_examples = []\n",
    "        if len(heading) >1:\n",
    "            for example in heading:\n",
    "                extracted_ex_start_w_word = extract_examples_start_w_word(example.find_next_siblings())\n",
    "                if len(extracted_ex_start_w_word) > 0:\n",
    "                    extracted_examples.append(extracted_ex_start_w_word)\n",
    "                else:\n",
    "                    extracted_ex_w_word = process_siblings(example.find_next_siblings())\n",
    "                    if extracted_ex_w_word:\n",
    "                        if len(extracted_ex_w_word) > 0:\n",
    "                            extracted_examples.append(extracted_ex_w_word)\n",
    "                    else:\n",
    "                        num_dot_examples = extract_num_dot_examples(str(heading[0].find_next_siblings()))\n",
    "                        if len(num_dot_examples) > 0:\n",
    "                            extracted_examples.append(num_dot_examples)\n",
    "        elif len(heading) == 1:\n",
    "            example_start_w = extract_examples_start_w_word(heading[0].find_next_siblings())\n",
    "            if example_start_w:\n",
    "                if len(example_start_w[0][\"content\"]) == 0:\n",
    "                    extracted_ex_w_word = process_siblings(heading[0].find_next_siblings())\n",
    "                    if len(extracted_ex_w_word) > 0:\n",
    "                        extracted_examples.append(extracted_ex_w_word)\n",
    "                else:\n",
    "                    extracted_examples.append(example_start_w)\n",
    "            else:\n",
    "                extracted_ex_w_word = process_siblings(heading[0].find_next_siblings())\n",
    "                if extracted_ex_w_word:\n",
    "                    if len(extracted_ex_w_word) > 0:\n",
    "                        extracted_examples.append(extracted_ex_w_word)\n",
    "                else:\n",
    "                    num_dot_examples = extract_num_dot_examples(str(heading[0].find_next_siblings()))\n",
    "                    if len(num_dot_examples) > 0:\n",
    "                        extracted_examples.append(num_dot_examples)\n",
    "        else:\n",
    "            extracted_ex_w_word = extract_examples_w_word(xml)\n",
    "            if extracted_ex_w_word:\n",
    "                extracted_examples.append(extracted_ex_w_word)\n",
    "            # else:\n",
    "            #     num_dot_examples = extract_num_dot_examples(str(heading[0].find_next_siblings()))\n",
    "            #     extracted_examples.append(num_dot_examples)\n",
    "    \n",
    "        \n",
    "    \n",
    "        if len(extracted_examples) > 0:\n",
    "            doc_w_exp[int(find_doc_number(xml)[0])] = extracted_examples\n",
    "            # if len(doc_w_exp) == 100:\n",
    "            #     break\n",
    "\n",
    "\n",
    "\"\"\"100/8048 so far found 17 docs with experiments\n",
    "200/8048 so far found 30 docs with experiments\n",
    "300/8048 so far found 33 docs with experiments\n",
    "400/8048 so far found 38 docs with experiments\n",
    "500/8048 so far found 42 docs with experiments\n",
    "600/8048 so far found 45 docs with experiments\n",
    "700/8048 so far found 46 docs with experiments\n",
    "800/8048 so far found 90 docs with experiments\n",
    "900/8048 so far found 146 docs with experiments\n",
    "1000/8048 so far found 187 docs with experiments\n",
    "1100/8048 so far found 233 docs with experiments\n",
    "1200/8048 so far found 251 docs with experiments\n",
    "1300/8048 so far found 258 docs with experiments\n",
    "1400/8048 so far found 263 docs with experiments\n",
    "1500/8048 so far found 294 docs with experiments\n",
    "1600/8048 so far found 298 docs with experiments\n",
    "1700/8048 so far found 302 docs with experiments\n",
    "1800/8048 so far found 320 docs with experiments\n",
    "1900/8048 so far found 326 docs with experiments\n",
    "2000/8048 so far found 329 docs with experiments\n",
    "2100/8048 so far found 331 docs with experiments\n",
    "2200/8048 so far found 332 docs with experiments\n",
    "2300/8048 so far found 334 docs with experiments\n",
    "2400/8048 so far found 361 docs with experiments\n",
    "2500/8048 so far found 435 docs with experiments\n",
    "with dup: 1196   out of 8048 in 12 minutes\"\"\"\n",
    "\n",
    "\"\"\"100/7824 so far found 20 docs with experiments\n",
    "200/7824 so far found 34 docs with experiments\n",
    "300/7824 so far found 36 docs with experiments\n",
    "400/7824 so far found 42 docs with experiments\n",
    "500/7824 so far found 45 docs with experiments\n",
    "600/7824 so far found 49 docs with experiments\n",
    "700/7824 so far found 51 docs with experiments\n",
    "800/7824 so far found 98 docs with experiments\n",
    "900/7824 so far found 162 docs with experiments\n",
    "1000/7824 so far found 217 docs with experiments\n",
    "1100/7824 so far found 253 docs with experiments\n",
    "1200/7824 so far found 261 docs with experiments\n",
    "1300/7824 so far found 266 docs with experiments\n",
    "1400/7824 so far found 295 docs with experiments\n",
    "1500/7824 so far found 304 docs with experiments\n",
    "1600/7824 so far found 309 docs with experiments\n",
    "1700/7824 so far found 328 docs with experiments\n",
    "1800/7824 so far found 333 docs with experiments\n",
    "1900/7824 so far found 339 docs with experiments\n",
    "2000/7824 so far found 342 docs with experiments\n",
    "2100/7824 so far found 343 docs with experiments\n",
    "2200/7824 so far found 345 docs with experiments\n",
    "2300/7824 so far found 358 docs with experiments\n",
    "2400/7824 so far found 439 docs with experiments\n",
    "2500/7824 so far found 510 docs with experiments\n",
    "no dup 949  out of 7824 in 13.5 minutes\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved as 957_w_dup.json\n"
     ]
    }
   ],
   "source": [
    "save_as_json(doc_w_exp,filename=\"test_no_dup.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_document_examples(xml_parts, max_docs=100):\n",
    "    \"\"\"Process and extract examples from XML documents.\n",
    "    \n",
    "    Args:\n",
    "        xml_parts (list): List of XML document parts\n",
    "        max_docs (int): Maximum number of documents to process\n",
    "        \n",
    "    Returns:\n",
    "        dict: Document numbers mapped to extracted examples\n",
    "    \"\"\"\n",
    "    doc_with_examples = {}\n",
    "    \n",
    "    for xml in xml_parts:\n",
    "        example_headings = extract_experiments_w_heading(xml)\n",
    "        if not example_headings:\n",
    "            continue\n",
    "            \n",
    "        extracted_examples = []\n",
    "        \n",
    "        try:\n",
    "            if len(example_headings) > 1:\n",
    "                # Process multiple headings\n",
    "                for heading in example_headings:\n",
    "                    siblings = heading.find_next_siblings()\n",
    "                    examples = extract_examples_start_w_word(siblings)\n",
    "                    \n",
    "                    if examples:\n",
    "                        extracted_examples.append(examples)\n",
    "                    else:\n",
    "                        processed = process_siblings(siblings)\n",
    "                        if processed:\n",
    "                            extracted_examples.append(processed)\n",
    "                            \n",
    "            else:\n",
    "                # Process single heading\n",
    "                siblings = example_headings[0].find_next_siblings()\n",
    "                examples = extract_examples_start_w_word(siblings)\n",
    "                \n",
    "                if examples and examples[0][\"content\"]:\n",
    "                    extracted_examples.append(examples)\n",
    "                else:\n",
    "                    processed = process_siblings(siblings)\n",
    "                    if processed:\n",
    "                        extracted_examples.append(processed)\n",
    "            \n",
    "            # Fall back to direct XML processing if needed\n",
    "            if not extracted_examples:\n",
    "                examples = extract_examples_w_word(xml)\n",
    "                if examples:\n",
    "                    extracted_examples.append(examples)\n",
    "            \n",
    "            # Store results if examples found\n",
    "            if extracted_examples:\n",
    "                doc_number = int(find_doc_number(xml)[0])\n",
    "                doc_with_examples[doc_number] = extracted_examples\n",
    "                \n",
    "                if len(doc_with_examples) >= max_docs:\n",
    "                    break\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing document: {str(e)}\")\n",
    "            continue\n",
    "            \n",
    "    return doc_with_examples\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
