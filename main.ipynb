{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TO DO\n",
    "1. extract all text untill next example\n",
    "2. clean the collected urls to only include lates version ( removes duplication)\n",
    "https://ppubs.uspto.gov/pubwebapp/static/pages/ppubsbasic.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collection import collect_urls,download_files,unzip_files\n",
    "from utils import extract_examples_start_w_word,find_doc_number,process_siblings,extract_num_dot_examples,extract_examples_w_word,extract_experiments_w_heading,save_as_json,remove_duplicate_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Urls collected for the period from 2023-01 to 2023-05 successfully\n"
     ]
    }
   ],
   "source": [
    "start_date = input(\"Enter start date YYYY-MM: \")\n",
    "end_date = input(\"Enter end date YYYY-MM: \")\n",
    "download_path = \"patents_data\"\n",
    "unzip_path = \"unzipped_patents_data\"\n",
    "\n",
    "files = collect_urls(start_date, end_date)\n",
    "if files:\n",
    "    total_size = sum(file[\"fileSize\"] / 1024 / 1024 for file in files)\n",
    "    print(\n",
    "        f\"\\nTotal file size: {round(total_size / 1024, 2)} GB between {start_date} and {end_date}\\n\"\n",
    "    )\n",
    "    download_files(files, download_path)\n",
    "    unzip_files(download_path, unzip_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"unzipped_patents_data\\\\ipa230105.xml\", \"r\", encoding=\"utf-8\") as file:\n",
    "    content = file.read()\n",
    "xml_parts = content.split('<?xml version=\"1.0\" encoding=\"UTF-8\"?>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "xml_no_dup = remove_duplicate_docs(xml_parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from bs4 import BeautifulSoup\n",
    "\n",
    "# def extract_experiments_w_heading(text):\n",
    "#     \"\"\"Extracts the 'Examples' section and its experiments from a patent text.\"\"\"\n",
    "\n",
    "#     # Use BeautifulSoup to parse the structure (for HTML-like patents)\n",
    "#     soup = BeautifulSoup(text, \"html.parser\")\n",
    "\n",
    "#     # Find the \"EXAMPLES\" section heading\n",
    "#     examples_heading = soup.find(\n",
    "#         lambda tag: tag.name == \"heading\"\n",
    "#         and (\n",
    "#             \"EXAMPLES\" in tag.text.upper()\n",
    "#             or \"EXAMPLE\" == tag.text.upper()\n",
    "#             or \"EXPERIMENT\" == tag.text.upper()\n",
    "#             or \"EXPERIMENTS\" in tag.text.upper()\n",
    "#         )\n",
    "#     )\n",
    "#     if not examples_heading:\n",
    "#         # print(\"No 'Examples' section found.\")\n",
    "#         return None\n",
    "\n",
    "\n",
    "#     return examples_heading "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/7824 so far found 20 docs with experiments\n",
      "200/7824 so far found 34 docs with experiments\n",
      "300/7824 so far found 36 docs with experiments\n",
      "400/7824 so far found 42 docs with experiments\n",
      "500/7824 so far found 45 docs with experiments\n",
      "600/7824 so far found 49 docs with experiments\n",
      "700/7824 so far found 51 docs with experiments\n",
      "800/7824 so far found 98 docs with experiments\n",
      "900/7824 so far found 162 docs with experiments\n",
      "1000/7824 so far found 217 docs with experiments\n",
      "1100/7824 so far found 253 docs with experiments\n",
      "1200/7824 so far found 261 docs with experiments\n",
      "1300/7824 so far found 266 docs with experiments\n",
      "1400/7824 so far found 295 docs with experiments\n",
      "1500/7824 so far found 304 docs with experiments\n",
      "1600/7824 so far found 309 docs with experiments\n",
      "1700/7824 so far found 328 docs with experiments\n",
      "1800/7824 so far found 333 docs with experiments\n",
      "1900/7824 so far found 339 docs with experiments\n",
      "2000/7824 so far found 342 docs with experiments\n",
      "2100/7824 so far found 343 docs with experiments\n",
      "2200/7824 so far found 345 docs with experiments\n",
      "2300/7824 so far found 358 docs with experiments\n",
      "2400/7824 so far found 439 docs with experiments\n",
      "2500/7824 so far found 510 docs with experiments\n",
      "2600/7824 so far found 578 docs with experiments\n",
      "2700/7824 so far found 627 docs with experiments\n",
      "2800/7824 so far found 691 docs with experiments\n",
      "2900/7824 so far found 720 docs with experiments\n",
      "3000/7824 so far found 731 docs with experiments\n",
      "3100/7824 so far found 732 docs with experiments\n",
      "3200/7824 so far found 732 docs with experiments\n",
      "3300/7824 so far found 733 docs with experiments\n",
      "3400/7824 so far found 735 docs with experiments\n",
      "3500/7824 so far found 738 docs with experiments\n",
      "3600/7824 so far found 739 docs with experiments\n",
      "3700/7824 so far found 747 docs with experiments\n",
      "3800/7824 so far found 771 docs with experiments\n",
      "3900/7824 so far found 774 docs with experiments\n",
      "4000/7824 so far found 776 docs with experiments\n",
      "4100/7824 so far found 789 docs with experiments\n",
      "4200/7824 so far found 793 docs with experiments\n",
      "4300/7824 so far found 795 docs with experiments\n",
      "4400/7824 so far found 797 docs with experiments\n",
      "4500/7824 so far found 797 docs with experiments\n",
      "4600/7824 so far found 801 docs with experiments\n",
      "4700/7824 so far found 802 docs with experiments\n",
      "4800/7824 so far found 803 docs with experiments\n",
      "4900/7824 so far found 808 docs with experiments\n",
      "5000/7824 so far found 811 docs with experiments\n",
      "5100/7824 so far found 818 docs with experiments\n",
      "5200/7824 so far found 827 docs with experiments\n",
      "5300/7824 so far found 829 docs with experiments\n",
      "5400/7824 so far found 835 docs with experiments\n",
      "5500/7824 so far found 839 docs with experiments\n",
      "5600/7824 so far found 846 docs with experiments\n",
      "5700/7824 so far found 858 docs with experiments\n",
      "5800/7824 so far found 860 docs with experiments\n",
      "5900/7824 so far found 861 docs with experiments\n",
      "6000/7824 so far found 863 docs with experiments\n",
      "6100/7824 so far found 864 docs with experiments\n",
      "6200/7824 so far found 880 docs with experiments\n",
      "6300/7824 so far found 914 docs with experiments\n",
      "6400/7824 so far found 918 docs with experiments\n",
      "6500/7824 so far found 922 docs with experiments\n",
      "6600/7824 so far found 922 docs with experiments\n",
      "6700/7824 so far found 925 docs with experiments\n",
      "6800/7824 so far found 926 docs with experiments\n",
      "6900/7824 so far found 926 docs with experiments\n",
      "7000/7824 so far found 930 docs with experiments\n",
      "7100/7824 so far found 933 docs with experiments\n",
      "7200/7824 so far found 935 docs with experiments\n",
      "7300/7824 so far found 941 docs with experiments\n",
      "7400/7824 so far found 944 docs with experiments\n",
      "7500/7824 so far found 945 docs with experiments\n",
      "7600/7824 so far found 945 docs with experiments\n",
      "7700/7824 so far found 946 docs with experiments\n",
      "7800/7824 so far found 949 docs with experiments\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1196   out of 8048 in 12 minutes'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "doc_w_exp = {}\n",
    "i = 0\n",
    "for xml in xml_no_dup:\n",
    "    i += 1\n",
    "    if i % 100 == 0:\n",
    "        print(f\"{i}/{len(xml_no_dup)} so far found {len(doc_w_exp)} docs with experiments\")\n",
    "    heading = extract_experiments_w_heading(xml)\n",
    "    if heading:\n",
    "        extracted_examples = []\n",
    "        if len(heading) >1:\n",
    "            for example in heading:\n",
    "                extracted_ex_start_w_word = extract_examples_start_w_word(example.find_next_siblings())\n",
    "                if len(extracted_ex_start_w_word) > 0:\n",
    "                    extracted_examples.append(extracted_ex_start_w_word)\n",
    "                else:\n",
    "                    extracted_ex_w_word = process_siblings(example.find_next_siblings())\n",
    "                    if extracted_ex_w_word:\n",
    "                        if len(extracted_ex_w_word) > 0:\n",
    "                            extracted_examples.append(extracted_ex_w_word)\n",
    "                    else:\n",
    "                        num_dot_examples = extract_num_dot_examples(str(heading[0].find_next_siblings()))\n",
    "                        if len(num_dot_examples) > 0:\n",
    "                            extracted_examples.append(num_dot_examples)\n",
    "        elif len(heading) == 1:\n",
    "            example_start_w = extract_examples_start_w_word(heading[0].find_next_siblings())\n",
    "            if example_start_w:\n",
    "                if len(example_start_w[0][\"content\"]) == 0:\n",
    "                    extracted_ex_w_word = process_siblings(heading[0].find_next_siblings())\n",
    "                    if len(extracted_ex_w_word) > 0:\n",
    "                        extracted_examples.append(extracted_ex_w_word)\n",
    "                else:\n",
    "                    extracted_examples.append(example_start_w)\n",
    "            else:\n",
    "                extracted_ex_w_word = process_siblings(heading[0].find_next_siblings())\n",
    "                if extracted_ex_w_word:\n",
    "                    if len(extracted_ex_w_word) > 0:\n",
    "                        extracted_examples.append(extracted_ex_w_word)\n",
    "                else:\n",
    "                    num_dot_examples = extract_num_dot_examples(str(heading[0].find_next_siblings()))\n",
    "                    if len(num_dot_examples) > 0:\n",
    "                        extracted_examples.append(num_dot_examples)\n",
    "        else:\n",
    "            extracted_ex_w_word = extract_examples_w_word(xml)\n",
    "            if extracted_ex_w_word:\n",
    "                extracted_examples.append(extracted_ex_w_word)\n",
    "            # else:\n",
    "            #     num_dot_examples = extract_num_dot_examples(str(heading[0].find_next_siblings()))\n",
    "            #     extracted_examples.append(num_dot_examples)\n",
    "    \n",
    "        \n",
    "    \n",
    "        if len(extracted_examples) > 0:\n",
    "            doc_w_exp[int(find_doc_number(xml)[0])] = extracted_examples\n",
    "            # if len(doc_w_exp) == 100:\n",
    "            #     break\n",
    "\n",
    "\n",
    "\"\"\"100/8048 so far found 17 docs with experiments\n",
    "200/8048 so far found 30 docs with experiments\n",
    "300/8048 so far found 33 docs with experiments\n",
    "400/8048 so far found 38 docs with experiments\n",
    "500/8048 so far found 42 docs with experiments\n",
    "600/8048 so far found 45 docs with experiments\n",
    "700/8048 so far found 46 docs with experiments\n",
    "800/8048 so far found 90 docs with experiments\n",
    "900/8048 so far found 146 docs with experiments\n",
    "1000/8048 so far found 187 docs with experiments\n",
    "1100/8048 so far found 233 docs with experiments\n",
    "1200/8048 so far found 251 docs with experiments\n",
    "1300/8048 so far found 258 docs with experiments\n",
    "1400/8048 so far found 263 docs with experiments\n",
    "1500/8048 so far found 294 docs with experiments\n",
    "1600/8048 so far found 298 docs with experiments\n",
    "1700/8048 so far found 302 docs with experiments\n",
    "1800/8048 so far found 320 docs with experiments\n",
    "1900/8048 so far found 326 docs with experiments\n",
    "2000/8048 so far found 329 docs with experiments\n",
    "2100/8048 so far found 331 docs with experiments\n",
    "2200/8048 so far found 332 docs with experiments\n",
    "2300/8048 so far found 334 docs with experiments\n",
    "2400/8048 so far found 361 docs with experiments\n",
    "2500/8048 so far found 435 docs with experiments\n",
    "with dup: 1196   out of 8048 in 12 minutes\"\"\"\n",
    "\n",
    "\"\"\"100/7824 so far found 20 docs with experiments\n",
    "200/7824 so far found 34 docs with experiments\n",
    "300/7824 so far found 36 docs with experiments\n",
    "400/7824 so far found 42 docs with experiments\n",
    "500/7824 so far found 45 docs with experiments\n",
    "600/7824 so far found 49 docs with experiments\n",
    "700/7824 so far found 51 docs with experiments\n",
    "800/7824 so far found 98 docs with experiments\n",
    "900/7824 so far found 162 docs with experiments\n",
    "1000/7824 so far found 217 docs with experiments\n",
    "1100/7824 so far found 253 docs with experiments\n",
    "1200/7824 so far found 261 docs with experiments\n",
    "1300/7824 so far found 266 docs with experiments\n",
    "1400/7824 so far found 295 docs with experiments\n",
    "1500/7824 so far found 304 docs with experiments\n",
    "1600/7824 so far found 309 docs with experiments\n",
    "1700/7824 so far found 328 docs with experiments\n",
    "1800/7824 so far found 333 docs with experiments\n",
    "1900/7824 so far found 339 docs with experiments\n",
    "2000/7824 so far found 342 docs with experiments\n",
    "2100/7824 so far found 343 docs with experiments\n",
    "2200/7824 so far found 345 docs with experiments\n",
    "2300/7824 so far found 358 docs with experiments\n",
    "2400/7824 so far found 439 docs with experiments\n",
    "2500/7824 so far found 510 docs with experiments\n",
    "no dup 949  out of 7824 in 13.5 minutes\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved as 1200_patents_w_experiments.json\n"
     ]
    }
   ],
   "source": [
    "# save_as_json(doc_w_exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_document_examples(xml_parts, max_docs=100):\n",
    "    \"\"\"Process and extract examples from XML documents.\n",
    "    \n",
    "    Args:\n",
    "        xml_parts (list): List of XML document parts\n",
    "        max_docs (int): Maximum number of documents to process\n",
    "        \n",
    "    Returns:\n",
    "        dict: Document numbers mapped to extracted examples\n",
    "    \"\"\"\n",
    "    doc_with_examples = {}\n",
    "    \n",
    "    for xml in xml_parts:\n",
    "        example_headings = extract_experiments_w_heading(xml)\n",
    "        if not example_headings:\n",
    "            continue\n",
    "            \n",
    "        extracted_examples = []\n",
    "        \n",
    "        try:\n",
    "            if len(example_headings) > 1:\n",
    "                # Process multiple headings\n",
    "                for heading in example_headings:\n",
    "                    siblings = heading.find_next_siblings()\n",
    "                    examples = extract_examples_start_w_word(siblings)\n",
    "                    \n",
    "                    if examples:\n",
    "                        extracted_examples.append(examples)\n",
    "                    else:\n",
    "                        processed = process_siblings(siblings)\n",
    "                        if processed:\n",
    "                            extracted_examples.append(processed)\n",
    "                            \n",
    "            else:\n",
    "                # Process single heading\n",
    "                siblings = example_headings[0].find_next_siblings()\n",
    "                examples = extract_examples_start_w_word(siblings)\n",
    "                \n",
    "                if examples and examples[0][\"content\"]:\n",
    "                    extracted_examples.append(examples)\n",
    "                else:\n",
    "                    processed = process_siblings(siblings)\n",
    "                    if processed:\n",
    "                        extracted_examples.append(processed)\n",
    "            \n",
    "            # Fall back to direct XML processing if needed\n",
    "            if not extracted_examples:\n",
    "                examples = extract_examples_w_word(xml)\n",
    "                if examples:\n",
    "                    extracted_examples.append(examples)\n",
    "            \n",
    "            # Store results if examples found\n",
    "            if extracted_examples:\n",
    "                doc_number = int(find_doc_number(xml)[0])\n",
    "                doc_with_examples[doc_number] = extracted_examples\n",
    "                \n",
    "                if len(doc_with_examples) >= max_docs:\n",
    "                    break\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing document: {str(e)}\")\n",
    "            continue\n",
    "            \n",
    "    return doc_with_examples\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
